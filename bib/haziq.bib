@Article{jamil2017,
  author  = {Haziq Jamil and Wicher Bergsma},
  title   = {iprior: {An R Package for Regression Modelling using I-priors}},
  journal = {Manuscript in submission},
  year    = {2017},
}

@Article{bergsma2017,
  author  = {Wicher Bergsma},
  title   = {{Regression with I-priors}},
  journal = {Unpublished manuscript},
  year    = {2017},
}

@Misc{R,
  title = {\proglang{R}: {A} Language and Environment for Statistical Computing},
  author = {{\proglang{R} Core Team}},
  organization = {\proglang{R} Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2017},
  url = {https://www.R-project.org/},
}

@article{jaynes1957a,
  title={Information Theory and Statistical Mechanics},
  author={Jaynes, Edwin T},
  journal={Physical Review},
  volume={106},
  number={4},
  pages={620},
  year={1957},
  publisher={APS}
}

@article{jaynes1957b,
  title={Information Theory and Statistical Mechanics II},
  author={Jaynes, Edwin T},
  journal={Physical Review},
  volume={108},
  number={2},
  pages={171},
  year={1957},
  publisher={APS}
}

@book{jaynes2003probability,
  title={Probability Theory: The Logic of Science},
  author={Jaynes, Edwin T},
  year={2003},
  publisher={Cambridge University Press}
}

@book{skorohod2012integration,
  title={Integration in Hilbert Space},
  author={Skorohod, Anatolij Vladimirovi{\v{c}}},
  volume={79},
  year={2012},
  publisher={Springer-Verlag},
  doi={10.1007/978-3-642-65632-3}
}

@incollection{lifshits2012lectures,
  title={Lectures on Gaussian processes},
  author={Lifshits, Mikhail},
  booktitle={Lectures on Gaussian Processes},
  pages={1--117},
  year={2012},
  publisher={Springer-Verlag},
  doi={10.1007/978-3-642-24939-6}
}

@book{rasmussen2006gaussian,
  title={Gaussian Processes for Machine Learning},
  author={Rasmussen, Carl Edward and Williams, Christopher K I},
  year={2006},
  publisher={The MIT Press}
}

@article{eddelbuettel2011rcpp,
   author = {Dirk Eddelbuettel and Romain Francois},
   title = {\pkg{Rcpp}: Seamless \proglang{R} and \proglang{C++} Integration},
   journal = {Journal of Statistical Software},
   volume = {40},
   number = {8},
   year = {2011},
   keywords = {},
   abstract = {The Rcpp package simplifies integrating C++ code with R. It provides a consistent C++ class hierarchy that maps various types of R objects (vectors, matrices, functions, environments, . . . ) to dedicated C++ classes. Object interchange between R and C++ is managed by simple, flexible and extensible concepts which include broad support for C++ Standard Template Library idioms. C++ code can both be compiled, linked and loaded on the fly, or added via packages. Flexible error and exception code handling is provided. Rcpp substantially lowers the barrier for programmers wanting to combine C++ code with R.},

   pages = {1--18},
   doi = {10.18637/jss.v040.i08}
   
}
% issn = {1548-7660}, url = {https://www.jstatsoft.org/v040/i08}

@article{lunn2000winbugs,
author="Lunn, David J.
and Thomas, Andrew
and Best, Nicky
and Spiegelhalter, David",
title="WinBUGS - A Bayesian modelling framework: Concepts, structure, and extensibility",
journal="Statistics and Computing",
year="2000",
month="Oct",
day="01",
volume="10",
number="4",
pages="325--337",
abstract="WinBUGS is a fully extensible modular framework for constructing and analysing Bayesian full probability models. Models may be specified either textually via the BUGS language or pictorially using a graphical interface called DoodleBUGS. WinBUGS processes the model specification and constructs an object-oriented representation of the model. The software offers a user-interface, based on dialogue boxes and menu commands, through which the model may then be analysed using Markov chain Monte Carlo techniques. In this paper we discuss how and why various modern computing concepts, such as object-orientation and run-time linking, feature in the software's design. We also discuss how the framework may be extended. It is possible to write specific applications that form an apparently seamless interface with WinBUGS for users with specialized requirements. It is also possible to interface with WinBUGS at a lower level by incorporating new object types that may be used by WinBUGS without knowledge of the modules in which they are implemented. Neither of these types of extension require access to, or even recompilation of, the WinBUGS source-code.",
doi="10.1023/A:1008929526011"
}
% issn="1573-1375",

@inproceedings{plummer2003jags,
  title={\proglang{JAGS}: A Program for Analysis of Bayesian Graphical Models Using Gibbs Sampling},
  author={Plummer, Martyn},
  booktitle={Proceedings of the 3rd International Workshop on Distributed Statistical Computing},
  volume={124},
  pages={125},
  year={2003},
  organization={Vienna, Austria}
}

@article{sturtz2005r2winbugs,
   author = {Sibylle Sturtz and Uwe Ligges and Andrew Gelman},
   title = {\pkg{R2WinBUGS}: A Package for Running WinBUGS from \proglang{R}},
   journal = {Journal of Statistical Software},
   volume = {12},
   number = {3},
   year = {2005},
   keywords = {},
   abstract = {The R2WinBUGS package provides convenient functions to call WinBUGS from R. It automatically writes the data and scripts in a format readable by WinBUGS for processing in batch mode, which is possible since version 1.4. After the WinBUGS process has finished, it is possible either to read the resulting data into R by the package itself--which gives a compact graphical summary of inference and convergence diagnostics--or to use the facilities of the coda package for further analyses of the output. Examples are given to demonstrate the usage of this package.},
   pages = {1--16},
   doi = {10.18637/jss.v012.i03}
}
% issn = {1548-7660},

@article{denwood2016runjags,
   author = {Matthew Denwood},
   title = {\pkg{runjags}: An \proglang{R} Package Providing Interface Utilities, Model Templates, Parallel Computing Methods and Additional Distributions for MCMC Models in \proglang{JAGS}},
   journal = {Journal of Statistical Software},
   volume = {71},
   number = {9},
   year = {2016},
   keywords = {MCMC; Bayesian; graphical models; interface utilities; JAGS; BUGS; R},
   abstract = {The runjags package provides a set of interface functions to facilitate running Markov chain Monte Carlo models in JAGS from within R. Automated calculation of appropriate convergence and sample length diagnostics, user-friendly access to commonly used graphical outputs and summary statistics, and parallelized methods of running JAGS are provided. Template model specifications can be generated using a standard lme4-style formula interface to assist users less familiar with the BUGS syntax. Automated simulation study functions are implemented to facilitate model performance assessment, as well as drop-k type cross-validation studies, using high performance computing clusters such as those provided by parallel. A module extension for JAGS is also included within runjags, providing the Pareto family of distributions and a series of minimally-informative priors including the DuMouchel and half-Cauchy priors. This paper outlines the primary functions of this package, and gives an illustration of a simulation study to assess the sensitivity of two equivalent model formulations to different prior distributions.},
   pages = {1--25},
   doi = {10.18637/jss.v071.i09}
}
% issn = {1548-7660}, url = {https://www.jstatsoft.org/v071/i09}

@article{carpenter2016stan,
  title={\proglang{Stan}: A Probabilistic Programming Language},
   author = {Bob Carpenter and Andrew Gelman and Matthew Hoffman and Daniel Lee and Ben Goodrich and Michael Betancourt and Marcus Brubaker and Jiqiang Guo and Peter Li and Allen Riddell},
   journal = {Journal of Statistical Software, Articles},
   volume = {76},
   number = {1},
   year = {2017},
   keywords = {probabilistic programming; Bayesian inference; algorithmic differentiation; Stan},
   abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
   pages = {1--32},
   doi = {10.18637/jss.v076.i01},
}
% issn = {1548-7660}, url = {https://www.jstatsoft.org/v076/i01}

@Misc{rstan,
  title = {\pkg{RStan}: The \proglang{R} Interface to \proglang{Stan}},
  author = {{Stan Development Team}},
  note = {\proglang{R} package version 2.14.1},
  year = {2016},
  url = {http://mc-stan.org/},
}

@article{quinonero2005unifying,
  title={A Unifying View of Sparse Approximate Gaussian Process Regression},
  author={Qui{\~n}onero-Candela, Joaquin and Rasmussen, Carl Edward},
  journal={Journal of Machine Learning Research},
  volume={6},
  month={Dec},
  pages={1939--1959},
  year={2005}
}

@inproceedings{williams2001using,
  title={Using the Nystr{\"o}m Method to Speed Up Kernel Machines},
  author={Williams, Christopher K I and Seeger, Matthias},
  booktitle={Advances in Neural Information Processing Systems 13},
  pages={682--688},
  year={2001},
  publisher={The MIT Press}
}

@inproceedings{fowlkes2001efficient,
  author={C Fowlkes and S Belongie and J Malik}, 
  booktitle={Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2001)}, 
  title={Efficient Spatiotemporal Grouping Using the Nystr\"om Method}, 
  year={2001}, 
  volume={1}, 
  number={}, 
  pages={231--238}, 
  keywords={eigenvalues and eigenfunctions;image segmentation;image sequences;video signal processing;Nystrom method;coherent groups;efficient spatiotemporal grouping;eigenfunction problems;extrapolation;image segmentation;numerical solution;pairwise voxel similarities;spectral graph theoretic methods;spectral partitioning;video sequence;Computer applications;Educational institutions;Eigenvalues and eigenfunctions;Extrapolation;Image segmentation;Image sequences;Pixel;Psychology;Spatiotemporal phenomena;Video sequences}, 
  doi={10.1109/CVPR.2001.990481}, 
  month={}
}
% ISSN={1063-6919}, 

@article{zeileis2004kernlab,
   author = {Alexandros Karatzoglou and Alexandros Smola and Kurt Hornik and Achim Zeileis},
   title = {\pkg{kernlab} - An \proglang{S4} Package for Kernel Methods in \proglang{R}},
   journal = {Journal of Statistical Software},
   volume = {11},
   number = {9},
   year = {2004},
   keywords = {},
   abstract = {kernlab is an extensible package for kernel-based machine learning methods in R. It takes advantage of R's new S4 ob ject model and provides a framework for creating and using kernel-based algorithms. The package contains dot product primitives (kernels), implementations of support vector machines and the relevance vector machine, Gaussian processes, a ranking algorithm, kernel PCA, kernel CCA, and a spectral clustering algorithm. Moreover it provides a general purpose quadratic programming solver, and an incomplete Cholesky decomposition method.},
   pages = {1--20},
   doi = {10.18637/jss.v011.i09},
}
% issn = {1548-7660}, url = {https://www.jstatsoft.org/v011/i09}

@Misc{gpfda,
  title = {\pkg{GPFDA}: Apply Gaussian Process in Functional Data Analysis},
  author = {Jian Qing Shi and Yafeng Cheng},
  year = {2014},
  note = {\proglang{R} package version 2.2},
  url = {https://CRAN.R-project.org/package=GPFDA},
}

@article{macdonald2015gpfit,
  title={\pkg{GPfit}: An \proglang{R} Package for Fitting a Gaussian Process Model to Deterministic Simulator Outputs},
  author = {Blake MacDonald and Pritam Ranjan and Hugh Chipman},
  journal = {Journal of Statistical Software},
  volume = {64},
  number = {12},
  year = {2015},
  keywords = {},
  abstract = {Gaussian process (GP) models are commonly used statistical metamodels for emulating expensive computer simulators. Fitting a GP model can be numerically unstable if any pair of design points in the input space are close together. Ranjan, Haynes, and Karsten (2011) proposed a computationally stable approach for fitting GP models to deterministic computer simulators. They used a genetic algorithm based approach that is robust but computationally intensive for maximizing the likelihood. This paper implements a slightly modified version ofthe model proposed by Ranjan et al.  (2011 ) in the R  package GPfit. A novel parameterization of the spatial correlation function and a clustering based multi-start gradient based optimization algorithm yield robust optimization that is typically faster than the genetic algorithm based approach. We present two examples with R  codes to illustrate the usage of the main functions in GPfit . Several test functions are used for performance comparison with the popular R  package mlegp . We also use GPfit  for a real application, i.e., for emulating the tidal kinetic energy model for the Bay of Fundy, Nova Scotia, Canada. GPfit  is free software and distributed under the General Public License and available from the Comprehensive R Archive Network.},
  pages = {1--23},
  doi = {10.18637/jss.v064.i12},
}
% issn = {1548-7660}, url = {https://www.jstatsoft.org/v064/i12}


@Misc{gptk,
  title = {\pkg{gptk}: Gaussian Processes Tool-Kit},
  author = {Alfredo Kalaitzis and Antti Honkela and Pei Gao and Neil D Lawrence},
  year = {2014},
  note = {\proglang{R} package version 1.08},
  url = {https://CRAN.R-project.org/package=gptk},
}

@article{kalaitzis2011simple,
  author="Kalaitzis, Alfredo 
  and Lawrence, Neil D",
  title="A Simple Approach to Ranking Differentially Expressed Gene Expression Time Courses through Gaussian Process Regression",
  journal="BMC Bioinformatics",
  year="2011",
  month="May",
  day="20",
  volume="12",
  number="1",
  pages="180",
  abstract="The analysis of gene expression from time series underpins many biological studies. Two basic forms of analysis recur for data of this type: removing inactive (quiet) genes from the study and determining which genes are differentially expressed. Often these analysis stages are applied disregarding the fact that the data is drawn from a time series. In this paper we propose a simple model for accounting for the underlying temporal nature of the data based on a Gaussian process.",
  doi="10.1186/1471-2105-12-180"
}
% issn="1471-2105"

@article{bates2014lme4,
  author = {Douglas Bates and Martin Mächler and Ben Bolker and Steve Walker},
  title = {Fitting Linear Mixed-Effects Models Using \pkg{lme4}},
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {1},
  year = {2015},
  keywords = {sparse matrix methods; linear mixed models; penalized least squares; Cholesky decomposition},
  abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
  pages = {1--48},
  doi = {10.18637/jss.v067.i01}
}
% issn = {1548-7660}

@Misc{rstanarm,
  title = {\pkg{rstanarm}: Bayesian Applied Regression Modeling via \proglang{Stan}},
  author = {{Stan Development Team}},
  note = {\proglang{R} package version 2.13.1},
  year = {2016},
  url = {http://mc-stan.org/},
}

@article{buerkner2016brms,
  author = {Paul-Christian B\"urkner},
  title = {\proglang{brms}: An \proglang{R} Package for Bayesian Multilevel Models Using \proglang{Stan}},
  journal = {Journal of Statistical Software},
  volume = {80},
  number = {1},
  year = {2017},
  keywords = {Bayesian inference; multilevel model; ordinal data; MCMC; Stan; R},
  abstract = {The brms package implements Bayesian multilevel models in R using the probabilistic programming language Stan. A wide range of distributions and link functions are supported, allowing users to fit  -  among others  -  linear, robust linear, binomial, Poisson, survival, ordinal, zero-inflated, hurdle, and even non-linear models all in a multilevel context. Further modeling options include autocorrelation of the response variable, user defined covariance structures, censored data, as well as meta-analytic standard errors. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. In addition, model fit can easily be assessed and compared with the Watanabe-Akaike information criterion and leave-one-out cross-validation.},
  pages = {1--28},
  doi = {10.18637/jss.v080.i01}
}
% issn = {1548-7660}

@Book{mgcv,
  title = {Generalized Additive Models: An Introduction with \proglang{R}},
  year = {2017},
  author = {Simon N Wood},
  edition = {2nd},
  publisher = {Chapman and Hall/CRC},
}

@Misc{fda,
  title = {\pkg{fda}: Functional Data Analysis},
  author = {J. O. Ramsay and Hadley Wickham and Spencer Graves and Giles Hooker},
  year = {2017},
  note = {\proglang{R} package version 2.4.7},
  url = {https://CRAN.R-project.org/package=fda},
}

@book{bishop2006pattern,
  title={Pattern Recognition and Machine Learning},
  author={Bishop, Christopher},
  year={2006},
  publisher={Springer-Verlag},
}

@inproceedings{gretton2005measuring,
  author="Gretton, Arthur
  and Bousquet, Olivier
  and Smola, Alex
  and Sch{\"o}lkopf, Bernhard",
  editor="Jain, Sanjay
  and Simon, Hans Ulrich
  and Tomita, Etsuji",
  title="Measuring Statistical Dependence with Hilbert-Schmidt Norms",
  bookTitle="Proceedings of the 16th International Conference of Algorithmic Learning Theory",
  year="2005",
  publisher="Springer-Verlag",
  pages="63--77",
  abstract="We propose an independence criterion based on the eigenspectrum of covariance operators in reproducing kernel Hilbert spaces (RKHSs), consisting of an empirical estimate of the Hilbert-Schmidt norm of the cross-covariance operator (we term this a Hilbert-Schmidt Independence Criterion, or HSIC). This approach has several advantages, compared with previous kernel-based independence criteria. First, the empirical estimate is simpler than any other kernel dependence test, and requires no user-defined regularisation. Second, there is a clearly defined population quantity which the empirical estimate approaches in the large sample limit, with exponential convergence guaranteed between the two: this ensures that independence tests based on HSIC do not suffer from slow learning rates. Finally, we show in the context of independent component analysis (ICA) that the performance of HSIC is competitive with that of previously published kernel-based criteria, and of other recently published ICA methods.",
  doi="10.1007/11564089_7"
}
% isbn="978-3-540-31696-1",

@Misc{foreach,
  title = {\pkg{foreach}: Provides Foreach Looping Construct for \proglang{R}},
  author = {{Revolution Analytics} and Steve Weston},
  year = {2015},
  note = {\proglang{R} package version 1.4.3},
  url = {https://CRAN.R-project.org/package=foreach},
}

@Misc{dosnow,
  title = {\pkg{doSNOW}: Foreach Parallel Adaptor for the \pkg{snow} Package},
  author = {{Microsoft Corporation} and Stephen Weston},
  year = {2017},
  note = {\proglang{R} package version 1.0.15},
  url = {https://CRAN.R-project.org/package=doSNOW},
}

@Misc{caret,
  title = {\pkg{caret}: Classification and Regression Training},
  author = {Max Kuhn and others},
  year = {2017},
  note = {\proglang{R} package version 6.0--77},
  url = {https://CRAN.R-project.org/package=caret},
}

@article{vila2000bayesian,
  author={J. P. Vila and V. Wagner and P. Neveu}, 
  journal={IEEE Transactions on Neural Networks}, 
  title={Bayesian Nonlinear Model Selection and Neural Networks: A Conjugate Prior Approach}, 
  year={2000}, 
  volume={11}, 
  number={2}, 
  pages={265-278}, 
  keywords={Bayes methods;belief networks;decision theory;feedforward neural nets;probability;statistical analysis;Bayesian nonlinear model selection;Gaussian likelihood;conjugate prior approach;embedded models;empirical Bayes-like approach;expected utility criterion;general Bayesian nonlinear regression model comparison procedure;internal consistency;model posterior predictive density;predictive neural-network architecture;predictive probability distribution;training set;Bayesian methods;Distributed computing;Feedforward neural networks;Function approximation;Network topology;Neural networks;Predictive models;Probability distribution;Testing;Utility theory}, 
  doi={10.1109/72.838999}, 
  month={March}
}

@book{ferraty2006nonparametric,
  title={Nonparametric Functional Data Analysis},
  author={Ferraty, Fr{\'e}d{\'e}ric and Vieu, Philippe},
  year={2006},
  publisher={Springer-Verlag},
  doi={10.1007/0-387-36620-2},
  edition={1st}
}

@article{chen2011single,
  title={Single and Multiple Index Functional Regression Models with Nonparametric Link},
  author={Chen, Dong and Hall, Peter and M{\"u}ller, Hans-Georg},
  journal={The Annals of Statistics},
  volume={39},
  number={3},
  pages={1720--1747},
  year={2011},
  publisher={The Institute of Mathematical Statistics},
  doi={10.1214/11-AOS882}
}

@incollection{goia2014some,
  title={Some Advances in Semiparametric Functional Data Modelling},
  author={Goia, Aldo and Vieu, Philippe},
  booktitle={Contributions in Infinite-Dimensional Statistics and Related Topics},
  pages={135--141},
  year={2014},
  publisher={Societa Editrice Esculapio},
  doi={10.15651/978-88-748-8763-7}
}

@article{lian2014series,
  title = "Series Expansion for Functional Sufficient Dimension Reduction",
  journal = "Journal of Multivariate Analysis",
  volume = "124",
  number = "C",
  pages = "150--165",
  year = "2014",
  doi = "10.1016/j.jmva.2013.10.019",
  author = "Heng Lian and Gaorong Li",
  keywords = "Functional principal component analysis, Polynomial splines, Sliced average variance estimation, Sliced inverse regression"
}

@article{zhu2014structured,
  author = {Zhu, Hongxiao and Yao, Fang and Zhang, Hao Helen},
  title = {Structured Functional Additive Regression in Reproducing Kernel Hilbert Spaces},
  journal = {Journal of the Royal Statistical Society B (Statistical Methodology)},
  volume = {76},
  number = {3},
  doi = {10.1111/rssb.12036},
  pages = {581--603},
  keywords = {Additive models, Component selection, Functional data analysis, Principal components, Reproducing kernel Hilbert space, Smoothing spline},
  year = {2014}
}

@Misc{jmcm,
  title = {\pkg{jmcm}: Joint Mean-Covariance Models using \proglang{Armadillo} and \proglang{S4}},
  author = {Jianxin Pan and Yi Pan},
  year = {2016},
  note = {\proglang{R} package version 0.1.7.0},
  url = {https://CRAN.R-project.org/package=jmcm},
}

@article{kenward1987method,
  doi = {10.2307/2347788},  
  abstract = {A method for the comparison of profiles of repeated measurements is described which is based on Gabriel's ante-dependence covariance structure. The method is intended for experiments in which no specific features of the profiles are known to be of interest a priori. The establishment of the appropriate order of ante-dependence structure is discussed and a test statistic for the overall comparison of profiles under the structure is defined. It is shown how this statistic can be decomposed into statistically independent components which can be used to investigate the form of the differences among profiles. In particular the use of these components is proposed as an improvement over the practice of calculating a separate t-test for each time of measurement. All the statistics described can be constructed from standard analysis of covariance sums of squares.},
  author = {Michael G. Kenward},
  journal = {Journal of the Royal Statistical Society C (Applied Statistics)},
  number = {3},
  pages = {296-308},
  title = {A Method for Comparing Profiles of Repeated Measurements},
  volume = {36},
  year = {1987}
}

@book{davidian1995nonlinear,
  title={Nonlinear Models for Repeated Measurement Data},
  author={Davidian, Marie and Giltinan, David M},
  year={1995},
  publisher={Chapman and Hall/CRC}
}

@book{pinheiro2000mixed,
  title={Mixed-Effects Models in S and S-plus},
  author={Pinheiro, Jos{\'e} C and Bates, Douglas M},
  year={2000},
  publisher={Springer-Verlag},
  doi = {10.1007/b98882}
}

@Misc{nlme,
  title = {\pkg{nlme}: Linear and Nonlinear Mixed Effects Models},
  author = {Jos{\'e}o Pinheiro and Douglas Bates and Saikat DebRoy and Deepayan Sarkar and {R Core Team}},
  year = {2017},
  note = {\proglang{R} package version 3.1-131},
  url = {https://CRAN.R-project.org/package=nlme},
}

@book{berlinet2011reproducing,
  title={Reproducing Kernel Hilbert Spaces in Probability and Statistics},
  author={Berlinet, Alain and Thomas-Agnan, Christine},
  year={2011},
  publisher={Springer-Verlag},
  doi = {10.1007/978-1-4419-9096-9}
}

@article{thodberg1996review,
  title={A Review of Bayesian Neural Networks with an Application to near Infrared Spectroscopy},
  author={Thodberg, Hans Henrik},
  journal={IEEE Transactions on Neural Networks},
  volume={7},
  number={1},
  pages={56--72},
  year={1996},
  doi={10.1109/72.478392}
}

@inproceedings{meila2003data,
  title={Data centering in feature space.},
  author={Meila, Marina},
  booktitle={Proceedings of the Ninth International Workshop on Artificial Intelligence
               and Statistics ({AISTATS})},
  year={2003},
  editor={Christopher Bishop and Brendan Frey},
  publisher={Society for Artificial Intelligence and Statistics},
}

@inproceedings{cheng2017variational,
  title={Variational Inference for Gaussian Process Models with Linear Complexity},
  author={Cheng, Ching-An and Boots, Byron},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5190--5200},
  year={2017}
}

@article{kimeldorf1970correspondence,
  title={A correspondence between Bayesian estimation on stochastic processes and smoothing by splines},
  author={Kimeldorf, George S and Wahba, Grace},
  journal={The Annals of Mathematical Statistics},
  volume={41},
  number={2},
  pages={495--502},
  year={1970},
  publisher={JSTOR}
}

@article{ra1922mathematical,
  title={On the mathematical foundations of theoretical statistics},
  author={RA Fisher},
  journal={Phil. Trans. R. Soc. Lond. A},
  volume={222},
  number={594-604},
  pages={309--368},
  year={1922},
  publisher={The Royal Society}
}

@article{efron1978assessing,
  title={Assessing the accuracy of the maximum likelihood estimator: Observed versus expected Fisher information},
  author={Efron, Bradley and Hinkley, David V},
  journal={Biometrika},
  volume={65},
  number={3},
  pages={457--483},
  year={1978},
  publisher={Oxford University Press}
}

@book{pawitan2001all,
  title={In all likelihood: statistical modelling and inference using likelihood},
  author={Pawitan, Yudi},
  year={2001},
  publisher={Oxford University Press}
}

@book{wasserman2013all,
  title={All of statistics: a concise course in statistical inference},
  author={Wasserman, Larry},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@book{steinwart2008support,
  title={Support vector machines},
  author={Steinwart, Ingo and Christmann, Andreas},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@article{mandelbrot1968fractional,
  title={Fractional Brownian motions, fractional noises and applications},
  author={Mandelbrot, Benoit B and Van Ness, John W},
  journal={SIAM review},
  volume={10},
  number={4},
  pages={422--437},
  year={1968},
  publisher={SIAM}
}

@incollection{cohen2002,
  title={Champs localement auto-similaires},
  booktitle={Lois d'{\'e}chelle, fractales et ondelettes},
  author={S Cohen},
  editor={Abry, Patrice and Gon{\c{c}}alves, Paulo and V{\'e}hel, Jacques L{\'e}vy},
  volume={1},
  year={2002},
  publisher={Herm{\`e}s Sciences Publications}
}

@book{embrechts2002selfsimilar,
  title={Selfsimilar Processes. Princeton series in applied mathematics},
  author={Embrechts, Paul and Maejima, Makoto},
  year={2002},
  publisher={Princeton University Press, Princeton, NJ}
}

@phdthesis{duvenaud2014automatic,
  title={Automatic model construction with Gaussian processes},
  author={Duvenaud, David},
  year={2014},
  school={University of Cambridge}
}

@article{steinwart2006explicit,
  title={An explicit description of the reproducing kernel Hilbert spaces of Gaussian RBF kernels},
  author={Steinwart, Ingo and Hush, Don and Scovel, Clint},
  journal={IEEE Transactions on Information Theory},
  volume={52},
  number={10},
  pages={4635--4643},
  year={2006},
  publisher={IEEE}
}

@article{micchelli2006universal,
  title={Universal kernels},
  author={Micchelli, Charles A and Xu, Yuesheng and Zhang, Haizhang},
  journal={Journal of Machine Learning Research},
  volume={7},
  number={Dec},
  pages={2651--2667},
  year={2006}
}

@article{kree1974produits,
  title={Produits tensoriels compl{\'e}t{\'e}s d’espaces de Hilbert},
  author={Kr{\'e}e, Paul},
  journal={S{\'e}minaire Paul Kr{\'e}e},
  volume={1},
  number={7},
  pages={1974--1975},
  year={1974},
  publisher={Secr{\'e}tariat math{\'e}matique}
}

@misc{reed1972methods,
  title={Methods of mathematical physics I: Functional analysis},
  author={Reed, Michael and Simon, Barry},
  year={1972},
  publisher={Academic Press New York}
}

@book{casella2002statistical,
  title={Statistical inference},
  author={Casella, George and Berger, Roger L},
  volume={2},
  year={2002},
  publisher={Duxbury Pacific Grove, CA}
}

@book{dean1999design,
  title={Design and analysis of experiments},
  author={Dean, Angela and Voss, Daniel},
  volume={1},
  year={1999},
  publisher={Springer}
}

@book{gu2013smoothing,
  title={Smoothing spline ANOVA models},
  author={Gu, Chong},
  volume={297},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{kuo2010decompositions,
  title={On decompositions of multivariate functions},
  author={Kuo, F and Sloan, I and Wasilkowski, G and Wo{\'z}niakowski, Henryk},
  journal={Mathematics of computation},
  volume={79},
  number={270},
  pages={953--966},
  year={2010}
}

@article{sobol2001global,
  title={Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates},
  author={Sobol, Ilya M},
  journal={Mathematics and computers in simulation},
  volume={55},
  number={1-3},
  pages={271--280},
  year={2001},
  publisher={Elsevier}
}

@article{durrande2013anova,
  title={ANOVA kernels and RKHS of zero mean functions for model-based sensitivity analysis},
  author={Durrande, Nicolas and Ginsbourger, David and Roustant, Olivier and Carraro, Laurent},
  journal={Journal of Multivariate Analysis},
  volume={115},
  pages={57--67},
  year={2013},
  publisher={Elsevier}
}

@book{rudin1987real,
  title={Real and complex analysis},
  author={Rudin, Walter},
  year={1987},
  publisher={Tata McGraw-Hill Education}
}

@inproceedings{ong2004learning,
  title={Learning with non-positive kernels},
  author={Ong, Cheng Soon and Mary, Xavier and Canu, St{\'e}phane and Smola, Alexander J},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={81},
  year={2004},
  organization={ACM}
}

@phdthesis{mary2003hilbertian,
  title={Hilbertian subspaces, subdualities and applications},
  author={Mary, Xavier},
  year={2003},
  school={INSA de Rouen}
}

@article{alpay1991some,
  title={Some remarks on reproducing kernel Krein spaces},
  author={Alpay, Daniel},
  journal={The Rocky Mountain Journal of Mathematics},
  pages={1189--1205},
  year={1991},
  publisher={JSTOR}
}

@article{sejdinovic2012,
author = {Sejdinovic, Dino and Gretton, Arthur},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Sejdinovic, Gretton/COMPGI13 Advanced Topics in Machine Learning. Lecture conducted at University College London/Sejdinovic, Gretton - 2012 - Lecture notes What is an RKHS.pdf:pdf},
journal = {COMPGI13 Advanced Topics in Machine Learning. Lecture conducted at University College London},
pages = {1--24},
title = {{Lecture notes: What is an RKHS?}},
url = {http://www.gatsby.ucl.ac.uk/{~}gretton/coursefiles/RKHS{\_}Notes1.pdf},
year = {2012}
}

@inproceedings{zafeiriou2012subspace,
  title={Subspace learning in krein spaces: Complete kernel fisher discriminant analysis with indefinite kernels},
  author={Zafeiriou, Stefanos},
  booktitle={European Conference on Computer Vision},
  pages={488--501},
  year={2012},
  organization={Springer}
}

@book{balakrishnan1981applied,
  title={Applied Functional Analysis},
  author={Balakrishnan, Alampallam V},
  edition={2},
  volume={3},
  year={1981},
  publisher={Springer Science \& Business Media},
  doi={10.1007/978-1-4612-5865-0}
}

@article{bouboulis2011extension,
  title={Extension of Wirtinger's calculus to reproducing kernel Hilbert spaces and the complex kernel LMS},
  author={Bouboulis, Pantelis and Theodoridis, Sergios},
  journal={IEEE Transactions on Signal Processing},
  volume={59},
  number={3},
  pages={964--978},
  year={2011},
  publisher={IEEE}
}

@inproceeding{tapia1971diff,
  title={The differentiation and integration of nonlinear operators},
  author={Tapia, R A},
  booktitle={Nonlinear functional analysis and applications: Proceedings of an advanced seminar conducted by the Mathematics Research Center, the University of Wisconsin, Madison, October 12-14, 1970},
  editor={Rall, Louis B},
  pages={45--101},
  year={1971},
  publisher={Elsevier}
}


@misc{kammar2016,
author={Ohad Kammar},
title={A note on Fréchet diffrentiation under Lebesgue integrals
},
year={2016},
url={https://www.cs.ox.ac.uk/people/ohad.kammar/notes/kammar-a-note-on-frechet-differentiation-under-lebesgue-integrals.pdf}
}

@article{schoenberg1937,
  title={On certain metric spaces arising from Euclidean spaces by a change of metric and their imbedding in Hilbert space},
  author={Schoenberg, Isaac J},
  journal={Annals of mathematics},
  pages={787--793},
  year={1937},
  publisher={JSTOR}
}

@article{hein2004kernels,
  title={Kernels, associated structures and generalizations},
  author={Hein, Matthias and Bousquet, Olivier},
  journal={Max-Planck-Institut fuer biologische Kybernetik, Technical Report},
  year={2004}
}

@book{wahba1990spline,
  title={Spline models for observational data},
  author={Wahba, Grace},
  volume={59},
  year={1990},
  publisher={Siam}
}