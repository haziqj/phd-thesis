Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@misc{Nettleton2012,
author = {Nettleton, Dan},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Nettleton/Unknown/Nettleton - 2012 - Linear Mixed-Effects Models.pdf:pdf},
pages = {1--34},
title = {{Linear Mixed-Effects Models}},
year = {2012}
}
@article{Ntzoufras2002,
abstract = {In this paper we discuss and present in detail the implementation of Gibbs variable selection as defined by Dellaportas et al. (2000, 2002) using the BUGS software (Spiegelhalter et al. ,'96a,b,c). The specification of the likelihood, prior and pseudo-prior distributions of the parameters as well as the prior term and model probabilities are described in detail. Guidance is also provided for the calculation of the posterior probabilities within BUGS environment when the number of models is limited. We illustrate the application of this methodology in a variety of problems including linear regression, log-linear and binomial response models.},
author = {Ntzoufras, Ioannis},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Ntzoufras/Journal of Statistical Software/Ntzoufras - 2002 - Gibbs Variable Selection using BUGS.pdf:pdf},
issn = {<null>},
journal = {Journal of Statistical Software},
keywords = {linear regression,logistic regression,mcmc,model selec-},
number = {i07},
pages = {1--19},
title = {{Gibbs Variable Selection using BUGS}},
url = {http://ideas.repec.org/a/jss/jstsof/07i07.html%5Cnpapers2://publication/uuid/4A46EAD4-E5FD-4634-A6CE-4BC64491A691},
volume = {07},
year = {2002}
}
@article{Durrant2013,
author = {Durrant, Gabriele B. and D'Arrigo, Julia and Steele, Fiona},
doi = {10.1111/j.1467-985X.2012.01073.x},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Durrant, D'Arrigo, Steele/Journal of the Royal Statistical Society Series A (Statistics in Society)/Durrant, D'Arrigo, Steele - 2013 - Analysing interviewer call record data by using a multilevel discrete time event history modelling ap.pdf:pdf},
issn = {09641998},
journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
keywords = {event history analysis,interviewer call record data,multilevel multinomial logistic,paradata,regression,survey co-operation},
month = {jan},
number = {1},
pages = {251--269},
title = {{Analysing interviewer call record data by using a multilevel discrete time event history modelling approach}},
url = {http://doi.wiley.com/10.1111/j.1467-985X.2012.01073.x},
volume = {176},
year = {2013}
}
@book{steinwart2008,
author = {Steinwart, Ingo and Christmann, Andreas},
publisher = {Springer Science & Business Media},
title = {{Support vector machines}},
year = {2008}
}
@book{Bishop2006,
author = {Bishop, Christopher M.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bishop/Unknown/Bishop - 2006 - Pattern Recognition and Machine Learning.pdf:pdf},
publisher = {Springer},
title = {{Pattern Recognition and Machine Learning}},
year = {2006}
}
@article{Sinha2004,
author = {Sinha, Sanjoy K},
doi = {10.1198/016214504000000340},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Sinha/Journal of the American Statistical Association/Sinha - 2004 - Robust Analysis of Generalized Linear Mixed Models.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = {jun},
number = {466},
pages = {451--460},
title = {{Robust Analysis of Generalized Linear Mixed Models}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214504000000340},
volume = {99},
year = {2004}
}
@article{Bates2014,
abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.5823v1},
author = {Bates, Douglas M. and M{\"{a}}chler, Martin and Bolker, Benjamin M and Walker, Steven C},
doi = {10.1177/009286150103500418},
eprint = {arXiv:1406.5823v1},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bates et al/arXiv1406.5823v1stat.CO23/Bates et al. - 2014 - Fitting linear mixed-effects models using lme4.pdf:pdf},
isbn = {%(},
issn = {0092-8615},
journal = {arXiv:1406.5823v1[stat.CO]23},
keywords = {cholesky,linear mixed models,penalized least squares,sparse matrix methods},
pages = {1 -- 51},
title = {{Fitting linear mixed-effects models using lme4}},
year = {2014}
}
@misc{ArrhythmiaData,
author = {Guvenir, H. Altay and {Burak Acar}, M.S. and Muderrisoglu, Haldun},
institution = {University of California, Irvine, School of Information and Computer Sciences},
title = {{UCI Machine Learning Repository: Arrhythmia Data Set}},
url = {https://archive.ics.uci.edu/ml/datasets/Arrhythmia},
year = {1998}
}
@article{Hoeting1999,
abstract = {Standard statistical practice ignores model uncertainty. Data analysts typically select a model from some class of models and then proceed as if the selected model had generated the data. This approach ignores the uncertainty in model selection, leading to over-confident inferences and decisions that are more risky than one thinks they are. Bayesian model averaging (BMA) provides a coherent mechanism for accounting for this model uncertainty. Several methods for implementing BMA have recently emerged. We discuss these methods and present a number of examples. In these examples, BMA provides improved out-of-sample predictive performance. We also provide a catalogue of currently available BMA software},
author = {Hoeting, J a and Madigan, D and Raftery, Adrian E and Volinsky, C T},
doi = {10.2307/2676803},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Hoeting et al/Statistical Science/Hoeting et al. - 1999 - Bayesian model averaging A tutorial.pdf:pdf},
isbn = {0883-4237},
issn = {08834237},
journal = {Statistical Science},
keywords = {Bayesian,Bayesian graphical models,Bayesian model averaging,COMPUTATION,Colorado,GIBBS SAMPLER,GRAPHICAL MODELS,Inference,LINEAR-REGRESSION,MODELS,Markov chain Monte Carlo,Methods,PREDICTION,PRIMARY BILIARY-CIRRHOSIS,REGRESSION-MODELS,SELECTION,Software,Time,Uncertainty,VARIABLE SELECTION,Washington,learning,model uncertainty,tutorial},
number = {4},
pages = {382--401},
title = {{Bayesian model averaging: A tutorial}},
url = {isi:000086094500003%5Cnfile://c/Article database/hoeting1999.pdf},
volume = {14},
year = {1999}
}
@article{Duff:2013,
annote = {revision #153309},
author = {Duff, I and U{\c{c}}ar, B},
journal = {Scholarpedia},
number = {10},
pages = {9700},
title = {{Direct methods for sparse matrix solution}},
url = {http://dx.doi.org/10.4249/scholarpedia.9700},
volume = {8},
year = {2013}
}
@article{Moustaki2000,
author = {Moustaki, Irini and Knott, Martin},
doi = {10.1111/1467-985X.00177},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Moustaki, Knott/Journal of the Royal Statistical Society Series A (Statistics in Society)/Moustaki, Knott - 2000 - Weighting for item non-response in attitude scales by using latent variable models with covariates.pdf:pdf},
issn = {0964-1998},
journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
keywords = {latent trait models,non-ignorable non-response,weighting},
month = {oct},
number = {3},
pages = {445--459},
title = {{Weighting for item non-response in attitude scales by using latent variable models with covariates}},
url = {http://doi.wiley.com/10.1111/1467-985X.00177},
volume = {163},
year = {2000}
}
@article{Kuo1998,
author = {Kuo, L and Mallick, B},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Kuo, Mallick/Sankhya The Indian Journal of Statistics, Series B/Kuo, Mallick - 1998 - Variable selection for regression models.pdf:pdf},
journal = {Sankhya: The Indian Journal of Statistics, Series B},
number = {1},
pages = {65--81},
title = {{Variable selection for regression models}},
volume = {60},
year = {1998}
}
@article{Chu2005,
abstract = {We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation prop-agation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative filtering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach.},
author = {Chu, Wei and Ghahramani, Zoubin},
doi = {10.1145/1102351.1102369},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Chu, Ghahramani/Journal of Machine Learning Research/Chu, Ghahramani - 2005 - Gaussian Processes for Ordinal Regression Zoubin Ghahramani.pdf:pdf},
isbn = {1595931805},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Gaussian processes,approximate Bayesian inference,collaborative filtering,feature selection,gene expression analysis,ordinal regression},
pages = {1019--1041},
pmid = {1889912417787065159},
title = {{Gaussian Processes for Ordinal Regression Zoubin Ghahramani}},
url = {http://www.jmlr.org/papers/volume6/chu05a/chu05a.pdf},
volume = {6},
year = {2005}
}
@article{Jaynes1957a,
abstract = {Treatment of the predictive aspect of statistical mechanics as a form of statistical inference is extended to the density-matrix formalism and applied to a discussion of the relation between irreversibility and information loss. A principle of "statistical complementarity" is pointed out, according to which the empirically verifiable probabilities of statistical mechanics necessarily correspond to incomplete predictions. A preliminary discussion is given of the second law of thermodynamics and of a certain class of irreversible processes, in an approximation equivalent to that of the semiclassical theory of radiation.\nIt is shown that a density matrix does not in general contain all the information about a system that is relevant for predicting its behavior. In the case of a system perturbed by random fluctuating fields, the density matrix cannot satisfy any differential equation because $\rho$̇(t) does not depend only on $\rho$(t), but also on past conditions The rigorous theory involves stochastic equations in the type $\rho$(t)=G(t, 0)$\rho$(0), where the operator G is a functional of conditions during the entire interval (0→t). Therefore a general theory of irreversible processes cannot be based on differential rate equations corresponding to time-proportional transition probabilities. However, such equations often represent useful approximations.},
author = {Jaynes, Edwin T.},
doi = {10.1103/PhysRev.108.171},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Jaynes/Physical Review/Jaynes - 1957 - Information theory and statistical mechanics II.pdf:pdf},
isbn = {1536-6065},
issn = {0031899X},
journal = {Physical Review},
number = {2},
pages = {171--190},
pmid = {17798674},
title = {{Information theory and statistical mechanics II}},
volume = {108},
year = {1957}
}
@techreport{Betancourt2014,
author = {Betancourt, Michael},
institution = {Machine Learning Summer School (Iceland 2014)},
title = {{Efficient Bayesian inference with Hamiltonian Monte Carlo}},
url = {https://www.youtube.com/watch?v=pHsuIaPbNbY},
year = {2014}
}
@article{Lee2007,
author = {Lee, Youngjo and Nelder, John A. and Noh, Maengseok},
doi = {10.1007/s11222-006-9006-7},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Lee, Nelder, Noh/Statistics and Computing/Lee, Nelder, Noh - 2007 - H-likelihood problems and solutions.pdf:pdf},
issn = {0960-3174},
journal = {Statistics and Computing},
keywords = {generalized linear models,h-likelihood,hierarchical,hierarchical-likelihood,marginal likelihood,random effects},
month = {feb},
number = {1},
pages = {49--55},
title = {{H-likelihood: problems and solutions}},
url = {http://link.springer.com/10.1007/s11222-006-9006-7},
volume = {17},
year = {2007}
}
@book{Jiang2007,
author = {Jiang, Jiming},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Jiang/Unknown/Jiang - 2007 - Linear and generalized linear mixed models and their applications.pdf:pdf},
isbn = {9780387479415},
title = {{Linear and generalized linear mixed models and their applications}},
url = {http://books.google.com/books?hl=en&lr=&id=aUFHAAAAQBAJ&oi=fnd&pg=PR7&dq=Linear+and+Generalized+Linear+Mixed+Models+and+Their+Applications&ots=USbTY3XnMN&sig=4C51uxZ-ZwXx2VkKHcKNWVKXXoM},
year = {2007}
}
@article{OHara2009,
author = {O'Hara, R B and Sillanp{\"{a}}{\"{a}}, M J},
doi = {10.1214/09-BA403},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/O'Hara, Sillanp{\"{a}}{\"{a}}/Bayesian Analysis/O'Hara, Sillanp{\"{a}}{\"{a}} - 2009 - A review of Bayesian variable selection methods what, how and which.pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {bugs,mcmc,variable selection},
number = {1},
pages = {85--117},
title = {{A review of Bayesian variable selection methods: what, how and which}},
url = {http://projecteuclid.org/euclid.ba/1340370391},
volume = {4},
year = {2009}
}
@article{Hankin2007,
author = {Hankin, Robin K S},
journal = {R News},
number = {3},
pages = {15--16},
title = {{Very Large Numbers in R: Introducing Package Brobdingnag.}},
url = {http://cran.r-project.org/doc/Rnews/.},
volume = {7},
year = {2007}
}
@misc{Wood,
abstract = {Lecture. University of Bath.},
author = {Wood, Simon},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Wood/Unknown/Wood - Unknown - (Generalized) Linear Mixed Models.pdf:pdf},
title = {{(Generalized) Linear Mixed Models}}
}
@book{Verbeke2010,
address = {Berlin, Heidelberg},
author = {Verbeke, Geert and Molenberghs, Geert and Rizopoulos, Dimitris},
doi = {10.1007/978-3-642-11760-2},
editor = {van Montfort, Kees and Oud, Johan H.L. and Satorra, Albert},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Verbeke, Molenberghs, Rizopoulos/Unknown/Verbeke, Molenberghs, Rizopoulos - 2010 - Longitudinal Research with Latent Variables.pdf:pdf},
isbn = {978-3-642-11759-6},
publisher = {Springer Berlin Heidelberg},
title = {{Longitudinal Research with Latent Variables}},
url = {http://link.springer.com/10.1007/978-3-642-11760-2},
year = {2010}
}
@article{Jaynes1968,
abstract = {In decision theory, mathematical analysis shows that once the sampling distribution, loss function, and sample are specified, the only remaining basis for a choice among different admissible decisions lies in the prior probabilities. Therefore, the logical foundations of decision theory cannot be put in fully satisfactory form until the old problem of arbitrariness (sometimes called "subjectiveness") in assigning prior probabilities is resolved. The principle of maximum entropy represents one step in this direction. Its use is illustrated, and a correspondence property between maximum-entropy probabilities and frequencies is demonstrated. The consistency of this principle with the principles of conventional "direct probability" analysis is illustrated by showing that many known results may be derived by either method. However, an ambiguity remains in setting up a prior on a continuous parameter space because the results lack invariance under a change of parameters; thus a further principle is needed. It is shown that in many problems, including some of the most important in practice, this ambiguity can be removed by applying methods of group theoretical reasoning which have long been used in theoretical physics. By finding the group of transformations on the parameter space which convert the problem into an equivalent one, a basic desideratum of consistency can be stated in the form of functional equations which impose conditions on, and in some cases fully determine, an "invariant measure" on the parameter space.},
author = {Jaynes, Edwin T.},
doi = {10.1109/TSSC.1968.300117},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Jaynes/IEEE Transactions on Systems Science and Cybernetics/Jaynes - 1968 - Prior Probabilities.pdf:pdf},
isbn = {0536-1567},
issn = {0536-1567},
journal = {IEEE Transactions on Systems Science and Cybernetics},
number = {3},
pages = {227--241},
title = {{Prior Probabilities}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4082152},
volume = {4},
year = {1968}
}
@book{Jamil2017iprobit,
address = {R Package version 0.1.0},
author = {HJ},
publisher = {GitHub},
title = {{iprobit: Binary Probit Regression with I-priors}},
year = {2017}
}
@book{Skrondal2004,
abstract = {This book unifies and extends latent variable models, including multilevel or generalized linear mixed models, longitudinal or panel models, item response or factor models, latent class or finite mixture models, and structural equation models. Following a gentle introduction to latent variable modeling, the authors clearly explain and contrast a wide range of estimation and prediction methods from biostatistics, psychometrics, econometrics, and statistics. They present exciting and realistic applications that demonstrate how researchers can use latent variable modeling to solve concrete problems in areas as diverse as medicine, economics, and psychology. The examples considered include many nonstandard response types, such as ordinal, nominal, count, and survival data. Joint modeling of mixed responses, such as survival and longitudinal data, is also illustrated. Numerous displays, figures, and graphs make the text vivid and easy to read.},
author = {Skrondal, Anders and Rabe-Hesketh, Sophia},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Skrondal, Rabe-Hesketh/Unknown/Skrondal, Rabe-Hesketh - 2004 - Generalized Latent Variable Modeling Multilevel, Longitudinal, and Structural Equation Models.pdf:pdf},
publisher = {Chapman & Hall/CRC},
title = {{Generalized Latent Variable Modeling: Multilevel, Longitudinal, and Structural Equation Models}},
year = {2004}
}
@article{Chipman2001,
author = {Chipman, Hugh and George, Edward I and McCulloch, Robert E},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Chipman, George, McCulloch/IMS Lecture Notes - Monograph Series/Chipman, George, McCulloch - 2001 - The Practical Implementation of Bayesian Model Selection.pdf:pdf},
journal = {IMS Lecture Notes - Monograph Series},
pages = {65--134},
title = {{The Practical Implementation of Bayesian Model Selection}},
volume = {38},
year = {2001}
}
@article{Mountford-Zimdars2013b,
abstract = {This article focuses on questions and attitudes towards higher education in the British Social Attitudes (BSA) survey series. First, we analyse the changing BSA questions (1983?2010) in the context of key policy reports. Our results show that changes in the framing of higher education questions correspond with changes in the macro-discourse of higher education policies. Second, we focus on the 2010 BSA survey responses to investigate how attitudes towards higher education are related to respondents? characteristics. Respondents? socio-economic position predicts attitudes towards higher education. Graduates and professionals are most likely to support a reduction in higher education opportunities, but those who have so far benefitted least from higher education are supportive of expansion. One interpretation ? with potential implications for social mobility ? is that those who have already benefited from higher education are most inclined to pull the ladder up behind them. This article focuses on questions and attitudes towards higher education in the British Social Attitudes (BSA) survey series. First, we analyse the changing BSA questions (1983?2010) in the context of key policy reports. Our results show that changes in the framing of higher education questions correspond with changes in the macro-discourse of higher education policies. Second, we focus on the 2010 BSA survey responses to investigate how attitudes towards higher education are related to respondents? characteristics. Respondents? socio-economic position predicts attitudes towards higher education. Graduates and professionals are most likely to support a reduction in higher education opportunities, but those who have so far benefitted least from higher education are supportive of expansion. One interpretation ? with potential implications for social mobility ? is that those who have already benefited from higher education are most inclined to pull the ladder up behind them.},
author = {Mountford-Zim dars, Anna and Jones, Steven and Sullivan, Alice and Heath, Anthony},
doi = {10.1080/01425692.2013.816040},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Mountford-Zim dars et al/British Journal of Sociology of Education/Mountford-Zim dars et al. - 2013 - Framing higher education questions and responses in the British Social Attitudes survey, 1983–2010.pdf:pdf},
issn = {0142-5692},
journal = {British Journal of Sociology of Education},
keywords = {access,attitudes,higher education,public policy,social class,survey questions},
month = {nov},
number = {5-06},
pages = {792--811},
publisher = {Routledge},
title = {{Framing higher education: questions and responses in the British Social Attitudes survey, 1983–2010}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01425692.2013.816040 http://dx.doi.org/10.1080/01425692.2013.816040},
volume = {34},
year = {2013}
}
@article{Skrondal2003,
abstract = {We describe generalized linear latent and mixed models (GLLAMMs) and illustrate their potential in epidemiology. GLLAMMs include many types of multilevel random effect, factor and structural equation models. A wide range of response types are accommodated including continuous, dichoto- mous, ordinal and nominal responses as well as counts and survival times. Multivariate responses can furthermore be of mixed types. The utility of GLLAMMs is illustrated in three applications involving repeated measurements, measurement error and multilevel data.},
author = {Skrondal, Anders and Rabe-Hesketh, S},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Skrondal, Rabe-Hesketh/Norsk epidemiologi/Skrondal, Rabe-Hesketh - 2003 - Some applications of generalized linear latent and mixed models in epidemiology repeated measures, measu.pdf:pdf},
journal = {Norsk epidemiologi},
number = {2},
pages = {265--278},
title = {{Some applications of generalized linear latent and mixed models in epidemiology: repeated measures, measurement error and multilevel modeling}},
url = {http://www.ntnu.no/ojs/index.php/norepid/article/download/295/273},
volume = {13},
year = {2003}
}
@misc{Thas2009,
author = {Thas, Olivier and {De Neve}, Jan and Clement, Lieven and Ottoy, Jean-pierre},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Thas et al/Unknown/Thas et al. - 2009 - Comment Probabilistic index models.pdf:pdf},
number = {1},
pages = {1--2},
title = {{Comment: Probabilistic index models}},
volume = {1},
year = {2009}
}
@misc{Firth2013,
abstract = {Introduction to composite likelihood methods at the ILike Launch Event, Oxford (31/1/2013)},
author = {Firth, David},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Firth/Unknown/Firth - 2013 - Plan Composite Likelihood Methods.pdf:pdf},
title = {{Plan Composite Likelihood Methods}},
year = {2013}
}
@article{Jr1997,
abstract = {The Analytic Hierarchy Process (AHP) is a decision analysis technique used to evaluate complex multiattribute alternatives among one or more decision makers. It imposes a hierarchical structure on any complex multicriterion problem. However, a major drawback of the AHP is that a large number of pairwise comparisons is needed to calibrate the hierarchy. When there are a few levels and sublevels, the AHP can be applied in a straightforward manner to derive the weights (relative preference for each alternative). As the size of the hierarchy increases, the number of pairwise comparisons increases rapidly. It is well established in the marketing and consumer behavior literature that in a very long interview, even under the best circumstances, the respondent is likely to suffer from information overload. Recognition of this problem was the motivation which led to the investigation of a modification of AHP which required less data collection, i.e., a reduction in the threat of information overload. The first question to be answered is the effect on AHP weights due to different patterns of missing data likely to result from reallife data collection. In this study, a Monte Carlo simulation was conducted, which uses the Incomplete Pairwise Comparisons (IPC) algorithm [14], to investigate the effect of reduced sets of pairwise comparisons in the AHP. Data for the study were generated with known structure and comparisons made between complete and incomplete matrices. The results of the simulation suggest that incomplete sets of pairwise comparison matrices can capture the attribute level weights without significant loss of accuracy and independent of decision model (form and amount of error) considered.},
author = {Jr, FJ Carmone and Kara, Ali and Zanakis, SH},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Jr, Kara, Zanakis/European Journal of Operational {\ldots}/Jr, Kara, Zanakis - 1997 - A Monte Carlo investigation of incomplete pairwise comparison matrices in AHP.pdf:pdf},
journal = {European Journal of Operational {\ldots}},
keywords = {1,a h p,ahp,c hierarchy process,decision theory,e c o m,i n t r,information overload,is a deci-,l u a t,l y t i,multiple criteria,o d u c,p l e x,sion analysis technique used,t i o n,the a n a,to e v a},
number = {3},
pages = {538--553},
title = {{A Monte Carlo investigation of incomplete pairwise comparison matrices in AHP}},
url = {http://www.sciencedirect.com/science/article/pii/S0377221796002500},
volume = {102},
year = {1997}
}
@article{Minin2005,
abstract = {MOTIVATION: We introduce a dual multiple change-point (MCP) model for recombination detection among aligned nucleotide sequences. The dual MCP model is an extension of the model introduced previously by Suchard and co-workers. In the original single MCP model, one change-point process is used to model spatial phylogenetic variation. Here, we show that using two change-point processes, one for spatial variation of tree topologies and the other for spatial variation of substitution process parameters, increases recombination detection accuracy. Statistical analysis is done in a Bayesian framework using reversible jump Markov chain Monte Carlo sampling to approximate the joint posterior distribution of all model parameters.

RESULTS: We use primate mitochondrial DNA data with simulated recombination break-points at specific locations to compare the two models. We also analyze two real HIV sequences to identify recombination break-points using the dual MCP model.},
author = {Minin, Vladimir N and Dorman, Karin S and Fang, Fang and Suchard, Marc a},
doi = {10.1093/bioinformatics/bti459},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Minin et al/Bioinformatics (Oxford, England)/Minin et al. - 2005 - Dual multiple change-point model leads to more accurate recombination detection.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Chromosome Mapping,Chromosome Mapping: methods,Computer Simulation,DNA Mutational Analysis,DNA Mutational Analysis: methods,DNA, Mitochondrial,DNA, Mitochondrial: genetics,Evolution, Molecular,Models, Genetic,Models, Statistical,Recombination, Genetic,Recombination, Genetic: genetics,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Software},
month = {jul},
number = {13},
pages = {3034--42},
pmid = {15914546},
title = {{Dual multiple change-point model leads to more accurate recombination detection.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15914546},
volume = {21},
year = {2005}
}
@misc{Lauritzen2007,
abstract = {Lectures 6 and 7. MSc Further Statistical Methods, University of Oxford.},
author = {Lauritzen, Steffen},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Lauritzen/Unknown/Lauritzen - 2007 - Latent Variable Models and Factor Analysis.pdf:pdf},
title = {{Latent Variable Models and Factor Analysis}},
year = {2007}
}
@article{Sha2004,
abstract = {Here we focus on discrimination problems where the number of predictors substantially exceeds the sample size and we propose a Bayesian variable selection approach to multinomial probit models. Our method makes use of mixture priors and Markov chain Monte Carlo techniques to select sets of variables that differ among the classes. We apply our methodology to a problem in functional genomics using gene expression profiling data. The aim of the analysis is to identify molecular signatures that characterize two different stages of rheumatoid arthritis.},
author = {Sha, Naijun and Vannucci, Marina and Tadesse, Mahlet G. and Brown, Philip J. and Dragoni, Ilaria and Davies, Nick and Roberts, Tracy C. and Contestabile, Andrea and Salmon, Mike and Buckley, Chris and Falciani, Francesco},
doi = {10.1111/j.0006-341X.2004.00233.x},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Sha et al/Biometrics/Sha et al. - 2004 - Bayesian variable selection in multinomial probit models to identify molecular signatures of disease stage.pdf:pdf},
isbn = {0006-341X (Print)\r0006-341X (Linking)},
issn = {0006341X},
journal = {Biometrics},
keywords = {Bayesian variable selection,DNA microarrays,Discrimination,Latent variables,MCMC,Multinomial probit model,Truncated sampling},
number = {3},
pages = {812--819},
pmid = {15339306},
title = {{Bayesian variable selection in multinomial probit models to identify molecular signatures of disease stage}},
volume = {60},
year = {2004}
}
@article{Blei2016,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability distributions. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation about the posterior. In this paper, we review variational inference (VI), a method from machine learning that approximates probability distributions through optimization. VI has been used in myriad applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of distributions and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this widely-used class of algorithms.},
archivePrefix = {arXiv},
arxivId = {1601.00670},
author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
eprint = {1601.00670},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Blei, Kucukelbir, McAuliffe/Unknown/Blei, Kucukelbir, McAuliffe - 2016 - Variational Inference A Review for Statisticians.pdf:pdf},
keywords = {Graphical Model,Variational Inference},
title = {{Variational Inference: A Review for Statisticians}},
year = {2016}
}
@article{Cannings2017,
author = {Cannings, Timothy I and Samworth, Richard J},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Cannings, Samworth/JRSSBd/Cannings, Samworth - 2017 - Random-projection ensemble classification.pdf:pdf},
journal = {\JRSSBd},
keywords = {aggregation,classification,high dimensional classification,random projection},
title = {{Random-projection ensemble classification}},
volume = {to appear},
year = {2017}
}
@book{SAS2008,
address = {Cary, NC},
author = {{SAS Institute Inc.}},
edition = {2nd},
isbn = {978-1-60764-566-5},
publisher = {SAS Institute Inc.},
title = {{SAS/STAT(R) 9.2 User's Guide}},
url = {https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_reg_sect055.htm},
year = {2008}
}
@article{Gelman2006,
author = {Gelman, Andrew},
doi = {10.1198/000313006X106190},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Gelman/The American Statistician/Gelman - 2006 - The boxer, the wrestler, and the coin flip A paradox of robust Bayesian inference and belief functions - response.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {aleatory uncertainty,bust bayes,dempster-shafer theory,epistemic and,foundations of probability,ignorance,ro-,subjective prior distribution},
pages = {347--347},
title = {{The boxer, the wrestler, and the coin flip: A paradox of robust Bayesian inference and belief functions - response}},
url = {http://apps.isiknowledge.com/full_record.do?product=UA&search_mode=GeneralSearch&qid=1&SID=N1PCO@82boCIgC9p17f&page=1&doc=1&colname=WOS&cacheurlFromRightClick=no%5Cnhttp://pubs.amstat.org/doi/abs/10.1198/000313006X106190},
volume = {60},
year = {2006}
}
@article{Cui2008,
abstract = {For the problem of variable selection for the normal linear model, fixed penalty selection criteria such as AIC, Cp, BIC and RIC correspond to the posterior modes of a hierarchical Bayes model for various fixed hyperparameter settings. Adaptive selection criteria obtained by empirical Bayes estimation of the hyperparameters have been shown by George and Foster [2000. Calibration and Empirical Bayes variable selection. Biometrika 87(4), 731-747] to improve on these fixed selection criteria. In this paper, we study the potential of alternative fully Bayes methods, which instead margin out the hyperparameters with respect to prior distributions. Several structured prior formulations are considered for which fully Bayes selection and estimation methods are obtained. Analytical and simulation comparisons with empirical Bayes counterparts are studied. ?? 2007 Elsevier B.V. All rights reserved.},
author = {Cui, Wen and George, Edward I.},
doi = {10.1016/j.jspi.2007.02.011},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Cui, George/Journal of Statistical Planning and Inference/Cui, George - 2008 - Empirical Bayes vs. fully Bayes variable selection.pdf:pdf},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Hyperparameter uncertainty,Model selection,Objective Bayes},
number = {4},
pages = {888--900},
title = {{Empirical Bayes vs. fully Bayes variable selection}},
volume = {138},
year = {2008}
}
@incollection{Lawrence2005,
abstract = {The informative vector machine (IVM) is a practical method for Gaussian process regression and classification. The IVM produces a sparse approximation to a Gaussian process by combining assumed density filtering with a heuristic for choosing points based on minimizing posterior entropy. This paper extends IVM in several ways. First, we propose a novel noise model that allows the IVM to be applied to a mixture of labeled and unlabeled data. Second, we use IVM on a block- diagonal covariance matrix, for “learning to learn” from related tasks. Third, we modify the IVM to incorporate prior knowledge from known invariances. All of these extensions are tested on artificial and real data.},
author = {Lawrence, Neil D. and Platt, John C. and Jordan, Michael I.},
booktitle = {Deterministic and Statistical Methods in Machine Learning},
doi = {10.1007/11559887_4},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Lawrence, Platt, Jordan/Deterministic and Statistical Methods in Machine Learning/Lawrence, Platt, Jordan - 2005 - Extensions of the informative vector machine.pdf:pdf},
isbn = {3540290737},
issn = {03029743},
pages = {56--87},
publisher = {Springer Berlin Heidelberg},
title = {{Extensions of the informative vector machine}},
year = {2005}
}
@article{Ishwaran2005,
abstract = {Variable selection in the linear regression model takes many apparent faces from both frequentist and Bayesian standpoints. In this paper we introduce a variable selection method referred to as a rescaled spike and slab model. We study the importance of prior hierarchical specifications and draw connections to frequentist generalized ridge regression estimation. Specifically, we study the usefulness of continuous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization. Several model selection strategies, some frequentist and some Bayesian in nature, are developed and studied theoretically. We demonstrate the importance of selective shrinkage for effective variable selection in terms of risk misclassification, and show this is achieved using the posterior from a rescaled spike and slab model. We also show how to verify a procedure's ability to reduce model uncertainty in finite samples using a specialized forward selection strategy. Using this tool, we illustrate the effectiveness of rescaled spike and slab models in reducing model uncertainty.},
archivePrefix = {arXiv},
arxivId = {math/0505633},
author = {Ishwaran, Hemant and Rao, J. Sunil},
doi = {10.1214/009053604000001147},
eprint = {0505633},
file = {:Users/haziqjamil/Downloads/0505633.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Generalized ridge regression,Hypervariance,Model averaging,Model uncertainty,Ordinary least squares,Penalization,Rescaling,Shrinkage,Stochastic variable selection,Zcut},
number = {2},
pages = {730--773},
pmid = {3448605},
primaryClass = {math},
title = {{Spike and slab variable selection: Frequentist and bayesian strategies}},
volume = {33},
year = {2005}
}
@article{Eyduran2008,
author = {Eyduran, Ecevit},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Eyduran/Journal of Research in Medical Sciences/Eyduran - 2008 - Usage of Penalized Maximum Likelihood Estimation Method in Medical Research An Alternative to Maximum Likelihood Estima.pdf:pdf},
journal = {Journal of Research in Medical Sciences},
keywords = {13,325-330,6,bias shrinking,hi-square,jrms 2008,likelihood ratio chi-square,logistic regression,penalized maximum likelihood estimation},
number = {6},
title = {{Usage of Penalized Maximum Likelihood Estimation Method in Medical Research: An Alternative to Maximum Likelihood Estimation Method}},
url = {http://journals.mui.ac.ir/index.php/jrms/article/viewArticle/2206},
volume = {13},
year = {2008}
}
@article{Rudas2010,
author = {Rudas, T. and Bergsma, Wicher P and Nemeth, R.},
doi = {10.1093/biomet/asq037},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Rudas, Bergsma, Nemeth/Biometrika/Rudas, Bergsma, Nemeth - 2010 - Marginal log-linear parameterization of conditional independence models.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
keywords = {chain graph,conditional independence,graphical},
month = {aug},
number = {4},
pages = {1006--1012},
title = {{Marginal log-linear parameterization of conditional independence models}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/asq037},
volume = {97},
year = {2010}
}
@article{Steele2013,
abstract = {There has been substantial interest in the social and health sciences in the reciprocal causal influences that people in close relationships have on one another. Most research has considered reciprocal processes involving only 2 units, although many social relationships of interest occur within a larger group (e.g., families, work groups, peer groups, classrooms). This article presents a general longitudinal multilevel modeling framework for the simultaneous estimation of reciprocal relationships among individuals with unique roles operating in a social group. We use family data for illustrative purposes, but the model is generalizable to any social group in which measurements of individuals in the social group occur over time, individuals have unique roles, and clustering of the data is evident. We allow for the possibility that the outcomes of family members are influenced by a common set of unmeasured family characteristics. The multilevel model we propose allows for residual variation in the outcomes of parents and children at the occasion, individual, and family levels and residual correlation between parents and children due to the unmeasured shared environment, genetic factors, and shared measurement. Another advantage of this method over approaches used in previous family research is it can handle mixed family sizes. The method is illustrated in an analysis of maternal depression and child delinquency using data from the Avon Brothers and Sisters Study.},
author = {Steele, Fiona and Rasbash, Jon and Jenkins, Jennifer},
doi = {10.1037/a0029316},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Steele, Rasbash, Jenkins/Psychological methods/Steele, Rasbash, Jenkins - 2013 - A multilevel simultaneous equations model for within-cluster dynamic effects, with an application to r.pdf:pdf},
issn = {1939-1463},
journal = {Psychological methods},
month = {mar},
number = {1},
pages = {87--100},
pmid = {22799627},
title = {{A multilevel simultaneous equations model for within-cluster dynamic effects, with an application to reciprocal parent-child and sibling effects.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22799627},
volume = {18},
year = {2013}
}
@article{Kass1994,
abstract = {Abstract Subjectivism has become the dominant philosophical foundation for Bayesian infer- ence. Yet, in practice, most Bayesian analyses are performed with so-called \noninfor- mative" priors, that is, priors constructed by some formal rule. We review the plethora of ...},
author = {Kass, Robert E and Wasserman, Larry},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Kass, Wasserman/Interpretation A Journal Of Bible And Theology/Kass, Wasserman - 1994 - Formal Rules for Selecting Prior Distributions A Review and Annotated Bibliography.pdf:pdf},
journal = {Interpretation A Journal Of Bible And Theology},
keywords = {and phrases,bayes factors,coherence,data-translated likelihoods,en-,fisher information,haar measure,improper priors,insu cient reason,je reys,marginalization paradoxes,noninformative priors,nuisance parameters,prior,priors,reference,s,sensitivity analysis,tropy},
pages = {1--81},
title = {{Formal Rules for Selecting Prior Distributions : A Review and Annotated Bibliography}},
year = {1994}
}
@manual{Jamil2017,
address = {R Package version 0.6.4},
annote = {R package version 0.6.4.9003},
author = {HJ},
publisher = {CRAN/GitHub},
title = {{iprior: Linear Regression using I-Priors}},
year = {2017}
}
@article{Jiang1998,
author = {Jiang, Jiming},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Jiang/Journal of the American Statistical Association/Jiang - 1998 - Consistent estimators in generalized linear mixed models.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {consistency,generalized linear mixed models,method of simulated},
number = {442},
pages = {720--729},
title = {{Consistent estimators in generalized linear mixed models}},
url = {http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1998.10473724},
volume = {93},
year = {1998}
}
@article{Liang2008,
abstract = {Zellner's g prior remains a popular conventional prior for use in Bayesian variable selection, despite several undesirable consistency issues. In this article we study mixtures of g priors as an alternative to default g priors that resolve many of the problems with the original formulation while maintaining the computational tractability that has made the g prior so popular. We present theoretical properties of the mixture g priors and provide real and simulated examples to compare the mixture formulation with fixed g priors, empirical Bayes approaches, and other default procedures.},
author = {Liang, F and Paulo, R and Molina, G and Clyde, M. and Berger, James O},
doi = {Doi 10.1198/016214507000001337},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Liang et al/Journal of the American Statistical Association/Liang et al. - 2008 - Mixtures of g priors for Bayesian variable selection.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {aic,approximations,bayesian model averaging,bic,cauchy,criterion,empirical bayes,gaussian hypergeometric functions,linear-regression,matrix,model selection,multiple-regression,zellner-siow priors},
number = {481},
pages = {410--423},
title = {{Mixtures of g priors for Bayesian variable selection}},
volume = {103},
year = {2008}
}
@misc{ILike,
author = {ILike},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/ILike/Unknown/ILike - Unknown - Background to ILike.pdf:pdf},
pages = {1--5},
title = {{Background to ILike}}
}
@phdthesis{Katsikatsou2013,
author = {Katsikatsou, Myrsini},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Katsikatsou/Unknown/Katsikatsou - 2013 - Composite Likelihood Estimation for Latent Variable Models with Ordinal and Continuous , or Ranking Variables.pdf:pdf},
isbn = {9789155485719},
keywords = {factor analysis,latent variable models,structura},
title = {{Composite Likelihood Estimation for Latent Variable Models with Ordinal and Continuous , or Ranking Variables}},
year = {2013}
}
@article{Loader1992,
author = {Loader, Clive R},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Loader/The Annals of Statistics/Loader - 1992 - log linear model for a poisson process change point.pdf:pdf},
journal = {The Annals of Statistics},
number = {3},
pages = {1391--1411},
title = {log linear model for a poisson process change point},
url = {http://www.jstor.org/stable/2242017},
volume = {20},
year = {1992}
}
@article{Dow2004,
abstract = {Several recent studies of voter choice in multiparty elections point to the advantages of multinomial probit (MNP) relative to multinomial/conditional logit (MNL). We compare the MNP and MNL models and argue that the simpler logit is often preferable to the more complex probit for the study of voter choice in multi-party elections. Our argument rests on three areas of comparison between MNP and MNL. First, within the limits of typical data-a small sample of revealed voter choices among a few candidates or parties-neither model will clearly appear to have generated the observed data. Second, MNP is susceptible to a number of estimation problems, the most serious of which is that the MNP is often weakly identified in application. Weak identification is difficult to diagnose and may lead to plausible, yet arbitrary or misleading inferences. Finally, the logit model is criticized because it imposes the independence of irrelevant alternatives (IIA) property on voter choice. For most applications the IIA property is neither relevant nor particularly restrictive. We illustrate our arguments using data from recent US and French presidential elections. {\textcopyright} 2003 Elsevier Ltd. All rights reserved.},
author = {Dow, Jay K. and Endersby, James W.},
doi = {10.1016/S0261-3794(03)00040-4},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Dow, Endersby/Electoral Studies/Dow, Endersby - 2004 - Multinomial probit and multinomial logit A comparison of choice models for voting research.pdf:pdf},
isbn = {0261-3794},
issn = {02613794},
journal = {Electoral Studies},
number = {1},
pages = {107--122},
title = {{Multinomial probit and multinomial logit: A comparison of choice models for voting research}},
volume = {23},
year = {2004}
}
@article{Mukhopadhyay2011,
abstract = {Small area estimation is important in survey analysis when domain (subpopulation) sample sizes are too small to provide adequate precision for direct domain estimators. Popular techniques for small area estimation use implicit or explicit statistical models to indirectly estimate the small area parameters of interest. Indirect estimation requires you to go beyond the survey data analysis methods that are available in the SAS/STAT{\textregistered} survey procedures. This paper describes the use of the MIXED, IML, and MCMC procedures to fit unit-level and area-level models, and to obtain small area predictions and the mean squared error of predictions. Hierarchical Bayes models are also discussed as extensions to the basic models.},
author = {Mukhopadhyay, Pushpal K. and McDowell, Allen},
file = {:Users/haziqjamil/Downloads/336-2011.pdf:pdf},
journal = {SAS Global Forum 2011},
pages = {1--19},
title = {{Small Area Estimation for Survey Data Analysis Using SAS Software}},
year = {2011}
}
@misc{Bushmeneva2010,
author = {Bushmeneva, Ksenia},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bushmeneva/Unknown/Bushmeneva - 2010 - Local Polynomial Kernel Regression.pdf:pdf},
title = {{Local Polynomial Kernel Regression}},
year = {2010}
}
@article{Beal2006,
author = {Beal, M J and Ghahramani, Z},
doi = {DOI:10.1214/06-BA126},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Beal, Ghahramani/Bayesian Analysis/Beal, Ghahramani - 2006 - Variational Bayesian Learning of Directed Graphical Models with Hidden Variables.pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {approximate bayesian inference,bayes factors,directed acyclic,em algorithm,graphical models,graphs,markov chain monte carlo,model,selection,variational bayes},
number = {4},
pages = {793--832},
title = {{Variational Bayesian Learning of Directed Graphical Models with Hidden Variables}},
volume = {1},
year = {2006}
}
@article{Raftery1986,
author = {Raftery, Adrian E and Akman, V E},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Raftery, Akman/Biometrika/Raftery, Akman - 1986 - Bayesian analysis of a Poisson process with a change-point.pdf:pdf},
journal = {Biometrika},
title = {{Bayesian analysis of a Poisson process with a change-point}},
url = {http://www.jstor.org/stable/2336274},
year = {1986}
}
@article{McCulloch1997,
author = {McCulloch, Charles E.},
doi = {10.1080/01621459.1997.10473613},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/McCulloch/Journal of the American Statistical Association/McCulloch - 1997 - Maximum Likelihood Algorithms for Generalized Linear Mixed Models.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {importance sampling,metropolis-hastings algorithm,monte carlo em,newton-raphson algorithm,penalized,quasi-likelihood,simulated maximum likelihood},
month = {mar},
number = {437},
pages = {162--170},
title = {{Maximum Likelihood Algorithms for Generalized Linear Mixed Models}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1997.10473613},
volume = {92},
year = {1997}
}
@article{akaike1974new,
author = {Akaike, Hirotugu},
journal = {IEEE transactions on automatic control},
number = {6},
pages = {716--723},
publisher = {Ieee},
title = {{A new look at the statistical model identification}},
volume = {19},
year = {1974}
}
@article{Sheffi1982,
abstract = {Multinomial Probit provides, perhaps, the most satisfactory form of discrete choice model based on the framework of utility maximization. It has clear advantages over more commonly used alternatives, such as the multinomial logit model, although its use involves more computational effort. This paper discusses the problems both of calculating the multinomial probit choice functions and in estimating the model parameters. Four techniques for calculating the choice functions are reviewed with regard to their comparative accuracy, computational requirements and applicability. With regard to model estimation different calibration methods are reviewed and a simulation based algoirthm is presented and tested. ?? 1982.},
author = {Sheffi, Yosef and Hall, Randy and Daganzo, Carlos},
doi = {10.1016/0191-2607(82)90071-1},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Sheffi, Hall, Daganzo/Transportation Research Part A General/Sheffi, Hall, Daganzo - 1982 - On the estimation of the multinomial probit model.pdf:pdf},
issn = {01912607},
journal = {Transportation Research Part A: General},
number = {5-6},
pages = {447--456},
title = {{On the estimation of the multinomial probit model}},
volume = {16},
year = {1982}
}
@article{Plummer2014,
author = {Plummer, PJ and Chen, Jie},
doi = {10.1080/02664763.2013.840272},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Plummer, Chen/Journal of Applied Statistics/Plummer, Chen - 2014 - A Bayesian approach for locating change points in a compound Poisson process with application to detecting DNA co.pdf:pdf},
issn = {0266-4763},
journal = {Journal of Applied Statistics},
month = {sep},
number = {2},
pages = {423--438},
title = {{A Bayesian approach for locating change points in a compound Poisson process with application to detecting DNA copy number variations}},
url = {http://www.tandfonline.com/doi/abs/10.1080/02664763.2013.840272},
volume = {41},
year = {2014}
}
@article{Kuk1995,
author = {Kuk, Anthony Y. C.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Kuk/Journal of the Royal Statistical Society. Series B ( {\ldots}/Kuk - 1995 - Asymptotically unbiased estimation in generalized linear models with random effects.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B ( {\ldots}},
keywords = {bias correction,carlo method,consistent estimator,estimating equations,monte,random effects model},
number = {2},
pages = {395--407},
title = {{Asymptotically unbiased estimation in generalized linear models with random effects}},
url = {http://www.jstor.org/stable/2345969},
volume = {57},
year = {1995}
}
@article{Sejdinovic2012,
author = {Sejdinovic, Dino and Gretton, Arthur},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Sejdinovic, Gretton/COMPGI13 Advanced Topics in Machine Learning. Lecture conducted at University College London/Sejdinovic, Gretton - 2012 - Lecture notes What is an RKHS.pdf:pdf},
journal = {COMPGI13 Advanced Topics in Machine Learning. Lecture conducted at University College London},
pages = {1--24},
title = {{Lecture notes: What is an RKHS?}},
url = {http://www.gatsby.ucl.ac.uk/$\sim$gretton/coursefiles/RKHS_Notes1.pdf},
year = {2012}
}
@article{Fouskakis2008,
abstract = {Traditional variable-selection strategies in generalized linear models (GLMs) seek to optimize a measure of predictive accuracy without regard for the cost of data collection. When the purpose of such model building is the creation of predictive scales to be used in future studies with constrained budgets, the standard approach may not be optimal. We propose a Bayesian decision-theoretic framework for variable selection in binary-outcome GLMs where the budget for data collection is constrained and potential predictors may vary considerably in cost. The method is illustrated using data from a large study of quality of hospital care in the U.S. in the 1980s. Especially when the number of available predictors p is large, it is important to use an appropriate technique for optimization (e.g., in an application presented here where p = 83, the space over which we search has 283 .= 1025 elements, which is too large to explore using brute force enumeration). Specifically, we investigate simulated annealing (SA), genetic algorithms (GAs), and the tabu search (TS) method used in operations research, and we develop a context-specific version of SA, improved simulated annealing (ISA), that performs better than its generic counterpart. When p was modest in our study, we found that GAs performed relatively poorly for all but the very best user-defined input configurations, generic SA did not perform well, and TS had excellent median performance and was much less sensitive to suboptimal choice of user-defined inputs. When p was large in our study, the best versions of GA and ISA outperformed TS and generic SA. Our results are presented in the context of health policy but can apply to other quality assessment settings with dichotomous outcomes as well.},
author = {Fouskakis, Dimitris and Draper, David},
doi = {10.1198/016214508000001048},
file = {:Users/haziqjamil/Downloads/016214508000001048.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {annealing,bayesian decision theory,cross-validation,genetic algorithm,input,logistic regression,maximization,monte carlo methods,of expected utility,output analysis,prediction,quality of health care,sickness at hospital admission,simulated,tabu search,variable selection},
number = {484},
pages = {1367--1381},
title = {{Comparing Stochastic Optimization Methods for Variable Selection in Binary Outcome Prediction, With Application to Health Policy}},
volume = {103},
year = {2008}
}
@article{Steele1996,
author = {Steele, BM},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Steele/Biometrics/Steele - 1996 - A modified EM algorithm for estimation in generalized mixed models.pdf:pdf},
journal = {Biometrics},
number = {4},
pages = {1295--1310},
title = {{A modified EM algorithm for estimation in generalized mixed models}},
url = {http://www.jstor.org/stable/2532845},
volume = {52},
year = {1996}
}
@article{Bergsma2017,
author = {Bergsma, Wicher},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bergsma/Manuscript in preparation/Bergsma - 2017 - Regression with I-priors.pdf:pdf},
journal = {Manuscript in preparation},
title = {{Regression with I-priors}},
year = {2017}
}
@article{Qin2007,
author = {Qin, Jing and Zhang, Biao},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Qin, Zhang/Journal of the Royal Statistical Society Series {\ldots}/Qin, Zhang - 2007 - Empirical-likelihood-based inference in missing response problems and its application in observational studies.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series {\ldots}},
keywords = {auxiliary information,average treatment effect,biased sampling,causal,empirical likelihood,inference,missing data,observational studies,propensity score,survey},
pages = {101--122},
title = {{Empirical-likelihood-based inference in missing response problems and its application in observational studies}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2007.00579.x/full},
year = {2007}
}
@article{Borman2009,
abstract = {This tutorial discusses the ExpectationMaximization (EM) algorithm of Demp- ster, Laird and Rubin 1. The approach taken follows that of an unpublished note by Stuart Russel, but fleshes out some of the gory details. In order to ensure that the presentation is reasonably self-contained, some of the results on which the derivation of the algorithm is based are presented prior to the main results. The EM algorithm has become a popular tool in statistical estimation problems involving incomplete data, or in problems which can be posed in a sim- ilar form, such as mixture estimation 3, 4. The EM algorithm has also been used in various motion estimation frameworks 5 and variants of it have been used in multiframe superresolution restoration methods which combine motion estimation along the lines of 2.},
author = {Borman, Sean},
doi = {10.1097/RLU.0b013e3181b06c41\r00003072-200909000-00002},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Borman/Submitted for publication/Borman - 2009 - The Expectation Maximization Algorithm A short tutorial.pdf:pdf},
issn = {15360229},
journal = {Submitted for publication},
pages = {1--9},
pmid = {19692813},
title = {{The Expectation Maximization Algorithm A short tutorial}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.150.8193&rep=rep1&type=pdf},
volume = {25},
year = {2009}
}
@article{Solomon1992,
author = {Solomon, P J and Cox, D R},
doi = {10.2307/2337142},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Solomon, Cox/Biometrika/Solomon, Cox - 1992 - Nonlinear Component of Variance Models.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
month = {mar},
number = {1},
pages = {1},
title = {{Nonlinear Component of Variance Models}},
url = {http://www.jstor.org/stable/2337142?origin=crossref},
volume = {79},
year = {1992}
}
@book{Wallace2005,
abstract = {The Minimum Message Length (MML) Principle is an information-theoretic approach to induction, hypothesis testing, model selection, and statistical inference. MML, which provides a formal specification for the implementation of Occam's Razor, asserts that the ‘best' explanation of observed data is the shortest. Further, an explanation is acceptable (i.e. the induction is justified) only if the explanation is shorter than the original data. This book gives a sound introduction to the Minimum Message Length Principle and its applications, provides the theoretical arguments for the adoption of the principle, and shows the development of certain approximations that assist its practical application. MML appears also to provide both a normative and a descriptive basis for inductive reasoning generally, and scientific induction in particular. The book describes this basis and aims to show its relevance to the Philosophy of Science. Statistical and Inductive Inference by Minimum Message Length will be of special interest to graduate students and researchers in Machine Learning and Data Mining, scientists and analysts in various disciplines wishing to make use of computer techniques for hypothesis discovery, statisticians and econometricians interested in the underlying theory of their discipline, and persons interested in the Philosophy of Science. The book could also be used in a graduate-level course in Machine Learning and Estimation and Model-selection, Econometrics and Data Mining. C.S. Wallace was appointed Foundation Chair of Computer Science at Monash University in 1968, at the age of 35, where he worked until his death in 2004. He received an ACM Fellowship in 1995, and was appointed Professor Emeritus in 1996. Professor Wallace made numerous significant contributions to diverse areas of Computer Science, such as Computer Architecture, Simulation and Machine Learning. His final research focused primarily on the Minimum Message Length Principle.},
author = {Wallace, C S},
doi = {10.1007/0-387-27656-4},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Wallace/Unknown/Wallace - 2005 - Statistical and Inductive Inference by Minimum Message Length.pdf:pdf},
isbn = {038723795X},
pages = {429},
title = {{Statistical and Inductive Inference by Minimum Message Length}},
url = {http://books.google.com/books?id=3NmFwNHaNbUC&pgis=1},
year = {2005}
}
@article{Bickel2009,
abstract = {We show that, under a sparsity scenario, the Lasso estimator and the Dantzig selector exhibit similar behavior. For both methods, we derive, in par-allel, oracle inequalities for the prediction risk in the general nonparametric regression model, as well as bounds on the p estimation loss for 1 ≤ p ≤ 2 in the linear model when the number of variables can be much larger than the sample size.},
archivePrefix = {arXiv},
arxivId = {0801.1095},
author = {Bickel, Peter J. and Ritov, Y. and Tsybakov, Alexandre B.},
doi = {10.1214/08-AOS620},
eprint = {0801.1095},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bickel, Ritov, Tsybakov/Annals of Statistics/Bickel, Ritov, Tsybakov - 2009 - Simultaneous analysis of lasso and dantzig selector.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Linear models,Model selection,Nonparametric statistics},
number = {4},
pages = {1705--1732},
title = {{Simultaneous analysis of lasso and dantzig selector}},
volume = {37},
year = {2009}
}
@article{Hankin,
author = {Hankin, Robin K S},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Hankin/Unknown/Hankin - Unknown - Very large numbers in R Introducing package Brobdingnag.pdf:pdf},
keywords = {brobdingnag,r,s4 methods},
number = {10},
pages = {1--5},
title = {{Very large numbers in R : Introducing package Brobdingnag}},
volume = {100}
}
@article{Nelder1972,
author = {Nelder, John A. and Wedderburn, R. W. M.},
doi = {10.2307/2344614},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Nelder, Wedderburn/Journal of the Royal Statistical Society. Series A (General)/Nelder, Wedderburn - 1972 - Generalized Linear Models.pdf:pdf},
issn = {00359238},
journal = {Journal of the Royal Statistical Society. Series A (General)},
number = {3},
pages = {370},
title = {{Generalized Linear Models}},
url = {http://www.jstor.org/stable/10.2307/2344614?origin=crossref},
volume = {135},
year = {1972}
}
@article{Bolduc1999,
abstract = {The Multinomial Probit (MNP) formulation provides a\nvery general framework to allow for inter-dependent\nalternatives in discrete choice analysis. Up until\nrecently, its use was rather limited, mainly because\nof the computational difficulties associated with\nthe evaluation of the choice probabilities which are\nmultidimensional normal integrals. In recent years,\nthe econometric estimation of Multinomial Probit\nmodels has greatly been focused on. Alternative\nsimulation based approaches have been suggested and\ncompared. Most approaches exploit a conventional\nestimation technique where easy to compute\nsimulators replace the choice probabilities. For\nsituations such as in transportation demand\nmodelling where samples and choice sets are large,\nthe existing literature clearly suggests the use of\na maximum simulated likelihood (MSL) framework\ncombined with a Geweke-Hajivassiliou-Keane (GHK)\nchoice probability simulator. The present paper\ngives the computational details regarding the\nimplementation of this practical estimation approach\nwhere the scores are computed analytically. This\nrepresents a contribution of the paper, because\nusually, numerical derivatives are used. The\napproach is tested on a 9-mode transportation choice\nmodel estimated with disaggregate data from\nSantiago, Chile.},
author = {Bolduc, Denis},
doi = {10.1016/S0191-2615(98)00028-9},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bolduc/Transportation Research Part B Methodological/Bolduc - 1999 - A practical technique to estimate multinomial probit models in transportation.pdf:pdf},
isbn = {1418656542},
issn = {01912615},
journal = {Transportation Research Part B: Methodological},
number = {1},
pages = {63--79},
title = {{A practical technique to estimate multinomial probit models in transportation}},
volume = {33},
year = {1999}
}
@article{Casella1985,
abstract = {Abstract Empirical Bayes methods have been shown to be powerful data-analysis tools in recent years. The empirical Bayes model is much richer than either the classical or the ordinary Bayes model and often provides superior estimates of parameters. An introduction to some empirical Bayes methods is given, and these methods are illustrated with two examples. Abstract Empirical Bayes methods have been shown to be powerful data-analysis tools in recent years. The empirical Bayes model is much richer than either the classical or the ordinary Bayes model and often provides superior estimates of parameters. An introduction to some empirical Bayes methods is given, and these methods are illustrated with two examples.},
author = {Casella, George},
doi = {10.1080/00031305.1985.10479400},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Casella/The American Statistician/Casella - 1985 - An Introduction to Empirical Bayes Data Analysis.pdf:pdf},
isbn = {00031305},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {bi-,nomial distribution,normal distribution,stein estimation},
number = {2},
pages = {83--87},
title = {{An Introduction to Empirical Bayes Data Analysis}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00031305.1985.10479400},
volume = {39},
year = {1985}
}
@article{Dellaportas2002,
abstract = {Several MCMC methods have been proposed for estimating probabilities of models and associated ‘model-averaged' posterior distributions in the presence of model uncertainty.We discuss, compare, develop and illustrate several of these methods, focussing on connections between them.},
author = {Dellaportas, Petros and Forster, Jonathan J. and Ntzoufras, Ioannis},
doi = {10.1023/A:1013164120801},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Dellaportas, Forster, Ntzoufras/Statistics and Computing/Dellaportas, Forster, Ntzoufras - 2002 - On Bayesian model and variable selection using MCMC.pdf:pdf},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Gibbs sampler,Independence sampler,Metropolis-Hastings,Reversible jump},
number = {1},
pages = {27--36},
title = {{On Bayesian model and variable selection using MCMC}},
volume = {12},
year = {2002}
}
@article{Petralias2013,
author = {Petralias, Athanassios and Dellaportas, Petros},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Petralias, Dellaportas/Journal of Statistical Computation and Simulation/Petralias, Dellaportas - 2013 - An MCMC model search algorithm for.pdf:pdf},
journal = {Journal of Statistical Computation and Simulation},
keywords = {gene selection,population sampling,reversible jump,tracking portfolio},
number = {9},
pages = {1722--1740},
title = {{An MCMC model search algorithm for}},
url = {https://www.tol-project.org/svn/tolp/OfficialTolArchiveNetwork/ArimaTools/doc/bibliografia/MCMC/An MCMC model search algorithm for.pdf},
volume = {83},
year = {2013}
}
@article{wainwright2009sharp,
author = {Wainwright, Martin J},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Wainwright/IEEE transactions on information theory/Wainwright - 2009 - Sharp thresholds for high-dimensional and noisy sparsity recovery using-constrained quadratic programming (Lasso).pdf:pdf},
journal = {IEEE transactions on information theory},
number = {5},
pages = {2183--2202},
publisher = {IEEE},
title = {{Sharp thresholds for high-dimensional and noisy sparsity recovery using-constrained quadratic programming (Lasso)}},
volume = {55},
year = {2009}
}
@techreport{Betancourt2016,
address = {Tokyo},
author = {Betancourt, Michael},
institution = {Tokyo Stan},
title = {{Scalable Bayesian Inference with Hamiltonian Monte Carlo}},
year = {2016}
}
@article{West2003,
abstract = {I discuss Bayesian factor regression models with many explanatory variables. These models are of particular interest and applicability in problems of prediction, but also for elucidating underlying structure in predictor variables. One key motivating application here is in studies of gene expression in functional genomics. I first discuss empirical factor (principal compo- nents) regression, and the use of general classes of shrinkage priors, with an example. These models raise foundational questions for Bayesians, and related practical issues, due to the use of design-dependent priors and the need to recover inferences on the effects of the original, high-dimensional predictors. I then discuss latent factor models for high-dimensional variables, and regression approaches in which low-dimensional latent factors are the predictor variables. These models generalise empirical factor regression, provide for more incisive evaluation of fac- tor structure underlying high-dimensional predictors, and resolve the modelling and practical issues in empirical factor models by casting the latter as limiting special cases. Finally, I turn to questions of prior specification in these models, and introduce sparse latent factor models to induce sparsity in factor loadings matrices. Embedding such sparse latent factor models in factor regressions provides a novel approach to variable selection with very many predictors. The paper concludes with an example of sparse factor analysis of gene expression data and comments about further research.},
author = {West, Mike},
doi = {10.1.1.18.3036},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/West/Bayesian Statistics 7 - Proceedings of the Seventh Valencia International Meeting/West - 2003 - Bayesian factor regression models in the “large p, small n” paradigm.pdf:pdf},
isbn = {978-0-19-852615-5},
issn = {08966273},
journal = {Bayesian Statistics 7 - Proceedings of the Seventh Valencia International Meeting},
keywords = {1,covariates,dimension reduction,empirical factor regression models,gene expression analysis,high-dimensional,latent factor models,shrinkage priors},
pages = {723--732},
pmid = {12495626},
title = {{Bayesian factor regression models in the “large p, small n” paradigm}},
url = {http://www.isds.duke.edu/courses/Spring06/sta376/Support/RegressionETC/v7.paper.pdf},
year = {2003}
}
@article{Laird1982,
author = {Laird, NM and Ware, JH},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Laird, Ware/Biometrics/Laird, Ware - 1982 - Random-effects models for longitudinal data.pdf:pdf},
journal = {Biometrics},
number = {4},
pages = {963--974},
title = {{Random-effects models for longitudinal data}},
url = {http://www.jstor.org/stable/2529876},
volume = {38},
year = {1982}
}
@article{Miyamoto,
author = {Miyamoto, Koji},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Miyamoto/Eductation Indicators in Focus/Miyamoto - Unknown - What are the social benefits of education.pdf:pdf},
journal = {Eductation Indicators in Focus},
pages = {1--4},
title = {{What are the social benefits of education?}},
volume = {01}
}
@article{Kosmidis2010,
author = {Kosmidis, Ioannis and Firth, David},
doi = {10.1214/10-EJS579},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Kosmidis, Firth/Electronic Journal of Statistics/Kosmidis, Firth - 2010 - A generic algorithm for reducing bias in parametric estimation.pdf:pdf},
issn = {1935-7524},
journal = {Electronic Journal of Statistics},
keywords = {adjusted score,and phrases,asymptotic bias correction,beta,bias reduction,fisher scoring,prater gasoline data,received september 2010,regression},
pages = {1097--1112},
title = {{A generic algorithm for reducing bias in parametric estimation}},
url = {http://projecteuclid.org/euclid.ejs/1287147913},
volume = {4},
year = {2010}
}
@article{Huber2004,
author = {Huber, Philippe and Ronchetti, Elvezio and Victoria-Feser, Maria-Pia},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Huber, Ronchetti, Victoria-Feser/Journal of the Royal Statistical Society Series B (Statistical Methodology)/Huber, Ronchetti, Victoria-Feser - 2004 - Estimation of generalized linear latent variable models.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
number = {4},
pages = {893--908},
title = {{Estimation of generalized linear latent variable models}},
url = {http://www.blackwell-synergy.com/doi/abs/10.1111/j.1467-9868.2004.05627.x},
volume = {66},
year = {2004}
}
@article{Kuha2011,
author = {Kuha, Jouni and Firth, David},
doi = {10.1016/j.csda.2010.05.005},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Kuha, Firth/Computational Statistics & Data Analysis/Kuha, Firth - 2011 - On the index of dissimilarity for lack of fit in loglinear and log-multiplicative models.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics & Data Analysis},
month = {jan},
number = {1},
pages = {375--388},
publisher = {Elsevier B.V.},
title = {{On the index of dissimilarity for lack of fit in loglinear and log-multiplicative models}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947310001921},
volume = {55},
year = {2011}
}
@misc{Imbens2007,
abstract = {What's New in Econometrics: Lecture 15},
author = {Imbens, Guido},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Imbens/Unknown/Imbens - 2007 - Generalized Method of Moments and Empirical Likelihood.pdf:pdf},
title = {{Generalized Method of Moments and Empirical Likelihood}},
year = {2007}
}
@misc{Varin,
abstract = {A survey of recent developments in the theory and application of com- posite likelihood is provided, building on the review paper of Varin (2008). A range of application areas, including geostatistics, spatial extremes and space-time mod- els as well as clustered and longitudinal data and time series are considered. The important area of applications to statistical genetics is omitted, in light of Larribe and Fearnhead (2009). Emphasis is given to the development of the theory, and the current state of knowledge on efficiency and robustness of composite likelihood inference.},
author = {Varin, Cristiano and Reid, Nancy and Firth, David},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Varin, Reid, Firth/Unknown/Varin, Reid, Firth - Unknown - An Overview of Composite Likelihood Methods.pdf:pdf},
keywords = {copulas,generalized estimating equations,geostatistics,godambe information,longitudinal data,multivariate binary data,pseudo-likelihood,quasi-likelihood,robustness,spatial extremes,time series},
number = {2008},
pages = {1--41},
title = {{An Overview of Composite Likelihood Methods}}
}
@book{MacKay2003,
author = {MacKay, David J.C.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/MacKay/Unknown/MacKay - 2003 - Information Theory, Inference, and Learning Algorithms.pdf:pdf},
isbn = {0521642981},
keywords = {HMC,Hamiltonian,Monte Carlo},
mendeley-tags = {HMC,Hamiltonian,Monte Carlo},
publisher = {Cambridge University Press},
title = {{Information Theory, Inference, and Learning Algorithms}},
year = {2003}
}
@article{Ligtvoet2011,
author = {Ligtvoet, R. and van der Ark, L. Andries and Bergsma, Wicher P and Sijtsma, K.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Ligtvoet et al/Psychometrika/Ligtvoet et al. - 2011 - Polytomous latent scales for the investigation of the ordering of items.pdf:pdf},
journal = {Psychometrika},
keywords = {increasingness in transposition,invariant item ordering,latent scales,manifest invariant item,nonparametric irt models,ordering,polytomous irt models},
number = {2},
pages = {200--216},
title = {{Polytomous latent scales for the investigation of the ordering of items}},
url = {http://link.springer.com/article/10.1007/s11336-010-9199-8},
volume = {76},
year = {2011}
}
@article{Skrondal2012,
author = {Skrondal, Anders and Kuha, Jouni},
doi = {10.1007/s11336-012-9285-1},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Skrondal, Kuha/Psychometrika/Skrondal, Kuha - 2012 - Improved Regression Calibration.pdf:pdf},
issn = {0033-3123},
journal = {Psychometrika},
month = {sep},
number = {4},
pages = {649--669},
title = {{Improved Regression Calibration}},
url = {http://link.springer.com/10.1007/s11336-012-9285-1},
volume = {77},
year = {2012}
}
@article{Chipman2008,
author = {Chipman, Hugh and George, Edward I. and McCulloch, Robert E},
doi = {10.1214/193940307000000455},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Chipman, George, McCulloch/IMS Lecture Notes - Monograph Series/Chipman, George, McCulloch - 2001 - The practical implementation of bayesian model selection(2).pdf:pdf},
isbn = {978-0-940600-74-4},
journal = {IMS Lecture Notes - Monograph Series},
pages = {65--134},
title = {{The practical implementation of bayesian model selection}},
url = {http://projecteuclid.org/euclid.imsc/1207580085},
volume = {38},
year = {2001}
}
@article{Kitamura2006,
author = {Kitamura, Yuichi},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Kitamura/Unknown/Kitamura - 2006 - Empirical likelihood methods in econometrics Theory and practice.pdf:pdf},
keywords = {c14,conditional moment restriction,empirical distribution,gmm,jel classification number,large deviation principle,moment restric-,nonparametric specification test,npmle,tion models,weak dependence},
pages = {1--66},
title = {{Empirical likelihood methods in econometrics: Theory and practice}},
url = {http://cowles.econ.yale.edu/P/cd/d15b/d1569.pdf},
year = {2006}
}
@article{Liang2008a,
abstract = {Zellner's g prior remains a popular conventional prior for use in Bayesian variable selection, despite several undesirable consistency issues. In this article we study mixtures of g priors as an alternative to default g priors that resolve many of the problems with the original formulation while maintaining the computational tractability that has made the g prior so popular. We present theoretical properties of the mixture g priors and provide real and simulated examples to compare the mixture formulation with fixed g priors, empirical Bayes approaches, and other default procedures.},
author = {Liang, F and Paulo, R and Molina, G and Clyde, M a and Berger, J O},
doi = {Doi 10.1198/016214507000001337},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Liang et al/Journal of the American Statistical Association/Liang et al. - 2008 - Mixtures of g priors for Bayesian variable selection(2).pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {aic,approximations,bayesian model averaging,bic,cauchy,criterion,empirical bayes,gaussian hypergeometric functions,linear-regression,matrix,model selection,multiple-regression,zellner-siow priors},
number = {481},
pages = {410--423},
title = {{Mixtures of g priors for Bayesian variable selection}},
volume = {103},
year = {2008}
}
@article{Saaty1990,
author = {Saaty, Thomas L.},
doi = {10.1016/0377-2217(90)90057-I},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Saaty/European Journal of Operational Research/Saaty - 1990 - How to make a decision The analytic hierarchy process.pdf:pdf},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {cost-benefit,decision,decision is to choose,decision problem,how to structure a,important,perhaps the most creative,priority,rank,ratios,scales,task in making a,the factors that are},
month = {sep},
number = {1},
pages = {9--26},
title = {{How to make a decision: The analytic hierarchy process}},
url = {http://linkinghub.elsevier.com/retrieve/pii/037722179090057I},
volume = {48},
year = {1990}
}
@article{Vasdekis2013,
abstract = {Random effects and latent variable models are widely used for capturing unobserved heterogene- ity in longitudinal studies and associations in multivariate data. The estimation of those models becomes cumbersome as the number of the random effects or latent variables increase due to the high-dimensional integrations involved. Composite likelihood is a pseudo-likelihood that combines lower-order marginal or conditional densities such as univariate or bivariate and it has been proposed in the literature as an alter- native to full maximum likelihood estimation.We propose a weighted pairwise likelihood estimator based on estimates obtained from separate maximizations of marginal pairwise likelihoods. The derived weights minimize the total variance of the estimated parameters. The proposed weighted estimator is found to be more efficient than the one that assumes all weights to be equal.},
author = {Vasdekis, V. G. S. and Rizopoulos, Dimitris and Moustaki, Irini},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Vasdekis, Rizopoulos, Moustaki/Unknown/Vasdekis, Rizopoulos, Moustaki - 2013 - A weighted pairwise likelihood estimator for a class of latent variable models.pdf:pdf},
keywords = {categorical data,composite likelihood,generalised linear latent variables,longitudinal data},
pages = {1--7},
title = {{A weighted pairwise likelihood estimator for a class of latent variable models}},
url = {http://meetings.sis-statistica.org/index.php/sis2013/ALV/paper/view/2560},
year = {2013}
}
@article{tibshirani2003class,
author = {Tibshirani, Robert and Hastie, Trevor and Narasimhan, Balasubramanian and Chu, Gilbert},
journal = {Statistical Science},
number = {1},
pages = {104--117},
title = {{Class prediction by nearest shrunken centroids, with applications to DNA microarrays}},
volume = {18},
year = {2003}
}
@article{Ghosh2014,
abstract = {In this article, we highlight some interesting facts about Bayesian variable selection methods for linear regression models in settings where the design matrix exhibits strong collinearity. We first demonstrate via real data analysis and simulation studies that summaries of the posterior distribution based on marginal and joint distributions may give conflicting results for assessing the importance of strongly correlated covariates. The natural question is which one should be used in practice. The simulation studies suggest that posterior inclusion probabilities and Bayes factors that evaluate the importance of correlated covariates jointly are more appropriate, and some priors may be more adversely affected in such a setting. To obtain a better understanding behind the phenomenon, we study some toy examples with Zellner's g-prior. The results show that strong collinearity may lead to a multimodal posterior distribution over models, in which joint summaries are more appropriate than marginal summaries. Thus, we recommend a routine examination of the correlation matrix and calculation of the joint inclusion probabilities for correlated covariates, in addition to marginal inclusion probabilities, for assessing the importance of covariates in Bayesian variable selection.},
author = {Ghosh, Joyee and Ghattas, Andrew E},
doi = {10.1080/00031305.2015.1031827},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Ghosh, Ghattas/The American Statistician/Ghosh, Ghattas - 2015 - Bayesian variable selection under collinearity.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {bayesian model averaging,chain monte carlo,linear regression,marginal inclusion probability,markov,median probability model,multimodality,s g -prior,zellner},
number = {3},
pages = {165--173},
title = {{Bayesian variable selection under collinearity}},
volume = {69},
year = {2015}
}
@article{StackOverflow,
author = {{Stack Overflow}},
journal = {http://stackoverflow.com/questions/2908822/speed-up-the-loop-operation-in-r},
title = {{Speed up the loop operation in R}},
url = {http://stackoverflow.com/questions/2908822/speed-up-the-loop-operation-in-r},
year = {2011}
}
@article{Breslow1993a,
abstract = {Statistical approaches to overdispersion, correlated errors, shrinkage estimation, and smoothing of regression relationships may be encompassed within the framework of the generalized linear mixed model (GLMM). Given an unobserved vector of random effects, observations are assumed to be conditionally independent with means that depend on the linear predictor through a specified link function and conditional variances that are specified by a variance function, known prior weights and a scale factor. The random effects are assumed to be normally distributed with mean zero and dispersion matrix depending on unknown variance components. For problems involving time series, spatial aggregation and smoothing, the dispersion may be specified in terms of a rank deficient inverse covariance matrix. Approximation of the marginal quasi-likelihood using Laplace's method leads eventually to estimating equations based on penalized quasilikelihood or PQL for the mean parameters and pseudo-likelihood for the variances. Implementation involves repeated calls to normal theory procedures for REML estimation in variance components problems. By means of informal mathematical arguments, simulations and a series of worked examples, we conclude that PQL is of practical value for approximate inference on parameters and realizations of random effects in the hierarchical model. The applications cover overdispersion in binomial proportions of seed germination; longitudinal analysis of attack rates in epilepsy patients; smoothing of birth cohort effects in an age-cohort model of breast cancer incidence; evaluation of curvature of birth cohort effects in a case-control study of childhood cancer and obstetric radiation; spatial aggregation of lip cancer rates in Scottish counties; and the success of salamander matings in a complicated experiment involving crossing of male and female effects. PQL tends to underestimate somewhat the variance components and (in absolute value) fixed effects when applied to clustered binary data, but the situation improves rapidly for binomial observations having denominators greater than one. Statistical approaches to overdispersion, correlated errors, shrinkage estimation, and smoothing of regression relationships may be encompassed within the framework of the generalized linear mixed model (GLMM). Given an unobserved vector of random effects, observations are assumed to be conditionally independent with means that depend on the linear predictor through a specified link function and conditional variances that are specified by a variance function, known prior weights and a scale factor. The random effects are assumed to be normally distributed with mean zero and dispersion matrix depending on unknown variance components. For problems involving time series, spatial aggregation and smoothing, the dispersion may be specified in terms of a rank deficient inverse covariance matrix. Approximation of the marginal quasi-likelihood using Laplace's method leads eventually to estimating equations based on penalized quasilikelihood or PQL for the mean parameters and pseudo-likelihood for the variances. Implementation involves repeated calls to normal theory procedures for REML estimation in variance components problems. By means of informal mathematical arguments, simulations and a series of worked examples, we conclude that PQL is of practical value for approximate inference on parameters and realizations of random effects in the hierarchical model. The applications cover overdispersion in binomial proportions of seed germination; longitudinal analysis of attack rates in epilepsy patients; smoothing of birth cohort effects in an age-cohort model of breast cancer incidence; evaluation of curvature of birth cohort effects in a case-control study of childhood cancer and obstetric radiation; spatial aggregation of lip cancer rates in Scottish counties; and the success of salamander matings in a complicated experiment involving crossing of male and female effects. PQL tends to underestimate somewhat the variance components and (in absolute value) fixed effects when applied to clustered binary data, but the situation improves rapidly for binomial observations having denominators greater than one.},
author = {Breslow, N. E. and Clayton, D. G.},
doi = {10.1080/01621459.1993.10594284},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Breslow, Clayton/Journal of the American Statistical Association/Breslow, Clayton - 1993 - Approximate Inference in Generalized Linear Mixed Models.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = {mar},
number = {421},
pages = {9--25},
publisher = {Taylor & Francis},
title = {{Approximate Inference in Generalized Linear Mixed Models}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10594284},
volume = {88},
year = {1993}
}
@book{jaynes2003,
author = {Jaynes, Edwin T.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Jaynes/Unknown/Jaynes - 2003 - Probability theory the logic of science.pdf:pdf},
publisher = {Cambridge university press},
title = {{Probability theory: the logic of science}},
year = {2003}
}
@article{Lau2013,
archivePrefix = {arXiv},
arxivId = {1307.0742},
author = {Lau, F. Din-Houn and Gandy, Axel},
eprint = {1307.0742},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Lau, Gandy/Unknown/Lau, Gandy - 2013 - RMCMC A System for Updating Bayesian Models.pdf:pdf},
month = {jul},
title = {{RMCMC: A System for Updating Bayesian Models}},
url = {http://arxiv.org/abs/1307.0742},
year = {2013}
}
@misc{Castro2013,
author = {Castro, Rui},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Castro/Unknown/Castro - 2013 - Lectures 12 and 13 - Complexity Penalized Maximum Likelihood Estimation.pdf:pdf},
keywords = {likelihood,maximum,penalized},
mendeley-tags = {likelihood,maximum,penalized},
pages = {1--18},
title = {{Lectures 12 and 13 - Complexity Penalized Maximum Likelihood Estimation}},
year = {2013}
}
@article{Ratmann,
archivePrefix = {arXiv},
arxivId = {arXiv:1305.4283v2},
author = {Ratmann, Oliver},
eprint = {arXiv:1305.4283v2},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Ratmann/Unknown/Ratmann - Unknown - Statistical modelling of summary values leads to accurate Approximate Bayesian Computations.pdf:pdf},
number = {1},
title = {{Statistical modelling of summary values leads to accurate Approximate Bayesian Computations}}
}
@article{Quoidbach2013,
abstract = {We measured the personalities, values, and preferences of more than 19,000 people who ranged in age from 18 to 68 and asked them to report how much they had changed in the past decade and/or to predict how much they would change in the next decade. Young people, middle-aged people, and older people all believed they had changed a lot in the past but would change relatively little in the future. People, it seems, regard the present as a watershed moment at which they have finally become the person they will be for the rest of their lives. This "end of history illusion" had practical consequences, leading people to overpay for future opportunities to indulge their current preferences.},
author = {Quoidbach, Jordi and Gilbert, Daniel T and Wilson, Timothy D},
doi = {10.1126/science.1229294},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Quoidbach, Gilbert, Wilson/Science (New York, N.Y.)/Quoidbach, Gilbert, Wilson - 2013 - The end of history illusion.pdf:pdf},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
keywords = {Adolescent,Adult,Aged,Female,Forecasting,History,Humans,Illusions,Male,Middle Aged,Personality,Self Report,Time Perception,Young Adult},
month = {jan},
number = {6115},
pages = {96--8},
pmid = {23288539},
title = {{The end of history illusion.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23288539},
volume = {339},
year = {2013}
}
@article{Bernardo1979,
author = {Bernardo, Jose M},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bernardo/Journal of Royal Statistical Socity/Bernardo - 1979 - Reference Posterior Distribution for Bayesian Inference (with Discussion).pdf:pdf},
journal = {Journal of Royal Statistical Socity},
number = {2},
pages = {113--147},
title = {{Reference Posterior Distribution for Bayesian Inference (with Discussion)}},
volume = {41},
year = {1979}
}
@book{Murphy1991,
abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package—PMTK (probabilistic modeling toolkit)—that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
author = {Murphy, Kevin P.},
booktitle = {Machine Learning: A Probabilistic Perspective},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Murphy/Machine Learning A Probabilistic Perspective/Murphy - 2012 - Machine Learning A Probabilistic Perspective.pdf:pdf},
publisher = {The MIT Press},
title = {{Machine Learning: A Probabilistic Perspective}},
year = {2012}
}
@article{George1997,
annote = {Edward I. George and Robert E. McCulloch},
author = {George, Edward I and McCulloch, Robert E},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/George, McCulloch/Statistica Sinica/George, McCulloch - 1997 - Approaches for Bayesian Variable Selection.pdf:pdf},
journal = {Statistica Sinica},
number = {7},
pages = {339--373},
title = {{Approaches for Bayesian Variable Selection}},
url = {http://www3.stat.sinica.edu.tw/statistica/oldpdf/A7n26.pdf},
year = {1997}
}
@article{Albert1993,
abstract = {A vast literature in statistics, biometrics, and econometrics is concerned with the analysis of binary and polychotomous response data. The classical approach fits a categorical response regression model using maximum likelihood, and inferences about the model are based on the associated asymptotic theory. The accuracy of classical confidence statements is questionable for small sample sizes. In this article, exact Bayesian methods for modeling categorical response data are developed using the idea of data augmentation. The general approach can be summarized as follows. The probit regression model for binary outcomes is seen to have an underlying normal regression structure on latent continuous data. Values of the latent data can be simulated from suitable truncated normal distributions. If the latent data are known, then the posterior distribution of the parameters can be computed using standard results for normal linear models. Draws from this posterior are used to sample new latent data, and the process is iterated with Gibbs sampling. This data augmentation approach provides a general framework for analyzing binary regression models. It leads to the same simplification achieved earlier for censored regression models. Under the proposed framework, the class of probit regression models can be enlarged by using mixtures of normal distributions to model the latent data. In this normal mixture class, one can investigate the sensitivity of the parameter estimates to the choice of "link function," which relates the linear regression estimate to the fitted probabilities. In addition, this approach allows one to easily fit Bayesian hierarchical models. One specific model considered here reflects the belief that the vector of regression coefficients lies on a smaller dimension linear subspace. The methods can also be generalized to multinomial response models with $J > 2$ categories. In the ordered multinomial model, the J categories are ordered and a model is written linking the cumulative response probabilities with the linear regression structure. In the unordered multinomial model, the latent variables have a multivariate normal distribution with unknown variance-covariance matrix. For both multinomial models, the data augmentation method combined with Gibbs sampling is outlined. This approach is especially attractive for the multivariate probit model, where calculating the likelihood can be difficult.},
author = {Albert, James and Chib, Siddhartha},
doi = {10.2307/2290350},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Albert, Chib/Journal of the American Statistical Association/Albert, Chib - 1993 - Bayesian Analysis of Binary and Polychotomous Response Data.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {bayesian_analysis,response_data},
number = {422},
pages = {669--679},
title = {{Bayesian Analysis of Binary and Polychotomous Response Data}},
volume = {88},
year = {1993}
}
@misc{Bates2011,
author = {Bates, Douglas},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bates/Unknown/Bates - 2011 - Mixed models in R using the lme4 package Part 5 Generalized linear mixed models.pdf:pdf},
keywords = {GLMM,generalized,linear,mixed,models},
mendeley-tags = {GLMM,generalized,linear,mixed,models},
title = {{Mixed models in R using the lme4 package Part 5 : Generalized linear mixed models}},
year = {2011}
}
@article{Jann2013,
author = {Jann, Ben},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Jann/University of Bern Social Sciences Working Papers/Jann - 2013 - Plotting regression coefficients and other estimates in Stata.pdf:pdf},
journal = {University of Bern Social Sciences Working Papers},
keywords = {coe ffi cients plot,coefplot,margins,marginsplot,regression plot},
number = {1},
pages = {0--34},
title = {{Plotting regression coefficients and other estimates in Stata}},
url = {http://www.uni-leipzig.de/$\sim$sozio/mitarbeiter/m79/content/dokumente/739/jann_2013_coefplot_(1).pdf},
volume = {41},
year = {2013}
}
@article{Pg.HajiHassan,
abstract = {The Government of Brunei Darussalam has played a central role in the provision of public housing in the form of landed property for many centuries. The period of modernity and economic growth has resulted in urban sprawl and declining available land. In a bid to arrest this unsustainable trend, the Government is promoting high-density, high-rise settlement complexes as an alternative to meet housing demands. The necessary legislation to support ownership of apartment units in multi-storey complexes, namely the Strata Title Act, has also been developed. The public, including real estate developers, however has not warmed to the initiative. Some analysts are of the view that there is a need to “re-engineer the Brunei society” to embrace vertical living via various means such as education and awareness. Nevertheless this paper will argue that the understanding and appreciation for Bruneians' lifestyle as well as social and cultural proxemics are actually important for urban planners to consider. This study shows the different cultural typologies in Brunei which enabled the researchers to understand the cultural and social determinants that influences respondents' choices with regards to the types and space preferences in relation to vertical housing. The paper puts forward a hypothesis that the Brunei society will warm up to vertical living if their cultural space needs are met in high-rise, high-density complexes. For the purpose of this study, a survey was conducted to validate the predictions of the model i.e. cultural architecture typology particularly targeting affected populations generated. The paper takes an insider-outsider approach which further emphasises the importance of local knowledge and understanding of local cultures and practices in research. It is hoped that the study will contribute to a better understanding of the housing demands of the Brunei society in general, and key cultural considerations with regards to vertical living in particular.},
author = {{Pg. Haji Hassan}, Dk. Noor Hasharina and {Noor Azalie}, Izni Azrein and {Haji Ibrahin}, Khairunnisa and Yong, Gabriel and {Haji Ali Maricar}, Hajah Hairuni},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Pg. Haji Hassan et al/earoph.info/Pg. Haji Hassan et al. - Unknown - Cultural Consideration in Vertical Living in Brunei Darussalam.pdf:pdf},
journal = {earoph.info},
keywords = {Brunei society,culture,proxemics,space needs,vertical living},
pages = {1--36},
title = {{Cultural Consideration in Vertical Living in Brunei Darussalam}},
url = {http://earoph.info/pdf/2011papers/2011-PAPER8.pdf}
}
@techreport{NationalAuditOffice2004,
abstract = {The National Audit Office scrutinises public spending on behalf of Parliament. The Comptroller and Auditor General, Sir John Bourn, is an Officer of the House of Commons. He is the head of the National Audit Office, which employs some 800 staff. He, and the National Audit Office, are totally independent of Government. He certifies the accounts of all Government departments and a wide range of other public sector bodies; and he has statutory authority to report to Parliament on the economy, efficiency and effectiveness with which departments and other bodies have used their resources. Our work saves the taxpayer millions of pounds every year. At least {\pounds}8 for every {\pounds}1 spent running the Office.},
author = {{National Audit Office}},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/National Audit Office/Unknown/National Audit Office - 2004 - The Rapid Procurement of Capability to Support Operations.pdf:pdf},
number = {November},
title = {{The Rapid Procurement of Capability to Support Operations}},
year = {2004}
}
@techreport{Hein2004,
abstract = {This paper gives a survey of results in the mathematical literature on positive definite kernels and their associated structures. We concentrate on properties which seem potentially relevant for Machine Learning and try to clarify some results that have been misused in the literature. Moreover we consider different lines of generalizations of positive definite kernels. Namely we deal with operator-valued kernels and present the general framework of Hilbertian subspaces of Schwartz which we use to introduce kernels which are distributions. Finally indefinite kernels and their associated reproducing kernel spaces are considered.},
author = {Hein, Matthias and Bousquet, Olivier},
booktitle = {Max–Planck–Institut f{\"{u}}r biologische Kybernetik},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Hein, Bousquet/Max–Planck–Institut f{\"{u}}r biologische Kybernetik/Hein, Bousquet - 2004 - Kernels, associated structures and generalizations.pdf:pdf},
number = {127},
title = {{Kernels, associated structures and generalizations}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.9510&rep=rep1&type=pdf%5Cnpapers2://publication/uuid/509B266F-E508-4031-8519-68B8FCB109CA},
year = {2004}
}
@article{Fan2011,
abstract = {The variance covariance matrix plays a central role in the inferential theories of high dimensional factor models in finance and economics. Popular regularization methods of directly exploiting sparsity are not directly applicable to many financial problems. Classical methods of estimating the covariance matrices are based on the strict factor models, assuming independent idiosyncratic components. This assumption, however, is restrictive in practical applications. By assuming sparse error covariance matrix, we allow for the presence of  the cross-sectional correlation even after taking out common factors, and it enables us to combine the merits of both sparsity and factor structures. We estimate the sparse  covariance using the adaptive thresholding technique as in Cai and Liu (2011), taking into account the fact that direct observations of the idiosyncratic components are unavailable. The impact of high dimensionality on the covariance matrix estimation based on the factor structure is then studied.},
author = {Fan, Jianqing and Liao, Yuan and Mincheva, Martina},
doi = {10.2139/ssrn.1849266},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Fan, Liao, Mincheva/SSRN Electronic Journal/Fan, Liao, Mincheva - 2011 - High Dimensional Covariance Matrix Estimation in Approximate Factor Models.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
month = {may},
title = {{High Dimensional Covariance Matrix Estimation in Approximate Factor Models}},
url = {http://papers.ssrn.com/abstract=1849266},
year = {2011}
}
@misc{Mckeague,
author = {Mckeague, Ian},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Mckeague/Unknown/Mckeague - Unknown - Workshop on Empirical Likelihood Methods in Survival Analysis.pdf:pdf},
title = {{Workshop on Empirical Likelihood Methods in Survival Analysis}}
}
@article{Leisch2009,
abstract = {This tutorial gives a practical introduction to creating R packages. We discuss how object oriented programming and S formulas can be used to give R code the usual look and feel, how to start a package from a collection of R functions, and how to test the code once the package has been created. As running example we use functions for standard linear regression analysis which are developed from scratch.},
author = {Leisch, F.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Leisch/Compstat 2008-Proceedings in Computational Statistics/Leisch - 2009 - Creating R Packages A Tutorial.pdf:pdf},
isbn = {978-3-7908-2083-6},
journal = {Compstat 2008-Proceedings in Computational Statistics},
keywords = {open source,r packages,software,statistical computing},
number = {36},
pages = {1--19},
title = {{Creating R Packages : A Tutorial}},
url = {http://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf},
year = {2009}
}
@book{Bergsma2009,
author = {Bergsma, Wicher P and Croon, Marcel and Hagenaars, Jacques A.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bergsma, Croon, Hagenaars/Unknown/Bergsma, Croon, Hagenaars - 2009 - Marginal Models for Depedent, Clustered and Longitudinal Categorical Data.pdf:pdf},
isbn = {9780387096094},
publisher = {Springer},
title = {{Marginal Models for Depedent, Clustered and Longitudinal Categorical Data}},
year = {2009}
}
@misc{Joe2012,
author = {Joe, Harry and Reid, Nancy and Song, Peter Xuekun and Firth, David and Varin, Cristiano},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Joe et al/Unknown/Joe et al. - 2012 - Composite Likelihood Methods.pdf:pdf},
pages = {1--5},
title = {{Composite Likelihood Methods}},
year = {2012}
}
@phdthesis{Jamil2010,
abstract = {This dissertation focusses mainly on the Bradley-Terry model and its extensions to in- vestigate three aspects of English Premier League football. Firstly, a comparison of the estimated model rankings with the actual league table will be discussed and how well the model serves as a predictor of the final standings at the end of the season after sev- eral games have been played. Secondly, a home advantage analysis of the teams will be conducted. Thirdly, an estimation of player rankings based on team performances will be attempted. All analyses were conducted in R using the glm() framework, with the exception of the third model, which was specifically coded and solved using an optimi- sation function in R. While the first two analyses generally showed a good fit and clear results, the third one was not as straightforward. The failure to find stationary points in the optimisation problem suggests more work needs to be done to refine the model.},
author = {Jamil, Haziq},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Jamil/Unknown/Jamil - 2010 - Analysis of paired comparison data using Bradley-Terry models with applications to football data.pdf:pdf},
keywords = {Statistics,bradley,terry},
mendeley-tags = {Statistics},
number = {May},
school = {University of Warwick},
title = {{Analysis of paired comparison data using Bradley-Terry models with applications to football data}},
type = {Masters Dissertation},
year = {2010}
}
@article{Jeffreys1946,
abstract = {It is shown that a certain differential form depending on the values of the parameters in a law of chance is invariant for all transformations of the parameters when the law is differentiable with regard to all parameters. For laws containing a location and a scale parameter a form with a somewhat restricted type of invariance is found even when the law is not everywhere differentiable with regard to the parameters. This form has the properties required to give a general rule for stating the prior probability in a large class of estimation problems. CR - Copyright &#169; 1946 The Royal Society},
author = {Jeffreys, Harold},
doi = {10.1098/rspa.1946.0056},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Jeffreys/Proceedings of the Royal Society of London. Series A Mathematical and physical sciences/Jeffreys - 1946 - An invariant form for the prior probability in estimation problems.pdf:pdf},
issn = {1364-5021},
journal = {Proceedings of the Royal Society of London. Series A: Mathematical and physical sciences},
keywords = {STATISTICS},
number = {1007},
pages = {453--461},
pmid = {20998741},
title = {{An invariant form for the prior probability in estimation problems.}},
volume = {186},
year = {1946}
}
@article{Goldstein1996,
author = {Goldstein, Harvey and Thomas, Sally},
doi = {10.2307/2983475},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Goldstein, Thomas/Journal of the Royal Statistical Society. Series A (Statistics in Society)/Goldstein, Thomas - 1996 - Using Examination Results as Indicators of School and College Performance.pdf:pdf},
issn = {09641998},
journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
keywords = {examination results,f,institutional comparisons,multilevel modelling,performance indicators,school effectiveness,value added},
number = {1},
pages = {149},
title = {{Using Examination Results as Indicators of School and College Performance}},
url = {http://www.jstor.org/stable/10.2307/2983475?origin=crossref},
volume = {159},
year = {1996}
}
@book{Neal2011,
abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard to compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories form taking much computation time.},
archivePrefix = {arXiv},
arxivId = {1206.1901},
author = {Neal, Radford M.},
booktitle = {Handbook of Markov Chain Monte Carlo},
doi = {doi:10.1201/b10905-6},
eprint = {1206.1901},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Neal/Handbook of Markov Chain Monte Carlo/Neal - 2011 - MCMC using Hamiltonian dynamics.pdf:pdf},
isbn = {9781420079418},
issn = {<null>},
keywords = {hamiltonian dynamics,mcmc},
pages = {113--162},
pmid = {25246403},
title = {{MCMC using Hamiltonian dynamics}},
year = {2011}
}
@article{Dempster1977,
abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
author = {Dempster, A.P. and Laird, N.M. and Rubin, D.B.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Dempster, Laird, Rubin/Journal of the Royal Statistical Society, Series B (Statistical Methodology)/Dempster, Laird, Rubin - 1977 - Maximum likelihood from incomplete data via the EM algorithm.pdf:pdf},
journal = {Journal of the Royal Statistical Society, Series B (Statistical Methodology)},
number = {1},
pages = {1--38},
title = {{Maximum likelihood from incomplete data via the EM algorithm}},
url = {http://www.jstor.org/stable/10.2307/2984875},
volume = {39},
year = {1977}
}
@article{Farkas2007,
author = {Farkas, Andr{\'{a}}s},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Farkas/Acta Polytechnica Hungarica/Farkas - 2007 - The analysis of the principal eigenvector of pairwise comparison matrices.pdf:pdf},
journal = {Acta Polytechnica Hungarica},
keywords = {algebraic eigenvalue-eigenvector problem,multiple criteria decision making,rank},
title = {{The analysis of the principal eigenvector of pairwise comparison matrices}},
url = {http://www.researchgate.net/publication/228365703_The_analysis_of_the_principal_eigenvector_of_pairwise_comparison_matrices/file/504635296f4a40538a.pdf},
year = {2007}
}
@article{alpay1991,
author = {Alpay, Daniel},
journal = {Rocky Mountain Journal of Mathematics},
number = {4},
pages = {1189--1205},
title = {{Some remarks on reproducing kernel Krein spaces}},
volume = {21},
year = {1991}
}
@article{Schall1991,
abstract = {A conceptually very simple but general algorithm for the estimation of the fixed effects, random effects, and components of dispersion in generalized linear models with random effects is proposed. Conditions are described under which the algorithm yields approxi- mate maximum likelihood or quasi-maximum likelihood estimates of the fixed effects and dispersion components, and approximate empirical Bayes estimates of the random effects. The algorithm is applied to two data sets to illustrate the estimation of components of dispersion and the modelling of overdispersion.},
author = {Schall, Robert},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Schall/Biometrika/Schall - 1991 - Estimation in generalized linear models with random effects.pdf:pdf},
journal = {Biometrika},
keywords = {Component of dispersion,Empirical Bayes,Quasi-likelihood,Restricted maximum likeli- hood,Variance component},
mendeley-tags = {Component of dispersion,Empirical Bayes,Quasi-likelihood,Restricted maximum likeli- hood,Variance component},
number = {4},
pages = {719--727},
title = {{Estimation in generalized linear models with random effects}},
url = {http://biomet.oxfordjournals.org/content/78/4/719.short},
volume = {78},
year = {1991}
}
@article{Breiman1985,
author = {Breiman, L and Friedman, J H},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Breiman, Friedman/Journal of the American Statistical Association/Breiman, Friedman - 1985 - Estimating optimal transformations for multiple-regression and correlation.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {smoothing},
number = {391},
pages = {614--619},
title = {{Estimating optimal transformations for multiple-regression and correlation}},
volume = {80},
year = {1985}
}
@book{Hastie2009,
author = {Hastie, T and Tibshirani, R and Friedman, J},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Hastie, Tibshirani, Friedman/Unknown/Hastie, Tibshirani, Friedman - 2009 - The elements of statistical learning.pdf:pdf},
title = {{The elements of statistical learning}},
url = {http://link.springer.com/content/pdf/10.1007/978-0-387-84858-7.pdf},
year = {2009}
}
@article{Ntzoufras,
author = {Ntzoufras, Ioannis},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Ntzoufras/Unknown/Ntzoufras - Unknown - Bayesian Variable Selection Tutorial.pdf:pdf},
pages = {1--63},
title = {{Bayesian Variable Selection Tutorial}}
}
@article{Natarajan1995,
author = {Natarajan, Ranjini and McCulloch, Charles E. and Kiefer, Nicholas M.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Natarajan, McCulloch, Kiefer/Unpublished manuscript/Natarajan, McCulloch, Kiefer - 1995 - Maximum likelihood for the multinomial probit model.pdf:pdf},
journal = {Unpublished manuscript},
keywords = {gibbs sampling,maximum likelihood estimation,monte carlo em,observed infor-},
title = {{Maximum likelihood for the multinomial probit model}},
year = {1995}
}
@article{Lee1996,
author = {Lee, Y and Nelder, John A.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Lee, Nelder/Journal of the Royal Statistical Society. Series B ( {\ldots}/Lee, Nelder - 1996 - Hierarchical Generalized Linear Models.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B ( {\ldots}},
number = {4},
pages = {619--678},
title = {{Hierarchical Generalized Linear Models}},
url = {http://www.jstor.org/stable/2346105},
volume = {58},
year = {1996}
}
@inproceedings{Sun2005,
address = {Honolulu},
author = {Sun, Hongcai},
booktitle = {International Symposium on the Analytic Hierarchy Process},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Sun/International Symposium on the Analytic Hierarchy Process/Sun - 2005 - No Title.pdf:pdf},
keywords = {ahp,ahp in engineering decisions,making,qualitative factors in decision},
number = {24},
pages = {1--21},
title = {{No Title}},
year = {2005}
}
@article{Ong2004,
abstract = {In this paper we show that many kernel methods can be adapted to deal with indefinite kernels, that is, kernels which are not positive semidefinite. They do not satisfy Mercer‘s condition and they induce associated functional spaces called Reproducing Kernel Kre&icaron;n Spaces (RKKS), a generalization of Reproducing Kernel Hilbert Spaces (RKHS).Machine learning in RKKS shares many "nice" properties of learning in RKHS, such as orthogonality and projection. However, since the kernels are indefinite, we can no longer minimize the loss, instead we stabilize it. We show a general representer theorem for constrained stabilization and prove generalization bounds by computing the Rademacher averages of the kernel class. We list several examples of indefinite kernels and investigate regularization methods to solve spline interpolation. Some preliminary experiments with indefinite kernels for spline smoothing are reported for truncated spectral factorization, Landweber-Fridman iterations, and MR-II.},
author = {Ong, Cheng Soon and Mary, Xavier and Canu, St{\'{e}}phane and Smola, Alex},
doi = {10.1145/1015330.1015443},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Ong et al/Proceedings of the Twenty-First International Conference on Machine Learning (ICML 2004)/Ong et al. - 2004 - Learning with Non-Positive Kernels.pdf:pdf},
isbn = {1581138385},
issn = {00200255},
journal = {Proceedings of the Twenty-First International Conference on Machine Learning (ICML 2004)},
keywords = {computational,information theoretic learning with statistics,learning,statistics & optimisation,theory & algorithms},
number = {7},
pages = {81},
pmid = {19487391},
title = {{Learning with Non-Positive Kernels}},
url = {http://eprints.pascal-network.org/archive/00000714/},
year = {2004}
}
@article{Tseng2001,
abstract = {We study the convergence properties of a (block) coordinate descent method applied to minimize a nondifferentiable (nonconvex) function f (x1 , . . . , xN ) with certain separability and regularity proper- ties. Assuming that f is continuous on a compact level set, the sub- sequence convergence of the iterates to a stationary point is shown when either f is pseudoconvex in every pair of coordinate blocks from among NA1 coordinate blocks or f has at most one minimum in each of NA2 coordinate blocks. If f is quasiconvex and hemivariate in every coordi- nate block, then the assumptions of continuity of f and compactness of the level set may be relaxed further. These results are applied to derive new (and old) convergence results for the proximal minimization algo- rithm, an algorithm of Arimoto and Blahut, and an algorithm of Han. They are applied also to a problem of blind source separation.},
author = {Tseng, P.},
doi = {10.1023/A:1017501703105},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Tseng/Journal of Optimization Theory and Applications/Tseng - 2001 - Convergence of a block coordinate descent method for nondifferentiable minimization.pdf:pdf},
isbn = {0022-3239},
issn = {00223239},
journal = {Journal of Optimization Theory and Applications},
keywords = {Block coordinate descent,Convergence,Gauss-seidel method,Nondifferentiable minimization,Pseudoconvex functions,Quasiconvex functions,Stationary point},
number = {3},
pages = {475--494},
title = {{Convergence of a block coordinate descent method for nondifferentiable minimization}},
volume = {109},
year = {2001}
}
@article{Fouskakis2009,
abstract = {In the field of quality of health care measurement, one approach to assessing patient sickness at admission involves a logistic regression of mortality within 30 days of admission on a fairly large number of sickness indicators (on the order of 100) to construct a sickness scale, employing classical variable selection methods to find an "optimal" subset of 10-20 indicators. Such "benefit-only" methods ignore the considerable differences among the sickness indicators in cost of data collection, an issue that is crucial when admission sickness is used to drive programs (now implemented or under consideration in several countries, including the U.S. and U.K.) that attempt to identify substandard hospitals by comparing observed and expected mortality rates (given admission sickness). When both data-collection cost and accuracy of prediction of 30-day mortality are considered, a large variable-selection problem arises in which costly variables that do not predict well enough should be omitted from the final scale. In this paper (a) we develop a method for solving this problem based on posterior model odds, arising from a prior distribution that (1) accounts for the cost of each variable and (2) results in a set of posterior model probabilities that corresponds to a generalized cost-adjusted version of the Bayesian information criterion (BIC), and (b) we compare this method with a decision-theoretic cost-benefit approach based on maximizing expected utility. We use reversible-jump Markov chain Monte Carlo (RJMCMC) methods to search the model space, and we check the stability of our findings with two variants of the MCMC model composition (MC3) algorithm. We find substantial agreement between the decision-theoretic and cost-adjusted-BIC methods; the latter provides a principled approach to performing a cost-benefit trade-off that avoids ambiguities in identification of an appropriate utility structure. Our cost-benefit approach results in a set of models with a noticeable reduction in cost and dimensionality, and only a minor decrease in predictive performance, when compared with models arising from benefit-only analyses.},
archivePrefix = {arXiv},
arxivId = {arXiv:0908.2313v1},
author = {Fouskakis, Dimitris and Ntzoufras, Ioannis and Draper, D.},
doi = {10.1214/08-AOAS207},
eprint = {arXiv:0908.2313v1},
file = {:Users/haziqjamil/Downloads/euclid.aoas.1245676190.pdf:pdf},
isbn = {1932-6157},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Bayesian information criterion (BIC),Cost-adjusted BIC,Cost-benefit analysis,Input-output analysis,Laplace approximation,MCMC model composition (MC 3),Quality of health care,Reversible-jump Markov chain Monte Carlo (RJMCMC),Sickness at hospital admission},
number = {2},
pages = {663--690},
title = {{Bayesian variable selection using cost-adjusted BIC, with application to cost-effective measurement of quality of health care}},
volume = {3},
year = {2009}
}
@article{Li2012,
author = {Li, Haocheng},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Li/Unknown/Li - 2012 - Longitudinal Data Analysis with Composite Likelihood Methods.pdf:pdf},
title = {{Longitudinal Data Analysis with Composite Likelihood Methods}},
url = {https://uwspace.uwaterloo.ca/handle/10012/6848},
year = {2012}
}
@book{berlinet2011,
author = {Berlinet, Alain and Thomas-Agnan, Christine},
publisher = {Springer Science & Business Media},
title = {{Reproducing kernel Hilbert spaces in probability and statistics}},
year = {2011}
}
@article{Daniels2016,
author = {Daniels, Michael J and Kass, Robert E},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Daniels, Kass/Journal of the American Statistical Association/Daniels, Kass - 2016 - Nonconjugate Bayesian Estimation of Covariance Matrices and Its Use in Hierarchical Models.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {givens angles,hierarchical prior,random effects,shrinkage,variance matrix},
number = {448},
pages = {1254--1263},
title = {{Nonconjugate Bayesian Estimation of Covariance Matrices and Its Use in Hierarchical Models}},
volume = {94},
year = {2016}
}
@book{Tanner1996,
abstract = {This book provides a unified introduction to a variety of computational algorithms for likelihood and Bayesian inference. The third edition expands the discussion of many of the techniques discussed, includes additional examples, and adds exercise sets at the end of each chapter.},
author = {Tanner, Martin Abba},
booktitle = {Springer series in statistics},
doi = {10.2307/2965746},
isbn = {0387946888 (hardcover alk. paper)},
issn = {01621459},
keywords = {Bayesian statistical decision theory.,Functions,Inference,Mathematical statistics.,Statistical,Statistical inference,distribution,index,statistics},
pages = {viii, 207},
pmid = {4182},
title = {{Tools for statistical inference : methods for the exploration of posterior distributions and likelihood functions}},
year = {1996}
}
@article{September2008,
abstract = {The lme4 package provides R functions to fit and analyze several different types of mixed-effects models, including linear mixed models, generalized linear mixed models and nonlinear mixed models. In this vignette we describe the formulation of these models and the compu- tational approach used to evaluate or approximate the log-likelihood of a model/data/parameter value combination.},
author = {Bates, Douglas M.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bates/Unknown/Bates - 2008 - Computational methods for mixed models.pdf:pdf},
title = {{Computational methods for mixed models}},
year = {2008}
}
@article{Shmueli2010,
abstract = {Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. In many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. Conflation between explanation and prediction is common, yet the distinction must be understood for progressing scientific knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process.},
archivePrefix = {arXiv},
arxivId = {1101.0891},
author = {Shmueli, Galit},
doi = {10.1214/10-STS330},
eprint = {1101.0891},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Shmueli/Statistical Science/Shmueli - 2010 - To Explain or to Predict.pdf:pdf},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Explanatory modeling, causality, predictive modeli,and phrases,causality,data mining,eling,explanatory modeling,predictive mod-,predictive power,scientific research,statistical strategy},
number = {3},
pages = {289--310},
title = {{To Explain or to Predict?}},
url = {http://www.jstor.org/stable/41058949%5Cnhttp://www.jstor.org/stable/pdfplus/10.2307/41058949.pdf?acceptTC=true},
volume = {25},
year = {2010}
}
@article{Dowe2007a,
abstract = {The advent of formal definitions of the simplicity of a theory has important implications for model selection. But what is the best way to define simplicity? Forster and Sober ([1994]) advocate the use of Akaike's Information Criterion (AIC), a non-Bayesian formalisation of the notion of simplicity. This forms an important part of their wider attack on Bayesianism in the philosophy of science. We defend a Bayesian alternative: the simplicity of a theory is to be characterised in terms of Wallace's Minimum Message Length (MML). We show that AIC is inadequate for many statistical problems where MML performs well. Whereas MML is always defined, AIC can be undefined. Whereas MML is not known ever to be statistically inconsistent, AIC can be. Even when defined and consistent, AIC performs worse than MML on small sample sizes. MML is statistically invariant under 1-to-1 re-parametrisation, thus avoiding a common criticism of Bayesian approaches. We also show that MML provides answers to many of Forster's objections to Bayesianism. Hence an important part of the attack on Bayesianism fails.},
author = {Dowe, David L. and Gardner, Steve and Oppy, Graham},
doi = {10.1093/bjps/axm033},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Dowe, Gardner, Oppy/British Journal for the Philosophy of Science/Dowe, Gardner, Oppy - 2007 - Bayes not bust! Why simplicity is no problem for Bayesians.pdf:pdf},
isbn = {0007-0882},
issn = {00070882},
journal = {British Journal for the Philosophy of Science},
pages = {709--754},
title = {{Bayes not bust! Why simplicity is no problem for Bayesians}},
volume = {58},
year = {2007}
}
@book{davis2006direct,
author = {Davis, Timothy A},
publisher = {Siam},
title = {{Direct methods for sparse linear systems}},
volume = {2},
year = {2006}
}
@article{Rogers2007,
abstract = {Recently, the null category noise model has been proposed as a simple and elegant solution to the problem of incorporating unlabeled data into a Gaussian process (GP) classification model. In this paper, we show how this binary likelihood model can be generalised to the multi-class setting through the use of the multinomial probit GP classifier. We present a Gibbs sampling scheme for sampling the GP parameters and also derive a more efficient variational updating scheme. We find that the performance improvement is roughly consistent with that observed in binary classification and that there is no significant difference in classification performance between the Gibbs sampling and variational schemes.},
author = {Rogers, Simon and Girolami, Mark},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Rogers, Girolami/JLMR Workshop and Conference Proceedings/Rogers, Girolami - 2007 - Multi-class Semi-supervised Learning With The $epsilon$-truncated Multinomial Probit Gaussian Process.pdf:pdf},
journal = {JLMR: Workshop and Conference Proceedings},
number = {2006},
pages = {17--32},
title = {{Multi-class Semi-supervised Learning With The $\epsilon$-truncated Multinomial Probit Gaussian Process}},
year = {2007}
}
@book{Owen2001,
author = {Owen, AB},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Owen/Unknown/Owen - 2001 - Empirical likelihood.pdf:pdf},
isbn = {1584880716},
pages = {1--289},
publisher = {Chapman & Hall/CRC},
title = {{Empirical likelihood}},
url = {http://books.google.com/books?hl=en&lr=&id=QUDPCpc1TNMC&oi=fnd&pg=PR13&dq=Empirical+Likelihood&ots=ZRUXp3q95i&sig=eVOxHyieOEAwcgV2Oa4eq_LGie8},
year = {2001}
}
@incollection{Geweke1992,
abstract = {Data augmentation and Gibbs sampling are two closely related, sampling-based approaches to the calculation of posterior moments. The fact that each produces a sample whose constituents are neither independent nor identically distributed complicates the assessment of convergence and numerical accuracy of the approximations to the expected value of functions of interest under the posterior. In this paper methods from spectral analysis are used to evaluate numerical accuracy formally and construct diagnostics for convergence. These methods are illustrated in the normal linear model with informative priors, and in the Tobit-censored regression model.},
author = {Geweke, John},
booktitle = {Bayesian Statistics 4},
pages = {169--193},
title = {{Evaluating the accuracy of sampling-based approaches to the calculation of posterior moments}},
url = {http://www.mpls.frb.org/research/SR/SR148.pdf},
year = {1992}
}
@incollection{Raftery1992,
abstract = {When the Gibbs sampler is used to estimate posterior distributions (Gelfand and Smith, 1990), the question of how many iterations are required is central to its implementation. When interest focuses on quantiles of functionals of the posterior distribution, we describe an easily-implemented method for determining the total number of iterations required, and also the number of initial iterations that should be discarded to allow for "burn-in". The method uses only the Gibbs iterates themselves, and does not, for example, require external specification of characteristics of the posterior density. Here the method is described for the situation where one long run is generated, but it can also be easily applied if there are several runs from different starting points. It also applies more generally to Markov chain Monte Carlo schemes other than the Gibbs sampler. It can also be used when several quantiles are to be estimated, when the quantities of interest are probabilities rather than full posterior distributions, and when the draws from the posterior distribution are required to be approximately independent. The method is applied to several different posterior distributions. These include},
author = {Raftery, Adrian E and Lewis, Steven},
booktitle = {Bayesian Statistics},
issn = {0160-0176},
pages = {763----773},
title = {{How many iterations in the Gibbs sampler?}},
year = {1992}
}
@article{McDonald1973,
author = {McDonald, Gary C and Schwing, Richard C},
journal = {Technometrics},
number = {3},
pages = {463--481},
publisher = {Taylor & Francis},
title = {{Instabilities of regression estimates relating air pollution to mortality}},
volume = {15},
year = {1973}
}
@book{Everitt2005,
author = {Everitt, Brian Sidney},
file = {:Users/haziqjamil/Downloads/chp%3A10.1007%2F1-84628-124-5_4.pdf:pdf},
keywords = {efa,exploratory factor analysis},
pages = {65--90},
publisher = {Springer Texts in Statistics},
title = {{An R and S-PLUS{\textregistered} Companion to Multivariate Analysis}},
year = {2005}
}
@article{Karcher2001,
author = {Karcher, Peter and Wang, Y},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Karcher, Wang/Journal of Computational and Graphical {\ldots}/Karcher, Wang - 2001 - Generalized Nonparametric Mixed Effects Models.pdf:pdf},
journal = {Journal of Computational and Graphical {\ldots}},
number = {4},
pages = {641--655},
title = {{Generalized Nonparametric Mixed Effects Models}},
url = {http://www.tandfonline.com/doi/abs/10.1198/106186001317243377},
volume = {10},
year = {2001}
}
@book{wickham2015r,
author = {Wickham, Hadley},
publisher = {O'Reilly Media, Inc. http://r-pkgs.had.co.nz},
title = {{R Packages: Organize, Test, Document, and Share Your Code}},
url = {http://r-pkgs.had.co.nz},
year = {2015}
}
@article{Gerber2014,
author = {Gerber, Mathieu and Chopin, N},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Gerber, Chopin/Unknown/Gerber, Chopin - 2014 - Sequential Quasi-Monte-Carlo Sampling.pdf:pdf},
keywords = {array randomized quasi-monte-carlo sampling,low discrepancy,particle filtering,quasi-monte-carlo sampling,randomized quasi-monte-carlo sampling,sequential monte},
title = {{Sequential Quasi-Monte-Carlo Sampling}},
url = {https://rss.org.uk/uploadedfiles/userfiles/files/Gerber_10_12_2014.pdf},
year = {2014}
}
@book{Rasmussen2006,
author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Rasmussen, Williams/Unknown/Rasmussen, Williams - 2006 - Gaussian Processes for Machine Learning.pdf:pdf},
isbn = {0-262-18253-X},
publisher = {The MIT Press},
title = {{Gaussian Processes for Machine Learning}},
year = {2006}
}
@article{Lee2003,
abstract = {Selection of significant genes via expression patterns is an important problem in microarray experiments. Owing to small sample size and the large number of variables (genes), the selection process can be unstable. This paper proposes a hierarchical Bayesian model for gene (variable) selection. We employ latent variables to specialize the model to a regression setting and uses a Bayesian mixture prior to perform the variable selection. We control the size of the model by assigning a prior distribution over the dimension (number of significant genes) of the model. The posterior distributions of the parameters are not in explicit form and we need to use a combination of truncated sampling and Markov Chain Monte Carlo (MCMC) based computation techniques to simulate the parameters from the posteriors. The Bayesian model is flexible enough to identify significant genes as well as to perform future predictions. The method is applied to cancer classification via cDNA microarrays where the genes BRCA1 and BRCA2 are associated with a hereditary disposition to breast cancer, and the method is used to identify a set of significant genes. The method is also applied successfully to the leukemia data. SUPPLEMENTARY INFORMATION: http://stat.tamu.edu/people/faculty/bmallick.html.},
author = {Lee, Kyeong Eun and Sha, Naijun and Dougherty, Edward R and Vannucci, Marina and Mallick, Bani K},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Lee et al/Bioinformatics (Oxford, England)/Lee et al. - 2003 - Gene selection a Bayesian variable selection approach.pdf:pdf},
isbn = {1367-4803 LA - eng PT - Evaluation Studies PT - Journal Article PT - Validation Studies},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
number = {1},
pages = {90--97},
pmid = {12499298},
title = {{Gene selection: a Bayesian variable selection approach}},
volume = {19},
year = {2003}
}
@article{George2000,
author = {George, Edward I},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/George/Journal of the American Statistical Association/George - 2000 - The Variable Selection Problem.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {452},
pages = {1304--1308},
title = {{The Variable Selection Problem}},
volume = {95},
year = {2000}
}
@article{goeman2012penalized,
author = {Goeman, Jelle and Meijer, Rosa and Chaturvedi, Nimisha},
title = {{penalized: L1 (lasso and fused lasso) and L2 (ridge) Penalized Estimation in GLMs and in the Cox Model}},
year = {2012}
}
@article{Bergsma2014a,
author = {Bergsma, Wicher P and Dassios, Angelos},
doi = {10.3150/13-BEJ514},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bergsma, Dassios/Bernoulli/Bergsma, Dassios - 2014 - A consistent test of independence based on a sign covariance related to Kendall's tau.pdf:pdf},
issn = {1350-7265},
journal = {Bernoulli},
month = {may},
number = {2},
pages = {1006--1028},
title = {{A consistent test of independence based on a sign covariance related to Kendall's tau}},
url = {http://projecteuclid.org/euclid.bj/1393594014},
volume = {20},
year = {2014}
}
@article{Keane1992,
abstract = {Although formal conditions for identification in the multinomial probit (MNP) model are now clearly established, little is known about how various estimable MNP specifications perform in practice. This article shows that parameter identification in the MNP model is extremely tenuous in the absence of exclusion restrictions. This previously unnoticed fact is important because formal identification of MNP models does not require exclusion restrictions, and many potential economic applications of MNP are to situations in which exclusion restrictions are not readily available. Thus, failure to be aware of the difficulties present in such situations may lead to reporting of unreliable results.},
author = {Keane, Michael P.},
doi = {10.2307/1391677},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Keane/Journal of Business & Economic Statistics/Keane - 1992 - A Note on Identification in the Multinomial Probit Model.pdf:pdf},
issn = {0735-0015},
journal = {Journal of Business & Economic Statistics},
keywords = {discrete choice,latent variables,minneapolis,mn 55455,of minnesota,parameter estimability},
number = {2},
pages = {193--200},
title = {{A Note on Identification in the Multinomial Probit Model}},
url = {http://www.jstor.org/stable/1391677%5Cnhttp://www.jstor.org/stable/pdfplus/1391677.pdf?acceptTC=true},
volume = {10},
year = {1992}
}
@article{Wang2010,
abstract = {Efficient estimation of parameters is a major objective in analyzing longitudinal data. We propose two generalized empirical likelihood based methods that take into consideration within-subject correlations. A nonparametric version of the Wilks theorem for the limiting distributions of the empirical likelihood ratios is derived. It is shown that one of the proposed methods is locally efficient among a class of within-subject variance-covariance matrices. A simulation study is conducted to investigate the finite sample properties of the proposed methods and compare them with the block empirical likelihood method by You et al. (2006) and the normal approximation with a correctly estimated variance-covariance. The results suggest that the proposed methods are generally more efficient than existing methods which ignore the correlation structure, and better in coverage compared to the normal approximation with correctly specified within-subject correlation. An application illustrating our methods and supporting the simulation study results is also presented.},
author = {Wang, Suojin and Qian, Lianfen and Carroll, Raymond J},
doi = {10.1093/biomet/asp073},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Wang, Qian, Carroll/Biometrika/Wang, Qian, Carroll - 2010 - Generalized empirical likelihood methods for analyzing longitudinal data.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = {mar},
number = {1},
pages = {79--93},
pmid = {20305730},
title = {{Generalized empirical likelihood methods for analyzing longitudinal data.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2841365&tool=pmcentrez&rendertype=abstract},
volume = {97},
year = {2010}
}
@misc{Skrondal2004a,
abstract = {Applications of composite links and exploded likelihoods for general- ized linear latent and mixed models are explored.},
author = {Skrondal, Anders and Rabe-Hesketh, S},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Skrondal, Rabe-Hesketh/Unknown/Skrondal, Rabe-Hesketh - 2004 - Generalized linear latent and mixed models with composite links and exploded likelihoods.pdf:pdf},
keywords = {composite link,ex-,generalized linear latent and,mixed models},
title = {{Generalized linear latent and mixed models with composite links and exploded likelihoods}},
url = {http://www.gllamm.org/composite_conf.pdf},
volume = {1},
year = {2004}
}
@phdthesis{Emerson2009,
author = {Emerson, Sarah},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Emerson/Unknown/Emerson - 2009 - Small sample performance and calibration of the Empirical Likelihood method.pdf:pdf},
number = {September},
school = {Stanford University},
title = {{Small sample performance and calibration of the Empirical Likelihood method}},
url = {http://statweb.stanford.edu/$\sim$owen/students/SCEthesisComp.pdf},
year = {2009}
}
@article{Casella2006,
abstract = {A novel fully automatic Bayesian procedure for variable selection in normal regression models is proposed. The procedure uses the posterior probabilities of the models to drive a stochastic search. The posterior probabilities are computed using intrinsic priors, which can be considered default priors for model selection problems; that is, they are derived from the model structure and are free from tuning parameters. Thus they can be seen as objective priors for variable selection. The stochastic search is based on a Metropolis–Hastings algorithm with a stationary distribution proportional to the model posterior probabilities. The procedure is illustrated on both simulated and real examples.},
author = {Casella, George and Moreno, El{\'{i}}as},
doi = {10.1198/016214505000000646},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Casella, Moreno/Journal of the American Statistical Association/Casella, Moreno - 2006 - Objective Bayesian Variable Selection.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hastings algorithm,intrinsic prior,methods,metropolis,monte carlo markov chain,normal linear regression},
number = {473},
pages = {157--167},
title = {{Objective Bayesian Variable Selection}},
volume = {101},
year = {2006}
}
@article{Kass1995,
author = {Kass, Robert and Raftery, Adrian},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Kass, Raftery/Journal of the American Statistical Association/Kass, Raftery - 1995 - Bayes Factors.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {430},
pages = {773--795},
title = {{Bayes Factors}},
volume = {90},
year = {1995}
}
@article{Yau2002,
abstract = {Generalized linear mixed models (GLMMs) are widely used to analyse non-normal response data with extra-variation, but non-robust estimators are still routinely used. We propose robust methods for maximum quasi-likelihood and residual maximum quasi-likelihood estimation to limit the influence of outlying observations in GLMMs. The estimation procedure parallels the development of robust estimation methods in linear mixed models, but with adjustments in the dependent variable and the variance component. The methods proposed are applied to three data sets and a comparison is made with the nonparametric maximum likelihood approach. When applied to a set of epileptic seizure data, the methods proposed have the desired effect of limiting the influence of outlying observations on the parameter estimates. Simulation shows that one of the residual maximum quasi-likelihood proposals has a smaller bias than those of the other estimation methods. We further discuss the equivalence of two GLMM formulations when the response variable follows an exponential family. Their extensions to robust GLMMs and their comparative advantages in modelling are described. Some possible modifications of the robust GLMM estimation methods are given to provide further flexibility for applying the method.},
author = {Yau, Kelvin K. W. and Kuk, Anthony Y. C.},
doi = {10.1111/1467-9868.00327},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Yau, Kuk/Journal of the Royal Statistical Society Series B (Statistical Methodology)/Yau, Kuk - 2002 - Robust estimation in generalized linear mixed models.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
month = {jan},
number = {1},
pages = {101--117},
title = {{Robust estimation in generalized linear mixed models}},
url = {http://doi.wiley.com/10.1111/1467-9868.00327 http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00327/full},
volume = {64},
year = {2002}
}
@book{Akritas2002,
author = {Akritas, M. G. and Kuha, Jouni and Osgood, D. W.},
booktitle = {Sociological Methods & Research},
doi = {10.1177/0049124102030003006},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Akritas, Kuha, Osgood/Sociological Methods & Research/Akritas, Kuha, Osgood - 2002 - A Nonparametric Approach to Matched Pairs with Missing Data.pdf:pdf},
isbn = {0049124102030},
issn = {0049-1241},
month = {feb},
number = {3},
pages = {425--454},
title = {{A Nonparametric Approach to Matched Pairs with Missing Data}},
url = {http://smr.sagepub.com/cgi/doi/10.1177/0049124102030003006},
volume = {30},
year = {2002}
}
@article{Lindstrom1988,
author = {Lindstrom, Mary J. and Bates, Douglas M.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Lindstrom, Bates/Journal of the American {\ldots}/Lindstrom, Bates - 1988 - Newton Raphson and EM Algorithms for Linear Mixed Effects models for Repeated Measures Data.pdf:pdf},
journal = {Journal of the American {\ldots}},
number = {404},
pages = {1014--1022},
title = {{Newton Raphson and EM Algorithms for Linear Mixed Effects models for Repeated Measures Data}},
url = {http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1988.10478693},
volume = {83},
year = {1988}
}
@article{Kosmidis2011,
author = {Kosmidis, Ioannis and Firth, David},
doi = {10.1093/biomet/asr026},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Kosmidis, Firth/Biometrika/Kosmidis, Firth - 2011 - Multinomial logit bias reduction via the Poisson log-linear model.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = {jul},
number = {3},
pages = {755--759},
title = {{Multinomial logit bias reduction via the Poisson log-linear model}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/asr026},
volume = {98},
year = {2011}
}
@article{George1993,
author = {George, Edward I and McCulloch, Robert E},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/George, McCulloch/Journal of the American Statistical Association/George, McCulloch - 1993 - Variable Selection Via Gibbs Sampling.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {423},
pages = {881--889},
title = {{Variable Selection Via Gibbs Sampling}},
url = {http://www.jstor.org/stable/2290777?seq=1#page_scan_tab_contents},
volume = {88},
year = {1993}
}
@article{Liu2014,
author = {Liu, Ruitao and Chakrabarti, Arijit and Samanta, Tapas and Ghosh, Jayanta K and Ghosh, Malay},
doi = {10.1214/14-BA862},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Liu et al/Bayesian Analysis/Liu et al. - 2014 - On divergence measures leading to Jeffreys and other reference priors.pdf:pdf},
issn = {19316690},
journal = {Bayesian Analysis},
keywords = {Jeffreys prior,Reference prior,$\alpha$-divergences},
number = {2},
pages = {331--370},
title = {{On divergence measures leading to Jeffreys and other reference priors}},
volume = {9},
year = {2014}
}
@misc{Schoch2011,
author = {Schoch, Tobias},
file = {:Users/haziqjamil/Downloads/rsae.pdf:pdf},
title = {{Robust Small Area Estimation: a Vignette}},
year = {2011}
}
@article{Girolami2006,
abstract = {SUMMARY: Vbmp is an R package for Gaussian Process classification of data over multiple classes. It features multinomial probit regression with Gaussian Process priors and estimates class posterior probabilities employing fast variational approximations to the full posterior. This software also incorporates feature weighting by means of Automatic Relevance Determination. Being equipped with only one main function and reasonable default values for optional parameters, vbmp combines flexibility with ease of usage as is demonstrated on a breast cancer microarray study. AVAILABILITY: The R library vbmp implementing this method is part of Bioconductor and can be downloaded from http://www.dcs.gla.ac.uk/$\sim$girolami},
author = {Girolami, Mark and Rogers, Simon},
doi = {10.1093/bioinformatics/btm535},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Girolami, Rogers/Neural Computation/Girolami, Rogers - 2006 - Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Bayes Theorem,Biological,Biological: analysis,Breast Neoplasms,Breast Neoplasms: diagnosis,Breast Neoplasms: metabolism,Gene Expression Profiling,Gene Expression Profiling: methods,Humans,Neoplasm Proteins,Neoplasm Proteins: analysis,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition,Programming Languages,Regression Analysis,Software,Tumor Markers},
number = {8},
pages = {1790--1817},
title = {{Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors}},
volume = {18},
year = {2006}
}
@misc{d1999using,
author = {D'Souza, Aaron A.},
title = {{Using EM to estimate a probablity density with a mixture of gaussians}},
year = {1999}
}
@article{kachman2000introduction,
author = {Kachman, Stephen D.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Kachman/Proceedings of a symposium at the organizational meeting for a NCR coordinating committee on “Implementation Strategies for National Bee./Kachman - 2000 - An introduction to generalized linear mixed models.pdf:pdf},
journal = {Proceedings of a symposium at the organizational meeting for a NCR coordinating committee on “Implementation Strategies for National Beef Cattle Evaluation,” Athens},
pages = {59--73},
title = {{An introduction to generalized linear mixed models}},
url = {http://statistics.unl.edu/faculty/steve/glmm/paper.pdf},
year = {2000}
}
@article{Schwarz1978,
abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
author = {Schwarz, Gideon},
doi = {10.1214/aos/1176344136},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Schwarz/The Annals of Statistics/Schwarz - 1978 - Estimating the dimension of a model.pdf:pdf},
isbn = {0780394224},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {2},
pages = {461--464},
pmid = {2958889},
title = {{Estimating the dimension of a model}},
url = {http://projecteuclid.org/euclid.aos/1176344136},
volume = {6},
year = {1978}
}
@article{Song2005,
author = {Song, Peter Xuekun and Fan, Yanqin and Kalbfleisch, John D},
doi = {10.1198/016214505000000204},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Song, Fan, Kalbfleisch/Journal of the American Statistical Association/Song, Fan, Kalbfleisch - 2005 - Maximization by Parts in Likelihood Inference.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {copula model,fixed-point algorithm,generalized linear mixed model,information dominance,iterative algorithm,non-,normal random effects,score equation,state-space model},
month = {dec},
number = {472},
pages = {1145--1158},
title = {{Maximization by Parts in Likelihood Inference}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214505000000204},
volume = {100},
year = {2005}
}
@article{Bergsma,
abstract = {Cramer's V and Tschuprow's T are closely related nominal variable association mea- sures, which are usually estimated by their empirical values. Although these estimators are consistent, they can have large bias for finite samples, making interpretation difficult. We propose a new and simple bias correction and show via simulations that, for larger than 2×2 tables, the newly obtained estimators outperform the classical (empirical) ones. For 2 × 2 tables performance is comparable. The larger the table and the smaller the sample size, the greater the superiority of the new estimators. 1},
author = {Bergsma, Wicher P},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bergsma/Unknown/Bergsma - Unknown - A bias-correction for Cramer's V and Tschuprow's T.pdf:pdf},
pages = {1--9},
title = {{A bias-correction for Cramer's V and Tschuprow's T}}
}
@article{Washbrook2013a,
abstract = {We consider the effect of non-ignorable dropout in the analysis of residential mobility in household panel surveys. To investigate the effect of such dropout, we consider two types of selection model: the first allows dropout to depend directly on the individual's potentially missing moving status, and the second is a Heckman-type selection model with correlated errors. We discuss the identification and estimation of these models and use simulations to study the role of exclusion restrictions in minimizing the dependence of inferences on unverifiable parametric assumptions. The models are both applied to data from the British Household Panel Survey.},
author = {Washbrook, Elizabeth and Clarke, Paul S. and Steele, Fiona},
doi = {10.1111/rssc.12028},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Washbrook, Clarke, Steele/Journal of the Royal Statistical Society Series C (Applied Statistics)/Washbrook, Clarke, Steele - 2013 - Investigating non-ignorable dropout in panel studies of residential mobility.pdf:pdf},
issn = {00359254},
journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
keywords = {bivariate probit,heckman selection model,informative dropout,instrumental,missing data,selection model,variable},
month = {sep},
title = {{Investigating non-ignorable dropout in panel studies of residential mobility}},
url = {http://doi.wiley.com/10.1111/rssc.12028 http://onlinelibrary.wiley.com/doi/10.1111/rssc.12028/full},
year = {2013}
}
@article{Booth1999,
author = {Booth, J. G. and Hobert, J. P.},
doi = {10.1111/1467-9868.00176},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Booth, Hobert/Journal of the Royal Statistical Society Series B (Statistical Methodology)/Booth, Hobert - 1999 - Maximizing generalized linear mixed model likelihoods with an automated Monte Carlo EM algorithm.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {approximation,con,dence ellipsoid,hastings,importance sampling,laplace,markov chain monte carlo,method,metropolis algorithm,rejection sampling,salamander data,sandwich},
month = {feb},
number = {1},
pages = {265--285},
title = {{Maximizing generalized linear mixed model likelihoods with an automated Monte Carlo EM algorithm}},
url = {http://doi.wiley.com/10.1111/1467-9868.00176},
volume = {61},
year = {1999}
}
@article{Zellner1986,
author = {Zellner, Arnold},
file = {:Users/haziqjamil/Downloads/Zellner-1986.pdf:pdf},
isbn = {0444877126},
journal = {Bayesian Inference and Decision Techniques: Essays in Honor of Bruno de Finetti},
keywords = {Arnold,Zellner},
pages = {233--243},
title = {{On assessing prior distributions and Bayesian regresison analysis with g-prior distributions}},
year = {1986}
}
@article{Jaynes1957,
abstract = {Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge, and leads to a type of statistical inference which is called the maximum-entropy estimate. it is the least biased estimate possible on the given information; i.e., it is maximally noncommital with regard to the missing information. If one considers statistical mechanics as a form of statistical inference rather than as a physical theory, it is found that the usual computational rules, starting with the determination of the partition function, are an immediate consequence of the maximum-entropy principle. In the resulting "subjective statistical mechanics", the usual rules are thus justified independently of any physical argument, and in particular independently of experimental verification; whether or not the results agree with experiment, they still represent the best estimates that cound have been made on the basis of the information available. It is concluded that statistical mechancis need not be regarded as a physical theory dependent for its validity on the truth of additional assumptions not contained in the laws of mechanics (such as ergodicity, metric transitivity, equal a priori probabilities, etc.). Furthermore, it is possible to maintain a sharp distinction between its physical and statistical aspects. The former consists only of the correct enumeration of the states of a system and their properties; the latter is a straightforward example of statistical inference.},
author = {Jaynes, Edwin T.},
doi = {10.1103/PhysRev.108.171},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Jaynes/Physical Review/Jaynes - 1957 - Information theory and statistical mechanics.pdf:pdf},
isbn = {1536-6065},
issn = {<null>},
journal = {Physical Review},
keywords = {information theory,statistical mechanics},
number = {4},
pages = {620--630},
pmid = {17798674},
title = {{Information theory and statistical mechanics}},
volume = {106},
year = {1957}
}
@phdthesis{Ryseck2014,
author = {Ryseck, Laura R},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Ryseck/Unknown/Ryseck - 2014 - The search for national identity in post-colonial, multi-communal states the cases of Eritrea and Lebanon, 1941-1991.pdf:pdf},
number = {June},
pages = {1941--1991},
school = {The London School of Economics and Political Science},
title = {{The search for national identity in post-colonial, multi-communal states: the cases of Eritrea and Lebanon, 1941-1991}},
year = {2014}
}
@book{DasGupta2011,
abstract = {The exponential family is a practically convenient and widely used unified family of distributions on finite-dimensional Euclidean spaces parametrized by a finite-dimensional parameter vector. Specialized to the case of the real line, the exponential family contains as special cases most of the standard discrete and continuous distributions that we use for practical modeling, such as the normal, Poisson, binomial, exponential, Gamma, multivariate normal, and so on. The reason for the special status of the exponential family is that a number of important and useful calculations in statistics can be done all at one stroke within the framework of the exponential family. This generality contributes to both convenience and larger-scale understanding. The exponential family is the usual testing ground for the large spectrum of results in parametric statistical theory that require notions of regularity or Cram{\'{e}}r–Rao regularity. In addition, the unified calculations in the exponential family have an element of mathematical neatness. Distributions in the exponential family have been used in classical statistics for decades. However, it has recently obtained additional importance due to its use and appeal to the machine learning community. A fundamental treatment of the general exponential family is provided in this chapter. Classic expositions are available in Barndorff-Nielsen (1978), Brown (1986), and Lehmann and Casella (1998). An excellent recent treatment is available in Bickel and Doksum (2006).},
address = {New York, NY},
author = {DasGupta, Anirban},
doi = {10.1007/978-1-4419-9634-3},
isbn = {978-1-4419-9633-6},
publisher = {Springer New York},
series = {Springer Texts in Statistics},
title = {{Probability for Statistics and Machine Learning}},
url = {http://link.springer.com/10.1007/978-1-4419-9634-3},
year = {2011}
}
@article{Weissman2013,
author = {Weissman, Alexander},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Weissman/Psychometrika/Weissman - 2013 - Global Convergence of the EM Algorithm for Unconstrained Latent Variable Models with Categorical Indicators.pdf:pdf},
journal = {Psychometrika},
keywords = {convex optimization,em algorithm,information theory,kullback,latent class models,latent variable models,leibler divergence,optimal bounds,relative entropy,variational calculus},
number = {1},
pages = {134--153},
title = {{Global Convergence of the EM Algorithm for Unconstrained Latent Variable Models with Categorical Indicators}},
volume = {78},
year = {2013}
}
@book{Little2002,
abstract = {Emphasizes the latest trends in the field. Includes a new chapter on evolving methods. Provides updated or revised material in most of the chapters.},
author = {Little, Roderick J A and Rubin, Donald B},
booktitle = {Statistical analysis with missing data Second edition},
isbn = {0471183865},
pages = {408},
title = {{Statistical Analysis with Missing Data, Second Edition}},
year = {2002}
}
@article{Barbieri2004,
abstract = {Often the goal of model selection is to choose a model for future prediction, and it is natural to measure the accuracy of a future prediction by squared error loss. Under the Bayesian approach, it is commonly perceived that the optimal predictive model is the model with highest posterior probability, but this is not necessarily the case. In this paper we show that, for selection among normal linear models, the optimal predictive model is often the median probability model, which is defined as the model consisting of those variables which have overall posterior probability greater than or equal to 1/2 of being in a model. The median probability model often differs from the highest probability model.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0406464v1},
author = {Barbieri, Maria Maddalena and Berger, James O},
doi = {10.1214/009053604000000238},
eprint = {0406464v1},
file = {:Users/haziqjamil/Downloads/euclid.aos.1085408489.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bayesian linear models,Predictive distribution,Squared error loss,Variable selection},
number = {3},
pages = {870--897},
primaryClass = {arXiv:math},
title = {{Optimal predictive model selection}},
volume = {32},
year = {2004}
}
@article{Geman1984,
abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
author = {Geman, Stuart and Geman, Donald},
doi = {10.1109/TPAMI.1984.4767596},
isbn = {0934613338},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Annealing,Gibbs distribution,MAP estimate,Markov random field,image restoration,line process,relaxation,scene modeling,spatial degradation},
pmid = {22499653},
title = {{Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images}},
volume = {PAMI-6},
year = {1984}
}
@article{Ghosh1994,
author = {Ghosh, M and Rao, J N K},
doi = {10.1214/ss/1177010647},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Ghosh, Rao/Statistical Science/Ghosh, Rao - 1994 - Small area estimation an appraisal.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
number = {1},
pages = {55--93},
title = {{Small area estimation: an appraisal}},
volume = {9},
year = {1994}
}
@article{Bergsma2014,
author = {Bergsma, Wicher P},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bergsma/Unpublished/Bergsma - 2016 - Objective Bayes regression with I-priors.pdf:pdf;:Users/haziqjamil/Documents/Mendeley Desktop/Bergsma/Unpublished/Bergsma - 2016 - Objective Bayes regression with I-priors(2).pdf:pdf},
journal = {[Unpublished]},
title = {{Objective Bayes regression with I-priors}},
year = {2016}
}
@article{Tibshirani2007,
abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
archivePrefix = {arXiv},
arxivId = {1369–7412/11/73273},
author = {Tibshirani, Robert},
doi = {10.1111/j.1467-9868.2011.00771.x},
eprint = {11/73273},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Tibshirani/Journal of the Royal Statistical Society. Series B Statistical Methodology/Tibshirani - 2007 - Regression Shrinkage and Selection via the Lasso.pdf:pdf},
isbn = {1467-9868},
issn = {0035-9246},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {1-penalty,Penalization,Regularization,l},
number = {1},
pages = {267--288},
pmid = {1000198927},
primaryClass = {1369–7412},
title = {{Regression Shrinkage and Selection via the Lasso}},
volume = {58},
year = {2007}
}
@book{Ntzoufras2008,
author = {Ntzoufras, Ioannis},
booktitle = {Wiley},
doi = {10.1002/9780470434567.ch11},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Ntzoufras/Wiley/Ntzoufras - 2011 - Bayesian Modeling Using WinBUGS.pdf:pdf},
keywords = {Bayes factors,Monte Carlo estimators,harmonic mean estimators,marginal likelihood,prior predictive distributions},
pages = {389--433},
publisher = {Wiley},
title = {{Bayesian Modeling Using WinBUGS}},
year = {2011}
}
@article{Xu2011,
author = {Xu, Ximing and Reid, N.},
doi = {10.1016/j.jspi.2011.03.026},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Xu, Reid/Journal of Statistical Planning and Inference/Xu, Reid - 2011 - On the robustness of maximum composite likelihood estimate.pdf:pdf},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Consistency,Godambe information,Model misspecification,Pseudo-likelihood},
month = {sep},
number = {9},
pages = {3047--3054},
publisher = {Elsevier},
title = {{On the robustness of maximum composite likelihood estimate}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0378375811001236},
volume = {141},
year = {2011}
}
@article{Vonesh2002,
author = {Vonesh, Edward F and Wang, Hao and Nie, Lei and Majumdar, Dibyen},
doi = {10.1198/016214502753479400},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Vonesh et al/Journal of the American Statistical Association/Vonesh et al. - 2002 - Conditional Second-Order Generalized Estimating Equations for Generalized Linear and Nonlinear Mixed-Effects Mode.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {asymptotics,first-order linearization,gaussian random effects,laplace approximation,penalized extended least,quadratic exponential family,squares},
month = {mar},
number = {457},
pages = {271--283},
title = {{Conditional Second-Order Generalized Estimating Equations for Generalized Linear and Nonlinear Mixed-Effects Models}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214502753479400},
volume = {97},
year = {2002}
}
