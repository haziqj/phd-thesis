



The Bayesian variable selection methods that we have seen have the appeal of reducing the problem of model search into one of estimation. At the outset, we aimed to seek a model which: 1) requires little tuning on the part of the user; 2) would work well in the presence of multicollinearity; and 3) is able to work well with little to no prior information. We think we have found this in the I-prior variable selection model.

While we have seen very promising simulation results in which I-prior selects the correct variables, there are instances in the real-world cases where the I-prior method results in models which are not so good at prediction (as determined by RMSE). The first thing to note is that the underlying model of the I-prior variable selection method is that of a (multiple) linear regression model. Thus, the predictive or explanatory abilities of the resulting models are only as a good as that of a linear model. As we know, one can always beat the linear regression model in terms of predictive abilities by considering models of higher complexity, such as non-linear models. In such cases, model complexity is sacrificed for an improvement in expected prediction error. \cite{Shmueli2010} argues that the ``wrong'' model can sometimes predict better than the correct one.

The second thing to note is that the criteria for these Bayesian selection methods are are unlike their frequentist counterparts. The models are selected on the basis of ``which of these variables are most likely to be correct, given the data?'', i.e. the posterior probabilities of the indicator variables $\boldsymbol{\gamma}$. This is unlike selecting models via optimisation of some criteria, say the AIC, Mallow's $C_p$, cross-validation error, etc. Perhaps measuring the validity of these Bayesian-based selected models based on frequentist criteria may be a little unfair.

Ultimately, the I-prior Bayesian variable selection method should be seen as a tool for a researcher to conduct their first level of analysis, insofar as using the methodology to objectively reduce the variable space (and hence complexity), or to determine which variables are most important, before continuing on further analysis. While we have implicitly made out the posterior model probabilities to be the main statistic of interest, the posterior inclusion probabilities for each variable can also be used to give some insight as to which variables are worth keeping.

In term of future direction, there may be potential in using the I-prior in cases where $p > n$ by considering dimension reduction. It might also be worthwhile to explore the use of I-priors in variable selection for other generalised linear models such as modelling Poisson counts or categorical responses.

