\subsection{EM Algorithm}

Substituting \eqref{eq:iprior-re} into \eqref{eq:linmod}, we can rewrite the I-prior model in a ``random-effects'' representation. Using matrix notation, we have%For $i = 1,\dots,n$
%\begin{align*}
%	\begin{gathered}
%		y_i = \alpha + \sum_{j=1}^n h_\lambda(x_i, x_j) w_j + \epsilon_i \\
%		w_1, \dots, w_n \overset{\text{iid}}{\sim} \N(0, \psi) \\
%		\epsilon_1, \dots, \epsilon_n \overset{\text{iid}}{\sim} \N(0, \psi^{-1}), \\
%	\end{gathered}
%\end{align*}
\begin{align}\label{eq:iprior-re2}
	\begin{gathered}
		\mathbf y = \boldsymbol{\alpha} + \Hlam\bw + \boldsymbol\epsilon \\
		\bw := (w_1, \dots, w_n) \sim \N(\mathbf 0, \psi\mathbf I_n) \\
		\boldsymbol\epsilon := (\epsilon_1, \dots, \epsilon_n) \sim \N(\mathbf 0, \psi^{-1} \mathbf I_n) 
	\end{gathered}
\end{align}
where the intercept $\boldsymbol\alpha = \alpha \mathbf 1_{n}$ has been separated from the regression function. 
Here, $\mathbf 1_{n}$ is a vector of length $n$ containing all ones, and $\Hlam$ is the matrix whose $(i,j)$th entries are $h_\lambda(x_i, x_j)$, where $h_\lambda$ is the (scaled) reproducing kernel over the set of covariates. 
An EM algorithm can be applied by treating the random effects $w_1, \dots, w_n$ as `missing' in order to estimate the parameters $\alpha$, $\blam$ and $\psi$. 
The assumption of normality also makes the EM algorithm particularly appealing as the required joint and conditional distributions are easy to obtain. 
To start, write $\mathbf y \sim \N(\boldsymbol\alpha, \mathbf V_y)$, where $\mathbf V_y = \psi{\mathbf H}_{\lambda}^2 + \psi^{-1}\mathbf I_n$ (the marginal distribution of the responses). 
Given the random effects $\bw$, the distribution of $\mathbf y|\bw$ is also multivariate normal with mean $\boldsymbol\alpha + \mathbf H_\lambda$ and covariance matrix $\psi^{-1}\mathbf I_n$.

The covariance between $\mathbf y$ and $\bw$ is
\begin{align*}
\Cov[\by, \bw] &= \Cov[\balpha + \Hlam \bw + \bepsilon, \bw] \\
&= \cancelto{0}{\Cov[\balpha, \bw]} + \Hlam \cancelto{\psi\bI_n}{\Cov[\bw, \bw]} +  \cancelto{0}{\Cov[\bepsilon, \bw]} \\
&= \psi\Hlam := \Cov[\bw, \mathbf y].
\end{align*}
Thus, the joint distribution of $(\mathbf y, \bw)$ is
\[
	\begin{pmatrix}
		\mathbf y \\
		\bw
	\end{pmatrix}
	\sim \N \left(
	\begin{pmatrix}
		\boldsymbol{\alpha} \\
		\mathbf 0
	\end{pmatrix},
	\begin{pmatrix}
		\mathbf V_y				&\psi\mathbf H_{\lambda} \\
		\psi\mathbf H_{\lambda}	&\psi\mathbf I_n
	\end{pmatrix}
	\right).
\]

Using standard results of multivariate normal distributions (see, for example, \citealt{krzanowski2000principles}), the conditional distribution of $\bw$ given $\mathbf y$ is normal with mean and variance
\begin{align}\label{eq:wcondy}
\E[\bw|\mathbf y] &= \E[\bw] + \Cov[\bw, \mathbf y] \, \Var[\mathbf y]^{-1} \, (\mathbf y - \E[\mathbf y]) \nonumber  \\
&= \psi\mathbf H_{\lambda} \mathbf V_y^{-1} (\mathbf y - \boldsymbol\alpha) =: \wtilde \nonumber \\
\\
\Var[\bw|\mathbf y] &= \Var[\bw] + \Cov[\bw, \mathbf y] \, \Var[\mathbf y]^{-1} \, \Cov[\mathbf y, \bw] \nonumber \\
&= \psi\mathbf I_n - \psi^2\mathbf H_{\lambda} \mathbf V_y^{-1} \mathbf H_{\lambda} \nonumber \\
&= \bV_y^{-1} =: \Vtilde_w \nonumber
\end{align}
where the last equality in the derivation of the conditional variance is obtained using the Woodbury matrix identity. Also, write the second posterior moment of the random effects as
\begin{equation}
\begin{aligned}\label{eq:Wtilde}
	\E[\bw\bw^\top|\mathbf y] &= \Var[\bw|\mathbf y] + \E[\bw|\mathbf y]\E[\bw|\mathbf y]^\top \\
	&= \Vtilde_w + \wtilde\wtilde^\top =: \Wtilde.
\end{aligned}
\end{equation}
	

From the marginal distribution of the responses, we notice that the mean and variance parameters are separable (i.e., they are not dependent on each other). It is straightforward to see that the maximum likelihood estimate for $\alpha$ is $\hat\alpha = \bar y = \sum_{i=1}^n y_i / n$. We can use this fact and treat the intercept parameter as known.

With $g$ denoting the relevant density functions, the complete data log-likelihood of $\boldsymbol\lambda$ and $\psi$ is given by
\begin{align*}
	l(\boldsymbol{\lambda}, \psi|\mathbf y, \bw)
	&= \log g(\mathbf y, \bw; \hat\alpha, \boldsymbol{\lambda}, \psi) \\
	&= \log g(\mathbf y; \hat\alpha, \boldsymbol{\lambda}, \psi) + \log g(\bw; \psi)  \\
	&= - n \log 2\pi - \cancel{\half\log|\psi^{-1}\mathbf I_n|} - \frac{\psi}{2} \Vert \mathbf y - \boldsymbol{\hat\alpha} - \mathbf H_\lambda\bw \Vert^2 \cancel{ - \half\log|\psi\mathbf I_n|} - \frac{\psi}{2}\bw ^\top \bw  \\
	&= - n \log 2\pi - \frac{\psi}{2} \Vert \mathbf y - \boldsymbol{\hat\alpha} \Vert^2 - \frac{\psi}{2}\bw^\top\mathbf H_\lambda^2\bw + \psi(\mathbf y - \boldsymbol{\hat\alpha})^\top \mathbf H_\lambda\bw -\frac{1}{2\psi} \bw ^\top \bw  \\
	&= - n \log 2\pi - \frac{\psi}{2} \Vert \mathbf y - \boldsymbol{\hat\alpha} \Vert^2 - \half\bw^\top( {\color{gray} \underbrace{{\color{black} \, \psi\mathbf H_\lambda^2 + \psi^{-1}\mathbf I_n}}_{\mathbf V_y} \, } )\bw + \psi(\mathbf y - \boldsymbol{\hat\alpha})^\top \mathbf H_\lambda\bw \\
	&= - n \log 2\pi - \frac{\psi}{2} \Vert \mathbf y - \boldsymbol{\hat\alpha} \Vert^2 - \half \tr \left( \mathbf V_y  \bw \bw^\top \right) + \psi(\mathbf y - \boldsymbol{\hat\alpha})^\top \mathbf H_\lambda\bw.
\end{align*}

The EM algorithm at iteration $t \in \{0, 1, \dots\}$ entails taking the expectation of the above complete data log-likelihood under $\mathbf w$ (the E-step, conditional on the responses $\mathbf y$ and some parameter values $(\boldsymbol{\lambda}^{(t)}, \psi^{(t)})$. Making use of the results in \eqref{eq:wcondy} and \eqref{eq:Wtilde}, we denote the $t$th iteration E-step by the function
\begin{align*}
  Q(\blam, \psi)
  &= \E_{\bw}\!\Big[\, l(\blam, \psi | \mathbf y, \bw) \, \Big| \, \mathbf y, \blam^{(t)}, \psi^{(t)} \Big]  \\
  &= - n \log 2\pi - \frac{\psi}{2} \Vert \mathbf y - \hat{\boldsymbol\alpha} \Vert^2 - \half \tr \left( \mathbf V_y \Wtilde^{(t)} \right) + \psi(\mathbf y - \hat{\boldsymbol\alpha})^\top \mathbf H_\lambda \wtilde^{(t)}.
\end{align*}
The M-step entails maximizing this $Q$ function with respect to the parameters, which then boils down to solving the system of differential equations
\begin{align}
	\frac{\partial Q(\blam, \psi)}{\partial \lambda_k}
	&= - \half \tr \left( \frac{\partial \mathbf V_y}{\partial \lambda_k} \tilde{\bW}^{(t)} \right) + \psi(\mathbf y - \hat{\boldsymbol\alpha})^\top \frac{\partial\mathbf H_\lambda}{\partial \lambda_k} \tilde{\bw}^{(t)} \label{eq:der1} \\
	\frac{\partial Q(\blam, \psi)}{\partial \psi}
	&= - \half \Vert \mathbf y - \hat{\boldsymbol\alpha} \Vert^2 - \tr \left( \frac{\partial \mathbf V_y}{\partial \psi} \tilde{\bW}^{(t)} \right) + (\mathbf y - \hat{\boldsymbol\alpha})^\top \mathbf H_\lambda \tilde{\bw}^{(t)} \label{eq:der2}
\end{align}
equated to zero for $k = 1, \dots, p$. The algorithm is made simpler by first conditioning on a value of $\psi$ to obtain updated values for $\boldsymbol\lambda$, and then conditioning on these $\boldsymbol\lambda$ values to obtain an update for $\psi$. Given some starting values $(\boldsymbol{\lambda}^{(0)}, \psi^{(0)})$, the E-step and the M-step are iterated until convergence is obtained. A practical stopping criterion would be when there is no longer a sizeable increase in the marginal log-likelihood value, i.e., iterate until
\[
	\log g(\mathbf y; \boldsymbol{\lambda}^{(t+1)}, \psi^{(t+1)}) - \log g(\mathbf y; \boldsymbol{\lambda}^{(t)}, \psi^{(t)}) < \delta.
\]
for some small value $\delta \in \mathbb R$.

For models with iid errors, such as the ones we are considering in this paper, the solution for $\psi$ in \eqref{eq:der2} has the closed-form expression 
\[
	\psi^{(t+1)} = \frac{\tr ( \Wtilde^{(t)} )}{\Vert \mathbf y - \hat{\boldsymbol\alpha} \Vert^2 + \tr(\Hlam^2 \Wtilde) - 2(\mathbf y - \hat{\boldsymbol\alpha})^\top\mathbf \Hlam \wtilde} .
\]
This is generally not the case for the scale parameters $\blam$, but even so, the system of equations can still be solved using numerical methods. In the next section, we describe cases when closed-form solutions exists for $\blam$, and how to derive them.



\subsection{Estimation of the scale parameters}

As long as (A) there are no covariates involving square, cubic or any other higher order terms; and (B) the maximum order of interactions between all covariates is two, then the solution to the M-step in \eqref{eq:der1} involving $\blam$ can be found to be in closed-form. This includes models with either a single or multiple scale parameter(s) with two-way interactions between some or all of the terms. For any other models such as ones involving squared terms and three-way interactions, the M-step can still be solved using numerical methods such as a downhill simplex method. We proceed under the assumptions (A) and (B).

Assume further that there are $p$ covariates, and each of the $p$ kernel matrices $\mathbf H_1, \dots, \mathbf H_p$ for the covariates are calculated (depending on whether the data is continuous or nominal). If two-way interactions are present between any $k,j \in \{1,\dots,p\}$, then these are also calculated as $\mathbf H_{kj} = \mathbf H_k \circ \mathbf H_j$ (the Hadamard product).

Let the number of unique scale parameters be $p$, i.e., one for each covariate. While the number of scale parameters could actually be less than $p$, implying that some of the covariates share a scale parameter. This can be thought of as a multi-dimensional covariate. In any case, for a group of such covariates, the kernel matrix is simply the sum of each of the kernel matrices, and all we have to do is re-index everything based on the number of kernel matrices there are, and we are back to $p$ (the number of kernel matrices).

In general, the scaled kernel matrix looks like
\[
	\mathbf H_{\lambda} = \sum_{k=1}^p \lambda_k \mathbf H_k + \sum_{k,j\in \mathcal M} \lambda_k\lambda_j \mathbf H_{kj}
\]
where the set $\mathcal M$ is the index of all two way interaction terms between the $p$ covariates, i.e., $\mathcal M=\{(k,j): k \text{ interacts with } j, \text{ and } k < j, \ \forall k,j=1,\dots,p \}$. Let the number of two-way interactions be $m=|\mathcal M|$. The total number of scale parameters is equal to $q=p+m$ when there are non-parsimonious interactions present, otherwise it is $q=p$. The non-parsimonious method of interactions assigns a new scale parameter for each of the Hadamard products of interacting kernel matrices\footnotemark. In comparison, the parsimonious method multiplies the corresponding scale parameters together. 

\footnotetext{The non-parsimonious method actually re-indexes both the kernel matrices and scale parameters from $\{1,\dots,p\}$ to $\{1,\dots,q\}$ where the Hadamard products are treated like ``regular'' kernel matrices, and the method proceeds as if there are no interactions present.}

For a particular $\lambda_k$, $k=1,\dots,q$, we partition the sum of the kernel matrix into parts which involve $\lambda_k$ and parts which do not:
\begin{align}
	\mathbf H_{\lambda} &= {\color{gray}
	\overbrace{\color{black}\lambda_k{\color{colred}\mathbf H_k} + \lambda_k {\color{colred} \sum_{j\in \mathcal M}\lambda_j\mathbf H_{kj}}\vphantom{\mathop{\sum_{j=1}^p}_{j\neq k}}}^{\lambda_k\text{ is here}}
	{\color{black}+}
	\overbrace{\color{black}{\color{colgre}\mathop{\sum_{j=1}^p}_{j\neq k}\lambda_j \mathbf H_j} + {\color{colblu}\mathop{\sum_{k',j \in \mathcal M}}_{k'\neq k}\lambda_{k'}\lambda_j \mathbf H_{k'j}}}^{\text{no $\lambda_k$ here}}
	} \nonumber \\
	&= \lambda_k\mathbf {\color{colred}P_k} + {\color{colgre}\mathbf R_k} + {\color{colblu}\mathbf U_k}.
\end{align}

$\mathbf P_k$ is the kernel matrix $\mathbf H_k$ plus the sum-product of the interaction kernel matrices with the scale parameters relating to covariate $k$, i.e., $\sum_j \lambda_j\mathbf H_{kj}$. $\mathbf R_k$ is the sum-product of the kernel matrices and scale parameters excluding $\lambda_k\mathbf H_k$. $\mathbf U_k$ is the sum of the interaction cross-product terms excluding those relating to covariate $k$. Thus, the squared kernel matrix is
\begin{align}\label{eq:kernpartsq}
		\mathbf H_{\lambda}^2 =& \ \lambda_k^2\mathbf P_k^2  + \lambda_k \left( \mathbf P_k\mathbf R_k + (\mathbf P_k\mathbf R_k)^\top + \mathbf P_k\mathbf U_k + (\mathbf P_k\mathbf U_k)^\top \right) \nonumber \\
		& + \mathbf R_k^2 + \mathbf U_k^2 + \mathbf R_k\mathbf U_k + (\mathbf R_k\mathbf U_k)^\top.
\end{align}

The M-step from \eqref{eq:der1} for each of the $\lambda_k$ now reduces to solving
\[
	\frac{\partial Q(\blam, \psi)}{\partial \lambda_k}
	= - \frac{\psi}{2} \, \tr \left[ \left( 2\lambda_k\mathbf P_k^2 + \mathbf S_k \right)  \tilde{\bW}^{(t)} \right] + \psi(\mathbf y - \hat{\boldsymbol\alpha})^\top \mathbf P_k \tilde{\bw}^{(t)}
\]
equal to zero, where we have defined $\mathbf S_k = \mathbf P_k\mathbf R_k + (\mathbf P_k\mathbf R_k)^\top + \mathbf P_k\mathbf U_k + (\mathbf P_k\mathbf U_k)^\top$. This yields the solution
\[
	\lambda_k^{(t+1)} = \frac{(\mathbf y - \hat{\boldsymbol\alpha})^\top \mathbf P_k \tilde{\bw}^{(t)} - \half \tr \big( \mathbf S_k \tilde{\bW}^{(t)} \big)}{\tr \big( \mathbf P_k^2 \tilde{\bW}^{(t)} \big)}
\]
for each $k = 1,\dots,p$.

For most cases, $\mathbf P_k$ and $\mathbf S_k$ only depend on the kernel matrices and not on the scale parameters, so can be calculated once and stored for efficiency. Further, $\mathbf U_k$ equals zero for most cases except in the parsimonious multiple scale parameter case thus simplifying calculations. In fact, we can avoid the expensive matrix multiplications involved in evaluating $\mathbf P_k$, its square, and $\mathbf S_k$, by calculating once and storing $\mathbf H_1, \dots, \mathbf H_p$, its squares and all possible two-way matrix multiplications of these kernel matrices, as the relevant calculation of the M-step merely involves a linear combination of these matrices. These operations are conducted by the \code{kernL()} function.

\subsection{Calculation of inverse variance and the likelihood}

Sometimes, the calculation of the inverse of $\mathbf V_y = \psi\mathbf H_\lambda + \psi^{-1}\mathbf I_n$ can become problematic, especially when $\psi$ gets extremely large (or extremely small). Notice that $\mathbf V_y$ is of the form $\mathbf A + s\mathbf I_n$, where $\mathbf A$ is a symmetric and positive-definite matrix and $s$ is a constant. An eigendecomposition of $\mathbf A$ yields $\mathbf A = \mathbf V \mathbf U \mathbf V^\top$, where $\mathbf V$ is the $n \times n$ matrix whose $j$th column is the normalised eigenvector $\mathbf v_j$ of $\mathbf A$ such that $\mathbf V\mathbf V^\top = \mathbf I_n$, and $\mathbf U$ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues, i.e., $\mathbf U_{ii} = u_i$, for $i=1,\dots,n$. Consider then the linear equation
\begin{align*}
	(\mathbf A + s\mathbf I_n)\mathbf a &= (\mathbf V \mathbf U \mathbf V^\top + s  \mathbf V \mathbf V^\top)\mathbf a \\
	&= \mathbf V \, \text{diag}(u_1 \!+\! s, \dots, u_n \!+\! s) \, \mathbf V^\top \mathbf a =: \mathbf b.
\end{align*}
Solving for $\mathbf a$ yields
\begin{align}\label{eq:linsolve}
	\mathbf a = \mathbf V \, \text{diag} \Bigg( \frac{1}{u_1 \!+\! s}, \dots, \frac{1}{u_n \!+\! s} \Bigg) \, \mathbf V^\top \mathbf b.
\end{align}
With $\mathbf A = \psi\Hlam^2$ and $s=1/\psi$, this is a much more stable way of computing $\mathbf a = \Vy^{-1} \mathbf b$, and is also useful for finding the inverse $\mathbf a = \mathbf V_y^{-1}$ by setting $\mathbf b=\mathbf I_n$. %Note that we actually decompose the matrix $\mathbf A = \Hlam = \mathbf V \mathbf U \mathbf V^\top$, and make use of the fact that $\psi\Hlam^2 = \mathbf V (\psi\mathbf U^2)\mathbf V^\top$.

This eigendecomposition is also used for a stable calculation of the log-likelihood. Firstly, the eigenvalues of $\mathbf A + s\mathbf I_n$ are simply $u_1 \!+\! s, \dots, u_n \!+\! s$, and the log-determinant can be calculated as
\[
	\log |\mathbf A + s\mathbf I_n| = \sum_{i=1}^n \log (u_i + s).
\]
Furthermore, the matrix $\Vy^{-1}(\mathbf y - \boldsymbol\alpha)$ is given by $\mathbf a$ in equation \eqref{eq:linsolve} with $\mathbf b = (\mathbf y - \boldsymbol\alpha)$. Thus, the marginal log-likelihood of the responses is given as
\[
	l(\alpha, \boldsymbol{\lambda}, \psi|\mathbf y) = -\frac{n}{2}\log(2\pi) - \half\sum_{i=1}^n \log (u_i + s) - \half(\mathbf y - \boldsymbol\alpha)^\top \mathbf a.
\]

\subsection{Calculation of standard errors}

Consider again the distribution $\by \sim \N (\balpha, \bV_y)$, where the mean and covariance matrix depends on different sets of parameters - namely $\alpha$ for the mean, and $\btheta=(\lambda_1,\dots,\lambda_p,\psi)$ for the covariance matrix. The Fisher information matrix $\cI[\alpha, \btheta]$ then has the form
\[
	\cI[\alpha, \btheta] = \text{diag} \big(\, \cI[\alpha], \cI[\btheta] \, \big),
\]
where
\begin{align*}
	\begin{gathered}
		I[\alpha] = \frac{\partial\boldsymbol{\alpha}^\top}{\partial\alpha} \mathbf V_y^{-1} \frac{\partial\boldsymbol{\alpha}}{\partial\alpha} = \mathbf V_y^{-1} \circ \mathbf J_n \\
		\text{and} \\
		I[\boldsymbol{\theta}]_{ij} = \half\tr\left(\mathbf V_y^{-1}\frac{\partial\mathbf V_y}{\partial\theta_i}\mathbf V_y^{-1}\frac{\partial\mathbf V_y}{\partial\theta_j} \right).
	\end{gathered}
\end{align*}


Recall that $\mathbf V_y = \psi\mathbf H_\lambda^2 + \psi^{-1}\mathbf I_n$ and that from \eqref{eq:kernpartsq}, the derivative of the squared scaled kernel matrix has the form $\partial\mathbf H_\lambda^2 / \partial\lambda_k = 2\lambda_k\mathbf P_k^2 + \mathbf S_k$. Therefore, the partial derivative of $\mathbf V_y$ with respect to $\lambda_k$ for $k = 1,\dots,p$ is
\[
	\frac{\partial\mathbf V_y}{\partial\lambda_k} = \psi\left(2\lambda_k\mathbf P_k^2 + \mathbf S_k \right),
\]
while the partial derivative of $\mathbf V_y$ with respect to $\psi$ is
\begin{align*}
	\frac{\partial\mathbf V_y}{\partial\psi} &= \mathbf H_\lambda^2 - \frac{1}{\psi^2}\mathbf I_n \\
	&= \frac{1}{\psi}\left(\psi\mathbf H_\lambda^2 + \frac{1}{\psi}\mathbf I_n \right) - \frac{2}{\psi^2}\mathbf I_n \\
	&= \frac{1}{\psi}\mathbf V_y - \frac{2}{\psi^2}\mathbf I_n.
\end{align*}
The Fisher information matrix can be obtained fairly inexpensively as the two matrices $\mathbf P_k^2$ and $\mathbf S_k$ have already been calculated through the EM algorithm (if the $\blam$ are in closed-form). Otherwise, there exist \proglang{R} functions to compute the Hessian of the (negative) log-likelihood numerically. The standard error for $\eta \in \{ \alpha, \lambda_1, \dots, \lambda_p, \psi \} =: \mathcal E$ is then given by
\[
	\text{s.e.}(\eta) = \sqrt{\big(I[\alpha,\boldsymbol{\theta}]^{-1}\big)_{kk}},
\]
where $k$ is the index of the parameter in the set $\cE$. The \pkg{iprior} package employs Wald tests of significance based off of these standard errors for the intercept and scale parameters in the \code{summary} of an \code{ipriorMod} object.

\section{The EM algorithm pseudocode}
\label{apx:a}

\hl{Something I just realised...}

Already have the eigendecomposition $\Hlam = \bV \diag(u_1, \dots, u_n) \bV^\top$ with $\bV^\top\bV = \bI_n$. 
Also, $\Wtilde = \bV_y^{-1} + \wtilde\wtilde^\top$, and $\bV_y = \psi\Hlam^2 + \psi^{-1}\bI_n$.
Therefore$\bV_y^{-1} = \bV \diag\big(\frac{1}{\psi u_i^2 + 1/\psi}\big) \bV^\top$.
Then,

\begin{align*}
  \tr(\Hlam^2\Wtilde) 
  &= \tr\big(\Hlam^2(\bV_y^{-1} + \wtilde\wtilde^\top)\big) \\
  &= \tr(\Hlam^2\bV_y^{-1}) + \tr(\Hlam^2\wtilde\wtilde^\top) \\
  &= \tr\left(
  \bV \diag(u_i) 
  \cancel{\bV^\top \bV}
  \diag\left(\frac{1}{\psi u_i^2 + 1/\psi}\right) \bV^\top \right) + 
  \wtilde^\top\Hlam\Hlam\wtilde \\
  &= \tr\left(
  \cancel{\bV^\top\bV}
  \diag\left(\frac{u_i}{\psi u_i^2 + 1/\psi}\right)  
  \right) + 
  \wtilde^\top\Hlam\Hlam\wtilde \\
  &= \sum_{i=1}^n \frac{u_i}{\psi u_i^2 + 1/\psi} + \wtilde^\top\Hlam\Hlam\wtilde
\end{align*}
This way, no need to square any matrices.

Here is the pseudocode for the EM algorithm to estimate the I-prior model. An estimate of the percentage run time for some of the heavier computations are also given.

\algrenewcommand{\algorithmiccomment}[1]{{\color{gray}\hskip2em$\triangleright$ #1}}
\begin{algorithm}[H]
\caption{EM algorithm for the I-prior model}\label{alg:EM1}
\begin{algorithmic}[1]
\Procedure{Initialise (part of the Kernel Loader)}{}
	\State Choose suitable $\boldsymbol\lambda^{(0)} = (\lambda_1^{(0)}, \dots, \lambda_l^{(0)})$ and $\psi^{(0)}$
	\State $t \gets 0$
	\State $\hat\alpha \gets \sum_{i=1}^n y_i/n$ \Comment{The MLE for $\hat{\alpha}$}
%	\State $r \gets $ no. of higher order terms (squared, cubic, etc.)
	\State $p \gets $ no. of covariates %$(l + r)$
	\State $m \gets $ no. of interactions
%	\State $h \gets $ total no. of kernel matrices $(p+m)$
	\State $q \gets$ no. of expanded scale parameters/kernel matrices $(p + m)$
	\For{$k=1,\dots,p$}
    \State Calculate $\mathbf H_k, \mathbf H_k^2$, and any Hadamard products (interactions).
    \State Calculate $\mathbf P_k, \mathbf P_k^2, \mathbf S_k$ using kernel matrices $\mathbf H_k$ and $\boldsymbol{\lambda}^{(0)}$.\Comment{time: 3.4\%}
	\EndFor
	\State Index all the relevant kernel matrices from $1$ to $q$.
\EndProcedure
\Statex
\Procedure{Block A update }{Iteration $0$}
    \State Expand $\boldsymbol{\lambda}_{1:l}^{(t)} \rightarrow \boldsymbol{\lambda}_{1:q}^{(t)}$ depending on interactions and higher order terms
    \State $\mathbf H_\lambda \gets \sum_{k=1}^q \boldsymbol{\lambda}^{(t)}_k \mathbf H_k$
	\Procedure{Eigendecomposition}{} \Comment{time: 51.6\%}
		\State $(\text{diag}(u_1, \dots, u_n), \mathbf V) \gets \text{eigen}(\mathbf H_\lambda)$
	\EndProcedure
	\State $s \gets 1/\psi^{(t)}$	
\EndProcedure
\Statex
\Function{Linear solver and inverse }{input $\mathbf b$}
	\State $\mathbf a \gets \mathbf V \, \text{diag} \left[ \frac{s}{u_1^2 + s^2}, \dots, \frac{s}{u_n^2 + s^2} \right] \mathbf V^\top \mathbf b$
\EndFunction
\Statex
\Procedure{Log-likelihood update}{}
	\State \textbf{call} \Call{Linear solver and inverse }{input $\mathbf b=\mathbf y - \hat{\boldsymbol\alpha}$}
	\State $l \gets -\frac{n}{2}\log(2\pi) - \half\sum_{i=1}^n \log (u_i^2/s + s) - \half(\mathbf y - \boldsymbol\alpha)^\top \mathbf a$
\EndProcedure	
\algstore{EMbreak}	
\end{algorithmic}
\end{algorithm}

%%% CONTINUATION OF ALGORITHM %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[H]
\begin{algorithmic}[1]
\algrestore{EMbreak}
\Procedure{Block B update }{$t$}
    \State Update $\mathbf P_k$ and $\mathbf S_k$. \Comment{time: 15.6\%}
\EndProcedure	
\Statex
\While{$l_{new} - l_{old} > \delta$ \textbf{or} $t < t_{max}$}\Comment{The EM iterations}
	\Procedure{Block C update }{Iteration $t$}
		\State \textbf{call} \Call{Linear solver and inverse }{input $\mathbf b=\mathbf I_n$} \Comment{time: 10.4\%}
		\State $\mathbf V_y^{-1} \gets \mathbf a$
		\State $\wtilde \gets \psi^{(t)} \mathbf H_\lambda \mathbf V_y^{-1}(\mathbf y - \hat{\boldsymbol\alpha})$
		\State $\Wtilde \gets \mathbf V_y^{-1} + \wtilde\wtilde^\top$
	\EndProcedure
	\Statex
	\Procedure{Update for $\boldsymbol\lambda$: Closed-form EM}{}
	\For{$k=1,\dots,l$}
		\State $T_1 \gets \tr\big(\mathbf P_k^2 \Wtilde\big)$
		\State $T_2 \gets (\mathbf y - \hat{\boldsymbol\alpha})\mathbf P_k \wtilde - \half\tr\big(\mathbf S_k\Wtilde\big)$ \Comment{time: 8.5\%}
		\State $\lambda_k^{(t+1)} \gets T_2/T_1$
	\EndFor
	\EndProcedure
	\State \textit{Note: If higher order terms and/or three-way (or more) interactions are present, then a numerical method is used.}
	\Statex
	\Procedure{Update for $\psi$}{} \Comment{time: 5.8\%}
    	\State $T_3 \gets (\mathbf y - \hat{\boldsymbol\alpha})^\top(\mathbf y - \hat{\boldsymbol\alpha}) + \tr(\mathbf H_\lambda^2 \Wtilde) - 2(\mathbf y - \hat{\boldsymbol\alpha})^\top\mathbf H_\lambda\wtilde$
    	\State $\psi^{(t+1)} \gets \sqrt{\tr(\Wtilde)/T_3}$ 
%    	\Comment{Alternative calculation: $\tr(\Wtilde) \gets \sum_{i=1}^n \frac{s}{u_i^2 + s^2} + \wtilde^\top\wtilde$}
	\EndProcedure
	\Statex
	\State \textbf{call} \Call{Block A update }{Iteration $t+1$}
	\State \textbf{call} \Call{Log-likelihood update}{}
	\State $t \gets t+1$
\EndWhile
\Statex
%\State Obtain the maximum likelihood estimates as
\State $(\hat{\boldsymbol\lambda}, \hat{\psi}) \gets (\boldsymbol\lambda^{(t)}, \psi^{(t)})$ \Comment{The maximum likelihood estimates}
%\State $\hat{\boldsymbol\lambda} \gets \boldsymbol\lambda^{(t+1)}$
%\State $\hat{\psi} \gets \psi^{(t+1)}$
\end{algorithmic}
\end{algorithm}