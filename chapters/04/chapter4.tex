\documentclass[showframe,11pt,twoside,openright]{report}
\usepackage{standalone}
\standalonetrue
\ifstandalone
  \usepackage{../../haziq_thesis}
  \usepackage{../../haziq_maths}
  \usepackage{../../haziq_glossary}
  \usepackage{../../knitr}
  \addbibresource{../../bib/haziq.bib}
  \externaldocument{../01/.texpadtmp/chapter1}
  \externaldocument{../02/.texpadtmp/chapter2}  
  \externaldocument{../03/.texpadtmp/chapter3}
%  \externaldocument{../04/.texpadtmp/chapter4}
  \externaldocument{../05/.texpadtmp/chapter5}
  \externaldocument{../06/.texpadtmp/chapter6}
  \externaldocument{../07/.texpadtmp/chapter7}
  \externaldocument{../appendix/.texpadtmp/appendix}  
\fi

\begin{document}
\hChapterStandalone[4]{Regression with I-priors}
\label{chapter4}
\thispagestyle{chapterfour}

In the previous chapter, we defined an I-prior for the normal regression model \cref{eq:model1} subject to \cref{eq:model1ass} and $f$ belonging to a reproducing kernel Hilbert or Kreĭn space of functions $\cF$, as a Gaussian distribution on $f$ with covariance function proportional to the Fisher information for $f$.
We also saw how new function spaces can be constructed via the polynomial and ANOVA reproducing kernel Kreĭn spaces (RKKSs).
In this chapter, we shall describe various regression models, and identify the regression function in each of these models as belonging to an appropriate RKKS, so that an I-prior may be defined.

Methods for estimating I-prior models are described in \cref{sec:ipriorestimation}.
Estimation here refers to obtaining the posterior distribution of the regression function under an I-prior, while optimising the kernel parameters of $\cF$ and the error precision $\bPsi$.
Likelihood based methods, namely direct optimisation of the likelihood and the expectation-maximisation (EM) algorithm, are the preferred estimation methods of choice.
Having said this, it is also possible to estimate I-prior models under a full Bayesian paradigm by employing Markov chain Monte Carlo methods to sample from the relevant posterior densities.
Once estimation is completed, post-estimation procedures such as inference and prediction for a new data point can be done.
This is described in \cref{sec:ipriorpostest}.

Careful considerations of the computational aspects are required to ensure efficient estimation of I-prior models, and these are discussed in \cref{sec:ipriorcompcons}.
The culmination of the computational work on I-prior estimation is the \pkg{iprior} package \citep{jamil2017iprior}, which is a publicly available \proglang{R} package that has been published to the Comprehensive \proglang{R} Archive Network (CRAN).

Finally, several examples of I-prior modelling are presented in  \cref{sec:ipriorexamples}: in particular, a multilevel data set, a longitudinal data set, and a data set involving a functional covariate, are analysed using the I-prior methodology.
Code for replication is available at \url{http://myphdcode.haziqj.ml}.

\section{Various regression models}
\label{sec:various-regression}
\input{04a-various-regression.tex}

\section{Estimation}
\label{sec:ipriorestimation}
\input{04b-iprior-estimation}

\section{Computational considerations and implementation}
\label{sec:ipriorcompcons}
\input{04c-iprior-computational}

\section{Post-estimation}
\label{sec:ipriorpostest}
\input{04d-iprior-post-estimation}

\section{Examples}\label{sec:ipriorexamples}
\input{04x-examples}
\section{Conclusion}

The steps for I-prior modelling are essentially three-fold:
\begin{enumerate}
  \item Select an appropriate function space (equivalently, kernels) for which specific effects are desired on the covariates. 
%  Choices included a linear effect (canonical RKHS), a polynomial effect (polynomial RKKS), smoothing effect (fBm or SE RKHS), 
  \item Estimate the posterior regression function and optimise the hyperparameters, which include the RKHS scale parameter(s), error precision, and any other kernel parameters such as the Hurst index.
  \item Perform post-estimation procedures such as
  \begin{itemize}
    \item Posterior predictive checks;
    \item Model comparison via log-likelihood ratio tests/empirical Bayes factors; and
    \item Prediction of new data point.
  \end{itemize}
\end{enumerate}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.85\textwidth]{figure/04-iprior_runtime}
  \caption[Average time taken to complete the estimation of an I-prior model.]{Average time taken to complete the estimation of an I-prior model (EM algorithm and direct optimisation) of varying sample sizes. The solid line represents actual timings, while the dotted lines are linear extrapolations.}
  \label{fig:ipriortime}
\end{figure}

The main sticking point with the estimation procedure is the involvement of the $n\times n$ kernel matrix, for which an inverse is needed.
This requires $O(n^2)$ storage and $O(n^3)$ computational time.
%The Nyström method of approximating the kernel matrix reduces complexity to $O(nm)$ storage and approximately $O(nm^2)$, and is highly advantageous if $m \ll n$.
The computational issue faced by I-priors are mirrored in GPR, so the methods to overcome these computational challenges in GPR can be explored further.
However, most efficient computational solutions exploit the nature of the SE kernel structure, which is the most common kernel used in GPR.
Nonetheless, we suggest the following as considerations for future work:
\begin{enumerate}
  \item \textbf{Sparse variational approximations}. \index{variational inference} Variational methods have seen an active development in recent times. By using inducing points \citep{titsias2009variational} or stochastic variational inference \citep{hensman2013gaussian}, such methods can greatly reduce computational storage and speed requirements. A recent paper by \citet{cheng2017variational} also suggests a variational algorithm with linear complexity for GPR-type models.
  \item \textbf{Accelerating the EM algorithm}. \index{EM algorithm!accelerating} Two methods can be explored. The first is called parameter-expansion EM algorithm (PXEM) by \citet{liu1998parameter}, which has been shown to be promising for random-effects type models. It involves correcting the M-step by a ``covariance adjustment'', so that extra information can be capitalised on to improve convergence rates. The second is a quasi-Newton acceleration of the EM algorithm as proposed by \citet{lange1995quasi}. A slight change to the EM gradient algorithm in the M-step steers the EM algorithm to the Newton-Raphson algorithm, thus exploiting the benefits of the EM algorithm in the early stages (monotonic increase in likelihood) and avoiding the pitfalls of Newton-Raphson (getting stuck in local optima). Both algorithms require an in-depth reassessment of the EM algorithm to be tailored to I-prior models.
\end{enumerate}

\hClosingStuffStandalone
\end{document}