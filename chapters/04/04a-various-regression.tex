In the introductory chapter \colp{\cref{sec:introregmod}}, we described several interesting regression models.
The goal of this section is to formulate the I-prior model that describes each of these models.
This is done by carefully choosing the RKHS/RKKS $\cF$ of real functions over a set $\cX$ to which the regression function $f$ belongs.
Without loss of generality and for simplicity, assume a prior mean of zero for the I-prior distribution.

\subsection{Multiple linear regression}

Let $\cX\equiv \bbR^p$ be equipped with the regular Euclidean dot product, and $\cF_\lambda$ be the scaled canonical RKHS of functions over $\cX$ with kernel $h_\lambda(\bx,\bx') = \lambda \bx^\top\bx'$, for any two $\bx,\bx'\in\bbR^p$. 
Then, an I-prior on $f$ implies that 
\begin{align*}
  f(\bx_i) &= \sum_{j=1}^n \lambda \bx_i^\top\bx_j w_j \\
  &= \sum_{j=1}^n \lambda \left( \sum_{k=1}^p x_{ik}x_{jk} \right) w_j \\
  &= \beta_1 x_{i1} + \dots + \beta_p x_{ip},
\end{align*}
where each $\beta_k := \lambda \sum_{j=1}^n  x_{jk}w_j$.
This implies a multivariate normal prior distribution for the regression coefficients   
\begin{align}\label{eq:ipriorcanonical}
  \boldsymbol\beta := (\beta_1,\dots,\beta_p) \sim \N_p(\bzero, \lambda^2 \bX^\top \bPsi \bX),
\end{align}
where $\bX$ is the $n \times p$ design matrix for the covariates, excluding the column of ones at the beginning typically reserved for the intercept. 
As expected, the covariance matrix for $\boldsymbol\beta$ is recognised as the scaled Fisher information matrix for the regression coefficients.

If the covariates are not measured similarly, e.g. weights in kilograms, heights in metres, etc., then it makes sense to introduce scale parameters $\lambda_k$ to account for the difference in scale.
One could decompose the regression function into
\[
  f(\bx_i) = f_1(x_{i1}) + \cdots + f_p(x_{ip}) 
\]
for which $f \in \cF_\lambda \equiv \cF_{\lambda_1}\oplus\cdots\oplus\cF_{\lambda_p}$, and $\cF_{\lambda_k}$, $k=1,\dots,p$ are unidimensional canonical RKHSs with kernels $h_{\lambda_k}(x_{ik},x_{jk}) = \lambda_k x_{ik} x_{jk}$.
In effect, we now have $p$ scale parameters, one for each of the RKHSs associated with the $p$ covariates.
The RKKS $\cF_\lambda$ therefore has kernel
\[
  h(\bx_i,\bx_j) = \sum_{k=1}^p \lambda_k  x_{ik} x_{jk},
\]
and hence each regression coefficient can now be written as $\beta_k =  \sum_{j=1}^n  \lambda_k x_{jk}w_j$, for which we see the $\lambda_k$'s scaling role on the $x_{jk}$'s.
Thus, the corresponding I-prior for $\boldsymbol{\beta}$ is
\[
  \boldsymbol{\beta} \sim \N_p(\bzero, \bX^\top\bLambda \bPsi \bLambda \bX),
\]
with $\bLambda = \diag(\lambda_1,\dots,\lambda_p)$.
Note that $\cF_\lambda$ can be seen as a special case of the ANOVA RKKS, in which only the main effects are considered, in which case the \emph{centred canonical RKHSs} should be considered instead.
This approach is disadvantageous when $p$ is large, in which case there would be numerous scale parameters to estimate.

\begin{remark}
  Of course, one could simply turn to standardisation of the $\bX$ variables, so as to make the variables measure on the same scale.
  We feel this is a rather ad-hoc approach which creates meaningless units (they are standard deviations) for the covariates which are then fiddly to interpret.
  On the other hand, there is a balance to be made between elegance and feasibility.
  With large $p$, standardising is much simpler and computationally less burdensome than estimating $p$ individual scale parameters.
  In \cref{chapter6}, where we tackle the problem of  Bayesian variable selection using I-priors in linear models, standardisation of the variables is done for the sake of streamlining the Gibbs sampler.
\end{remark}


\begin{remark}
  The I-prior for $\boldsymbol{\beta}$ in \cref{eq:ipriorcanonical} bears resemblance to the $g$-prior \citep{zellner1986assessing}, and in fact, the $g$-prior can be interpreted as an I-prior if the inner product of $\cX$ is the Mahalonobis inner product.
  See \cref{misc:gprior} for a discussion.
\end{remark}



\subsection{Multilevel linear modelling}
\label{sec:multilevelmodels}

Let $\cX\equiv\bbR^p$, and suppose that alongside the covariates, there is information on group levels $\cM= \{1,\dots,m\}$ for each unit $i$.
That is, every observation for unit $i$ is known to belong to a specific group $j$, and we write $\bx_i^{(j)}$ to indicate this.
Let $n_j$ denote the sample size for cluster $j$, and the overall sample size be $n = \sum_{j=1}^m n_j$.
When modelled linearly with the responses $y_i^{(j)}$, the model is known as a multilevel (linear) model, although it is known by many other names: random-effects models, random coefficient models, hierarchical models, and so on.
As this model is seen as an extension of linear models, applications are plenty, especially in research designs for which the data varies at more than one level.

Consider a functional ANOVA decomposition of the regression function as follows:
\begin{align}\label{eq:anovamultilevel}
  f(\bx_i^{(j)}, j) = \alpha + f_1(\bx_i^{(j)}) + f_2(j) + f_{12}(\bx_i^{(j)}, j).  
\end{align}
To mimic the standard linear multilevel model, assume $f_1\in\cF_1$ the Pearson RKHS, $f_2\in\cF_2$ the centred canonical RKHS, and $f_{12} \in \cF_{12} = \cF_1 \otimes \cF_2$, the tensor product space of $\cF_1$ and $\cF_2$.
As we know, $\alpha$ is the overall intercept, and the varying intercepts are given by the function $f_2$.
While $f_1$ is the (main) linear effect of the covariates, $f_{12}$ provides the varying linear effect of the covariates by each group.
The I-prior for $f-\alpha$ is assumed to lie in the function space $\cF-\alpha$, which is an ANOVA RKKS with kernel
\[
  h_\lambda\big((\bx_i^{(j)},j),(\bx_i^{(j')},j')\big) = \lambda_1 h_1(\bx_i^{(j)},\bx_{i'}^{(j')}) + \lambda_2 h_2(j,j') + \lambda_1\lambda_2 h_1(\bx_i^{(j)},\bx_{i'}^{(j')})h_2(j,j'),
\]
with $h_1$ the centred canonical kernel and $h_2$ the Pearson kernel.
The reason for not including an RKHS of constant functions in $\cF$ is because the overall intercept is usually simpler to estimate as an external parameter (see \cref{sec:intercept}).

We can show that the regression function \cref{eq:anovamultilevel} corresponds to the standard way of writing the multilevel model, 
\begin{equation}\label{eq:standmultilevel}
  f(\bx_i^{(j)}, j) = \beta_0 + \bx_i^{(j)\top}\boldsymbol{\beta}_1 + \beta_{0j} + \bx_i^{(j)\top}\boldsymbol{\beta}_{1j}.   
\end{equation}
and determine the prior distributions on $(\beta_{0j},\boldsymbol{\beta}_{1j}^\top)^\top \in \bbR^{p+1}$.
For the interested reader, the details are in \cref{misc:multilevelmodels}.
The standard multilevel random effects assumption is that $(\beta_{0j},\boldsymbol{\beta}_{1j}^\top)^\top$ is normally distributed with mean zero and covariance matrix $\bPhi$.
In total, there are $p+1$ regression coefficients and $(p+1)(p+2)/2$ covariance parameters in $\Phi$ to be estimated.
In contrast, the I-prior model is parameterised by only two RKKS scale parameters---one for $\cF_1$ and one for $\cF_2$---and the error precision $\psi$.
While the estimation procedure for $\bPhi$ in the standard multilevel model can result in non-positive covariance matrices, the I-prior model has the advantage that positive definiteness is taken care of automatically\footnote{By virtue of the estimate of the regression function belonging to $\cF_n$, an RKHS with a positive definite kernel equal to the Fisher information for $f$.}.
%This is seen from the calculations for $\Var \beta_{0j}$, $\Var \boldsymbol\beta_{1j}$ and the respective covariances.
%An example of multilevel modelling using I-priors is given in \hltodo{Section 4.3.1}.

As a remark, the following regression functions are nested 
\begin{itemize}
  \item $f(\bx_i^{(j)}, j) = \alpha + f_1(\bx_i^{(j)}) + f_2(j)$ (random intercept model);  %\cF_\emptyset \oplus \lambda_1\cF_1 \oplus \lambda_2\cF_2
  \item $f(\bx_i^{(j)}, j) = \alpha + f_1(\bx_i^{(j)})$ (linear regression model);
  \item $f(\bx_i^{(j)}, j) = \alpha + f_2(j)$ (ANOVA model);
  \item $f(\bx_i^{(j)}, j) = \alpha$ (intercept only model),
\end{itemize}
and thus one may compare likelihoods to ascertain the best fitting model.
In addition, one may add flexibility to the model in two possible ways:
\begin{enumerate}
  \item \textbf{More than two levels}. The model can be easily adjusted to reflect the fact that that the data is structured in a hierarchy containing three or more levels. For the three level case, let the indices $j\in\{1,\dots,m_1\}$ and $k\in\{1,\dots,m_2\}$ denote the two levels, and simply decompose the regression function accordingly:
  \begin{align*}
    \begin{gathered}
      f(\bx_i^{(j,k)}, j, k) = \alpha + f_1(\bx_i^{(j,k)}) + f_2(j) + f_3(k) + f_{12}(\bx_i^{(j,k)}, j) + f_{13}(\bx_i^{(j,k)}, k)\\ 
      + f_{23}(j, k) + f_{123}(\bx_i^{(j,k)}, j, k).
    \end{gathered}
  \end{align*}
  \item \textbf{Covariates not varying with levels}. Suppose now we would like to add covariates with a fixed effect to the model, i.e. covariates $\bz_i^{(j)}$ which are not assumed to affect the responses differently in each group. The regression function would be:
  \begin{align*}
    \begin{gathered}
      f(\bx_i^{(j)}, j, \bz_j) = \alpha + f_1(\bx_i^{(j)}) + f_2(j) + f_3(\bz_i^{(j)}) + f_{12}(\bx_i^{(j)}, j).
    \end{gathered}
  \end{align*}
  This can be seen as a limited functional ANOVA decomposition of $f$.
\end{enumerate}

%\begin{remark}
%  \setlength{\parindent}{0em}
%  Indexing can be tricky, but we find the following helpful.
%  Supposing $m=2$, and $n_1 = n_2 = 3$, then a typical panel data set looks like this:
%  \begin{table}[H]
%  \centering
%  \begin{tabular}{lllrrr}
%  \hline
%  $y$ & $x$  & $z$ & $i$ & $j$ & $k$ \\ 
%  \hline
%  $y_{11}$      & $x_{11}$    & $z_{1}$ &1&1& 1 \\
%  $y_{21}$      & $x_{21}$    & $z_{1}$ &2&1& 2 \\
%  $y_{31}$      & $x_{31}$    & $z_{1}$ &3&1& 3 \\
%  $y_{12}$      & $x_{12}$    & $z_{2}$ &1&2& 4 \\
%  $y_{22}$      & $x_{22}$    & $z_{2}$ &2&2& 5 \\
%  $y_{32}$      & $x_{32}$    & $z_{2}$ &3&2& 6 \\  
%  \hline
%  \end{tabular}
%  \end{table}
%  \vspace{-1.5em}
%  The $y$'s are the responses, $x$'s covariates, and $z$'s group-level covariates.
%  If $\iota:(i,j)\mapsto k$ is a function which maps the dual index set $(i,j)$ to the single index set $k\in\{1,\dots,n\}$, then the multilevel regression function can be expressed as the regression function in model \cref{eq:model1}.
%\end{remark}

\subsection{Longitudinal modelling}

Longitudinal or panel data observes covariate measurements $x_i\in\cX$ and responses $y_i(t)\in\bbR$ for individuals $i=1,\dots,n$ across a time period $t \in \{1,\dots,T\} =: \cT$. 
Often, the time indexing set $\cT$ may be unique to each individual $i$, so measurements for unit $i$ happens across a time period $\{t_{i1},\dots,t_{iT_i} \} =: \cT_i$---this is known as an unbalanced panel.
It is also possible that covariate measurements vary across time too, so appropriately they are denoted $x_i(t)$.
For example, $x_i(t)$ could be repeated measurements of the variable $x_i$ at time point $t\in\cT_i$.
The relationship between the response variables $y_i(t)$ at time $t\in\cT_i$ is captured through the equation
\[
  y_i(t) = f\big(i, x_i, t \big) + \epsilon_{i}(t)
\]
where the distribution of $\bepsilon_i = \big(\epsilon_i(t_{i1}),\dots,\epsilon_i(t_{iT_i}) \big)^\top$ is Gaussian with mean zero and covariance matrix $\bPsi_i$.
Assuming $\bPsi_i=\psi_i\bI_{T_i}$ or even $\bPsi_i=\psi\bI_{T_i}$ are perfectly valid choices, even though this seemingly ignores any time dependence between the observations.
In reality, the I-prior induces time dependence of the observations via the kernels in the prior covariance matrix for $f$.
Additionally, the random vectors $\bepsilon_i$ and $\bepsilon_{i'}$ are assumed to be independent for any two distinct $i,i'\in\{1,\dots,n\}$.
%, in which case, $y_i(t)$ are viewed as multidimensional responses.

Motivated by a functional ANOVA decomposition, we obtain
\begin{align}\label{eq:longitudinalanova}
\begin{split}
  f(i,x_i,t) 
  ={}& \alpha + f_1(i) + f_2(x_i) + f_3(t) + f_{13}(i,t) + f_{23}(x_i,t) + f_{12}(i,x_i)  \\
  &+ f_{123}(i,x_i,t)
\end{split}
\end{align}
where $\alpha$ is an overall constant, and each of the ANOVA component functions belongs to the appropriate (tensor product) space as described in \cref{sec:anovarkks} \colp{\mypageref{{sec:anovarkks}}}.
$\cF_1$ is the Pearson RKHS, but choices for $\cF_2$ and $\cF_3$ are plentiful.
In fact, any of the RKHS/RKKS described in Chapter 3 can be used to either model a linear dependence (canonical RKHS), nominal dependence (Pearson RKHS), polynomial dependence (polynomial RKKS) or smooth dependence (fBm or SE RKHS) on the $x_i$'s and $t$'s on $f$.

\subsection{Classification}
\label{sec:naiveclass}
\input{05a-naive}

\subsection{Smoothing models}

Single- and multi-variable smoothing models can be fitted under the I-prior methodology using the fBm RKHS.
In standard kernel based smoothing methods, the squared exponential kernel is often used, and the corresponding RKHS contains analytic functions.
There are several attractive properties of using the fBm RKHS, and for one-dimensional smoothing, these are discussed below.

Assume that, up to a constant, the regression function lies in the scaled, centred fBm RKHS $\cF$ of functions over $\cX \equiv \bbR$ with Hurst index $1/2$.
Thus, with a centring with respect to the empirical distribution $\Prob_n$ of $\{x_1,\dots,x_n\}$ and using the absolute norm on $\bbR$, $\cF$ has kernel
\[
  h_\lambda(x,x') = \frac{\lambda}{2n^2} \sum_{i=1}^n\sum_{j=1}^n \big( \abs{x-x_i} + \abs{x'-x_j} - \abs{x-x'} - \abs{x_i-x_j} \big).
\]
As proven by \citet[Section 10]{van2008reproducing}, $\cF$ contains absolutely continuous functions possessing a square integrable weak derivative satisfying $f(0)=0$.
The norm is given by $\norm{f}^2_\cF = \int \dot f^2 \dint x$.
The posterior mean of $f$ based on an I-prior is then a (one-dimensional) smoother for the data.
For $f$ of the form $f = \sum_{i=1}^n h(\cdot,x_i)w_i$, i.e. $f\in\cF_n$, the finite subspace of $\cF$ as in \cref{sec:inducedFisherRKHS} \colp{\mypageref{sec:inducedFisherRKHS}}, then \citet{bergsma2017} shows that $f$ can be represented as 
\vspace{-0.5em}
\begin{align}\label{eq:ipriorbrownianbridge}
  f(x) = \int_{-\infty}^x \beta(t) \dint t \vspace{-0.5em}
\end{align}
where 
\vspace{-0.5em}
\begin{align}
  \beta(t) = \sum_{i:x_i \leq t} w_i =  \frac{f(x_{i_t + 1}) - f(x_{i_t})}{x_{i_t + 1} - x_{i_t}} \vspace{-0.1em}
\end{align}
with $i_t = \max_{x_i \leq t} i$.
Under the I-prior with an iid assumption on the errors, the $w_i$'s are zero mean normal random variables with variance $\psi$, so that $\beta$ as defined above is an ordinary Brownian bridge with respect to the empirical distribution $\Prob_n$.
The I-prior for $f$ is piecewise linear with knots at $x_1,\dots,x_n$, and the same holds true for the posterior mean.
The implication is that the I-prior automatically adapts to irregularly spaced $x_i$: in any region where there are no observations, the resulting smoother is linear.
This is explained by the reduced Fisher information about the derivative of the regression curve in regions with no observation.

In \citet{bergsma2017}, it is shown that the covariance function for $\beta$ is 
\[
  \Cov\big(\beta(x),\beta(x') \big) = n \big( \min\{\Prob_n(X < x), \Prob_n(X_n < x') \} -  \Prob_n(X < x) \Prob_n(X_n < x') \big) 
\]
From this, notice that $\Var \big(\beta(x)\big) = \Prob_n(X_n < x)\big(1 - \Prob_n(X_n < x) \big)$, which shows an automatic boundary correction: close to the boundary there is little Fisher information on the derivative of the regression function $\beta(x)$, so the prior variance is small.
This will lead to more shrinkage of the posterior derivative of $f$ towards the derivative of the prior mean $f_0$.

%\[
%  \norm{f}^2_\cF = \sum_{i=1}^{n-1} \frac{\big( f(x_{i+1}) - f(x_i) \big)^2}{x_{i+1} - x_i}
%\]
%because $\cF_n$ is the set of functions which integrate to zero and are piecewise linear with knots at $x_1,\dots,x_n$.
%Assuming $x_1\leq x_2\leq \cdots \leq x_n$, then it can be shown that
%\[
%  w_1 = \frac{f(x_2) - f(x_1)}{x_2 - x_1}
%\]

%With $w_k\iid\N(0,\psi)$, functions in $\cF$ are of the form
%\begin{align*}
%  f(x) 
%  &= \sum_{k=1}^n h(x,x_k)w_k \\
%  &= \sum_{k=1}^n \left( \frac{\lambda}{2n^2} \sum_{i=1}^n\sum_{j=1}^n \big( \abs{x-x_i} + \abs{x_k-x_j} - \abs{x-x_k} - \abs{x_i-x_j} \big) \right) w_k.
%\end{align*}
%The derivative of $f(x)$ with respect to $x$ is
%\begin{align*}
%  \frac{\d }{\d x}f(x) 
%  &= \frac{\lambda}{2n^2} \sum_{k=1}^n  \sum_{i=1}^n\sum_{j=1}^n \left( \frac{\d}{\d x}\abs{x-x_i}  - \frac{\d}{\d x}\abs{x-x_k}  \right) w_k \\
%  &= \frac{\lambda}{2n^2} \sum_{k=1}^n  \sum_{i=1}^n\sum_{j=1}^n \left( \sign(x-x_i)  - \sign(x-x_k)  \right) w_k \\
%  &= \frac{\lambda}{2n} \sum_{k=1}^n  \sum_{i=1}^n           \left( \sign(x-x_i)  - \sign(x-x_k)  \right) w_k \\
%\end{align*}

Another advantage of the I-prior methodology is the ability to fit single or multidimensional smoothing models with just two parameters to be estimated: the RKHS scale parameter $\lambda$ and the error precision $\bPsi$.
The Hurst parameter $\gamma \in (0,1)$ of the fBm RKHS can also be treated as a free parameter for added flexibility, but for most practical applications, we find that the default setting of $\gamma = 1/2$ performs sufficiently well.

\begin{remark}
  From \cref{eq:ipriorbrownianbridge}, the prior process for $f$ is thus an integrated Brownian bridge. 
  This shows a close relation with cubic spline smoothers, which can be interpreted as the posterior mean when the prior is an integrated Wiener process \citep{wahba1990spline}.
  Unlike I-priors however, cubic spline smoothers do not have automatic boundary corrections, and typically the additional assumption is made that the smoothing curve is linear at the boundary knots.
\end{remark}

\subsection{Regression with functional covariates}
\label{sec:regfunctionalcov}

Suppose that we have functional covariates $x$ in the real domain, and that $\cX$ is a set of differentiable functions.
If so, it is reasonable to assume that $\cX$ is a Hilbert-Sobolev space with inner product
%
\[
  \langle x,x' \rangle_\cX = \int \dot{x}(t) \dot{x}'(t) \dint t,
\]
%
so that we may apply the linear, fBm or any other kernels which make use of inner products by making use of the polarisation identity.
Furthermore, let $z \in \bbR^T$ be the discretised realisation of the function $x \in \cX$ at regular intervals $t = 1,\dots,T$. Then
%
\[
  \langle x,x' \rangle_\cX \approx \sum_{t=1}^{T-1} (z_{t+1} - z_t)(z'_{t+1} - z_t').
\]
%
For discretised observations at non-regular intervals $\{t_1,\dots,t_T\}$ then a more general formula to the above one might be used, for instance,
\[
  \langle x,x' \rangle_\cX \approx \sum_{i=1}^{T-1} \frac{(z_{t_{i+1}} - z_{t_i})(z'_{t_{i+1}} - z_{t_i}')}{t_{i+1} - t_{i}}.
\]

%\subsection{Structural equation modelling}
%
%Let $y_i = (y_{i1},\dots,y_{ip})$, $i=1,\dots,n$ and $j=1,\dots,p$.
%Here, $y_{ij}$ denotes the $j$th item measurement for the $i$th individual.
%In a one factor model, this measurement is assumed to rely on an `item intercept', and individual level `factors', plus an error term which depends on the items.
%Specifically, the one factor model is
%\begin{align}
%  y_{ij} = \mu(j) + f(i,j) + \epsilon_{ij}  
%\end{align}
%with $\epsilon_{ij}\sim\N(0,\psi_j)$.





