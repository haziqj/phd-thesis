Without loss of generality, we assume a prior mean of zero for the I-prior distribution.

\subsection{Multiple linear regression}

Let $\cX\equiv \bbR^p$ equipped with the regular dot product, and $\cF_\lambda$ the scaled canonical RKHS of functions over $\cX$ with kernel $h_\lambda(\bx,\bx') = \lambda \bx^\top\bx'$. 
Then, an I-prior on $f$ implies that 
\begin{align*}
  f(\bx_i) &= \sum_{j=1}^n \lambda \bx_i^\top\bx_j w_j \\
  &= \sum_{j=1}^n \lambda \sum_{k=1}^p x_{ik}x_{jk} w_j \\
  &= \beta_1 x_{i1}+ \dots + \beta_p x_{ip},
\end{align*}
where each $\beta_k = \lambda \sum_{j=1}^n  x_{jk}w_j$.
This implies a multivariate normal prior distribution for the regression coefficients   
\begin{align*}
  \boldsymbol\beta := (\beta_1,\dots,\beta_p) \sim \N_p(\bzero, \lambda^2 \bX^\top \bPsi \bX),
\end{align*}
where $\bX$ is the $n \times p$ design matrix for the covariates, excluding the column of ones at the beginning typically reserved for the intercept. 
The covariance matrix for $\boldsymbol\beta$ is recognised as the scaled Fisher information matrix for the regression coefficients.

The I-prior for $\boldsymbol{\beta}$ resembles the objective $g$-prior \citep{zellner1986assessing} for regression coefficients,
\[
  \boldsymbol{\beta} \sim \N_p\big(\bzero, g (\bX^\top \bPsi \bX)^{-1} \big),
\]
although they are quite different objects. 
The $g$-prior for $\boldsymbol{\beta}$ has the \emph{inverse} (scaled) Fisher information matrix as its covariance matrix.
This, in itself, has a much different and arguably counterintuitive meaning: large amounts of Fisher information about $\boldsymbol{\beta}$ corresponds to a small prior variance, and hence less deviation away from the prior mean of zero in estimating $\boldsymbol\beta$.
The choice of the hyperparameter $g$ has been the subject of much debate, with choices ranging from fixing $g=n$ (corresponding to the concept of \emph{unit Fisher information}), to fully Bayesian and empirical Bayesian methods of estimating $g$ from the data.

On the other hand, we note that the $g$-prior has an I-prior interpretation when argues as follows.
Assume that the regression function $f$ lies in the continual dual space of $\bbR^p$ equipped with the inner product $\ip{\bx,\bx'}_\cX = \bx^\top(\bX^\top \bPsi \bX)^{-1}\bx$.
With this inner product and from \hltodo{(3.3)}, the Fisher information for $\boldsymbol{\beta}$ is
\begin{align*}
  \cI_g(\boldsymbol\beta) 
  &= \sum_{i=1}^n \sum_{j=1}^n \psi_{ij} (\bX^\top \bPsi \bX)^{-1}\bx_i \otimes (\bX^\top \bPsi \bX)^{-1}\bx_j \\
  &= (\bX^\top \bPsi \bX)^{-1} \cancel{(\bX^\top \bPsi \bX)} \cancel{(\bX^\top \bPsi \bX)^{-1}} \\
  &= (\bX^\top \bPsi \bX)^{-1},
\end{align*}
and this rather than the usual $\bX^\top \bPsi \bX$ as the prior covariance matrix for $\boldsymbol{\beta}$ means that the I-prior is in fact the standard $g$-prior.

The metric induced by the inner product is actually the \emph{Mahalanobis distance}, a scale-invariant natural distance if the covariates are measured on different scales.
To expand on this idea, circle back to the regression function and write it as $f(\bx) = \ip{\bx,\boldsymbol\beta}_\cX$.
In usual least squares regression, the choice of inner product is irrelevant, so the usual dot product is commonly used (however, as we have seen above, the choice of inner product determines the form of the Fisher information for $\boldsymbol{\beta}$).
In particular, suppose that all the $x_{ik}$'s, $k=1,\dots,p$ for each unit $i=1,\dots,n$ are measured on the same scale; for instance, these could be measurements in centimetres.
In this case, the dot product is reasonable, because $\ip{\bx_i,\bx_j} = \sum_{k=1}^p x_{ik}x_{jk}$ and the inner product has a coherent unit, namely the squared unit of the $x_{ik}$'s.
However, if they were a mix of various scaled measurements, then obviously the inner product's unit is incoherent---one would be resorted to adding measurements in different units, for example, $\text{cm}^2$ and $\text{kg}^2$ and so on.
In such a case, a unitless inner product is appropriate, like the Mahalonobis inner product, which technically rescales the $x_{ik}$'s to unity.
In summary, if the covariates are all measured on the same scale, then the I-prior is appropriate, and if not, the $g$-prior is appropriate.
\hltodo[Can I just standardise $x$?]{}

A different approach for covariate measurements of differing scales, without resorting to $g$-priors, is the ANOVA approach.
By considering only the main effects, one decomposes the regression function into
\[
  f(\bx_i) = \beta_0 + f_1(x_{i1}) + \cdots + f_p(x_{ip}) 
\]
for which $(f-\beta_0) \in \cF_\lambda \equiv \cF_{\lambda_1}\oplus\cdots\oplus\cF_{\lambda_p}$, and $\cF_{\lambda_k}$,$k=1,\dots,p$ are unidimensional centred canonical RKKSs with kernels $h_{\lambda_k}(x_{ik},x_{jk}) = \lambda_k (x_{ik}-\bar x_k) (x_{jk}-\bar x_k)$, where $\bar x_k = \sum_{i=1}^n x_{ik}/n$.
In effect, we now have $p$ scale parameters, one for each of the RKKSs associated with the $p$ covariates.
Denote $\tilde x_{ik} = x_{ik}-\bar x_k$, the centred covariates.
The RKKS $\cF_\lambda$ therefore has the kernel
\[
  h(\bx_i,\bx_j) = \sum_{k=1}^p \lambda_k \tilde x_{ik}\tilde x_{jk},
\]
and hence each regression coefficient can now be written as $\beta_k =  \sum_{j=1}^n  \lambda_k \tilde x_{jk}w_j$.
Thus, the corresponding I-prior for $\boldsymbol{\beta}$ is
\[
  \boldsymbol{\beta} \sim \N_p(\bzero, \lambda^2 \tilde\bX^\top\bLambda \bPsi \bLambda \tilde\bX),
\]
with $\bLambda = \diag(\lambda_1,\dots,\lambda_p)$.
Note that the overall effect $\beta_0$ can be treated in a number of ways, the simplest of which is as an intercept to be estimated.
This approach is disadvantageous when $p$ is large, in which case there would be numerous scale parameters to estimate.


\subsection{Multilevel linear modelling}
\label{sec:multilevelmodels}

Let $\cX\equiv\bbR^p$, and suppose that alongside the covariates, there is information on group levels $\cM= \{1,\dots,m\}$ for each unit $i$.
That is, every observation for unit $i$ is known to belong to a specific group $j$, and we write $\bx_i^{(j)}$ to indicate this.
Let $n_j$ denote the sample size for cluster $j$, and the overall sample size be $n = \sum_{j=1}^m n_j$.
When modelled linearly with the responses $y_i^{(j)}$, the model is known as a multilevel (linear) model, although it is known by many other names: random-effects models, random coefficient models, hierarchical models, and so on.
As this model is seen as an extension of linear models, applications are plenty, especially in research designs for which the data varies at more than one level.

Consider a functional ANOVA decomposition of the regression function as follows:
\begin{align}\label{eq:anovamultilevel}
  f(\bx_i^{(j)}, j) = f_0 + f_1(\bx_i^{(j)}) + f_2(j) + f_{12}(\bx_i^{(j)}, j).  
\end{align}
To mimic the multilevel model, assume $f_1\in\cF_1$ the Pearson RKHS, $f_2\in\cF_2$ the centred canonical RKHS, and $f_{12} \in \cF_{12} = \cF_1 \otimes \cF_2$, the tensor product space of $\cF_1$ and $\cF_2$.
As we know, $f_0$ is the overall intercept, and the varying intercepts are given by the function $f_2$.
$f_1$ is the (main) linear effect of the covariates, while $f_{12}$ provides the varying linear effect per group of the covariates.
The I-prior for $f-f_0$ is assumed to lie in the function space $\cF-f_0$, which is an ANOVA RKKS with kernel
\[
  h_\lambda\big((\bx_i^{(j)},j),(\bx_i^{(j')},j')\big) = \lambda_1 h_1(\bx_i^{(j)},\bx_{i'}^{(j')}) + \lambda_2 h_2(j,j') + \lambda_1\lambda_2 h_1(\bx_i^{(j)},\bx_{i'}^{(j')})h_2(j,j'),
\]
with $h_1$ the centred canonical kernel and $h_2$ the Pearson kernel.

We can show that the regression function \eqref{eq:anovamultilevel} corresponds to the standard way of writing the multilevel model, 
\[
  f(\bx_i^{(j)}, j) = \beta_0 + \bx_i^{(j)\top}\boldsymbol{\beta}_1 + \beta_{0j} + \bx_i^{(j)\top}\boldsymbol{\beta}_{1j}.   
\]
and determine the prior distributions on $(\beta_{0j},\boldsymbol{\beta}_{1j})^\top$.
Write $f_0=\beta_0$, and for simplicity, assume iid errors, i.e.,  $\bPsi = \psi\bI_n$.
The form of $f\in\cF$ is now $f(\bx_i^{(j)},j) = \sum_{i'=1}^{n_{j'}}\sum_{j'=1}^m h_\lambda\big((\bx_i^{(j)},j),(\bx_{i'}^{(j')},j')\big) w_{i'j'}$, where each $w_{i'j'}\sim\N(0,\psi^{-1})$.
We have seen from the previous section that $f_1(\bx_i^{(j)}) = \tilde\bx_i^{(j)\top}\boldsymbol{\beta}$, with $\boldsymbol{\beta} = \lambda_1\tilde\bX^\top\bw \sim \N_p(\bzero, \lambda_1^2\psi \tilde\bX^\top\tilde\bX )$.
Here, $\tilde\bX$ is the $(n_1+\cdots+n_m) \times p$ matrix containing centred entries of $\bx_i^{(j)}$.
Now, functions in the scaled RKHS $\cF_2$ have the form
\begin{align*}
  f_2(j) 
  &= \sum_{i=1}^{n_{j'}}\sum_{j'=1}^m \lambda_2\left( \frac{\delta_{jj'}}{p_j} - 1 \right)w_{ij'} \\
  &=  \lambda_2\left( \frac{w_{+j}}{p_j} - w_{++} \right),
\end{align*}
where a `$+$' in the index of $w_{ik}$ indicates a summation over that index, and $p_j$ is the empirical distribution over $\cM$, i.e. $p_j = n_j/n$.
Clearly $f_2(j)$ is a variable depending on $j$, so write $f_2(j)=\beta_{0j}$.
The distribution of $\beta_{0j}$ is normal with zero mean and variance
\begin{align*}
  \Var \beta_{0j} 
  &= \lambda_2^2 \left( \frac{\cancel{n_j}\psi}{n_j^{\cancel{2}} / n^2} + n\psi \right)  \\
  &= n\psi\lambda_2^2 \left( \frac{1}{p_j} + 1 \right).
\end{align*}
The covariance between any two random intercepts $\beta_{0j}$ and $\beta_{0j'}$ is
\begin{align*}
  \Cov(\beta_{0j},\beta_{0j'})
  &= \Cov\left( \lambda_2\left( \frac{w_{+j}}{p_j} - w_{++} \right), \lambda_2\left( \frac{w_{+j'}}{p_{j'}} - w_{++} \right) \right)  \\
  &= \frac{\lambda_2^2}{p_j p_{j'}} \cancelto{0}{\Cov(w_{+j},w_{+j'})} - \frac{\lambda_2^2}{p_j} \Cov(w_{+j},w_{++}) - \frac{\lambda_2^2}{p_{j'}} \Cov(w_{++},w_{+j'}) \\
  &\phantom{==} + \lambda_2^2 \Cov(w_{++},w_{++}) \\
  &= - \frac{\lambda_2^2}{\cancel{n_j}/n} \cancel{n_j}\psi - \frac{\lambda_2^2}{\cancel{n_{j'}}/n} \cancel{n_{j'}}\psi + \lambda_2^2 n\psi \\
  &= -n\psi\lambda_2^2.
\end{align*}

Functions in $\cF_{12}$, on the other hand, have the form
\begin{align*}
  f_{12}(\bx_i, j)
  &= \sum_{i'=1}^{n_{j'}}\sum_{j'=1}^m \lambda_1\lambda_2 \cdot \tilde \bx_i^{(j)\top} \tilde \bx_{i'}^{(j')} \cdot \left( \frac{\delta_{jj'}}{p_j} - 1 \right)  w_{i'j'} \\
  &=  \tilde \bx_i^{(j)\top}   
  {\color{gray}
  \underbrace{\color{black}
  \left( \frac{\lambda_1\lambda_2}{p_j} \sum_{i'=1}^{n_{j}}  \tilde \bx_{i'}^{(j)} w_{i'j} - \lambda_1\lambda_2\sum_{i'=1}^{n_{j'}}\sum_{j'=1}^m  \tilde \bx_{i'}^{(j')}  w_{i'j'} \right)
  }_{\boldsymbol\beta_{1j}}},
\end{align*}
and this is, as expected, a linear form dependent on cluster $j$.
We can calculate the variance for $\beta_{1j}$ to be
\begin{align*}
  \Var \boldsymbol{\beta}_{1j}
  &= \lambda_1^2\lambda_2^2 \Var\left( \frac{1}{p_j} \tilde\bX_j^\top \bw_j - \tilde\bX^\top \bw \right) \\
  &= \lambda_1^2\lambda_2^2 \left( \frac{\psi}{n_j^2/n^2} \tilde\bX_j^\top\tilde\bX_j + \psi \tilde\bX^\top  \tilde\bX - \frac{1}{p_j} \tilde\bX_j^\top \Cov( \bw_j,\bw) \tilde\bX^\top  \right) \\
  &= n\psi\lambda_1^2\lambda_2^2 \left( \frac{1}{p_j}\bS_j +  \bS - \bS_{j} \right) \\
  &= n\psi\lambda_1^2\lambda_2^2 \left( \left(\frac{1}{p_j}-1\right)\bS_j +  \bS  \right)
\end{align*}
where $\bS_j = \frac{1}{n_j} \sum_{i=1}^{n_j} (\bx_i^{(j)} - \bar \bx)^\top(\bx_i^{(j)} - \bar \bx)$, $\bS = \frac{1}{n} \sum_{i=1}^{n_j} \sum_{j=1}^{m} (\bx_i^{(j)} -  \bar \bx)^\top(\bx_i^{(j)} - \bar \bx)$, and $\bar \bx = \frac{1}{n} \sum_{i=1}^{n_j} \sum_{j=1}^{m} \bx_i^{(j)}$.
The covariance between two vectors of the random slopes is
\begin{align*}
  \Cov(\boldsymbol{\beta}_{1j},\boldsymbol{\beta}_{1j'}) 
  &= \lambda_1^2\lambda_2^2  \Cov \left( \frac{1}{p_j} \tilde\bX_j^\top \bw_j - \tilde\bX^\top \bw, \frac{1}{p_{j'}} \tilde\bX_{j'}^\top \bw_{j'} - \tilde\bX^\top \bw  \right) \\
  &= \psi\lambda_1^2\lambda_2^2 \left( \tilde\bX^\top\tilde\bX - \frac{1}{p_j}\tilde\bX_j^\top\tilde\bX_j  - \frac{1}{p_{j'}}\tilde\bX_{j'}^\top\tilde\bX_{j'} \right) \\
  &= n\psi\lambda_1^2\lambda_2^2 \left( \bS - \bS_j - \bS_{j'}  \right).
\end{align*}

Another quantity of interest is the covariance between the random intercepts and random slopes:
\begin{align*}
  \Cov(\beta_{0j}, \boldsymbol{\beta}_{1j}) 
  &= \lambda_1\lambda_2^2  \Cov\left( \frac{1}{p_{j}} \bone_{n_j}^\top \bw_{j} - \bone_n^\top \bw, \frac{1}{p_{j}} \tilde\bX_{j}^\top \bw_{j} - \tilde\bX^\top \bw   \right) \\
  &= \psi\lambda_1\lambda_2^2  \left( \cancelto{0}{\bone_{n}^\top \tilde\bX} + \frac{1}{p_{j}^2} \bone_{n_j}^\top \tilde\bX_j  - \frac{2}{p_{j}} \bone_{n_{j}}^\top \tilde\bX_{j} \right) \\
  &= n\psi\lambda_1\lambda_2^2  \left(  \left(\frac{1}{p_j} - 2 \right) \frac{1}{n_{j}} \sum_{i=1}^{n_j}(\bx_i^{(j)} - \bar \bx)  \right) \\
  &= n\psi\lambda_1\lambda_2^2 \left(\frac{1}{p_j} - 2 \right) ( \bar\bx^{(j)}   -\bar \bx  ) 
\end{align*}
and
\begin{align*}
  \Cov(\beta_{0j}, \boldsymbol{\beta}_{1j'}) 
  &= \lambda_1\lambda_2^2  \Cov\left( \frac{1}{p_{j}} \bone_{n_j}^\top \bw_{j} - \bone_n^\top \bw, \frac{1}{p_{j'}} \tilde\bX_{j'}^\top \bw_{j'} - \tilde\bX^\top \bw   \right) \\
  &= \psi\lambda_1\lambda_2^2  \left( \cancelto{0}{\bone_{n}^\top \tilde\bX} + \frac{1}{p_{j}p_{j'}} \bone_{n_j}^\top \cancelto{0}{\Cov(\bw_j,\bw_{j'})} \tilde\bX_{j'} - \frac{1}{p_{j}} \bone_{n_j}^\top \tilde\bX_j  - \frac{1}{p_{j'}} \bone_{n_{j'}}^\top \tilde\bX_{j'} \right) \\
  &= n\psi\lambda_1\lambda_2^2  \left(  - \frac{1}{n_{j}} \sum_{i=1}^{n_j}(\bx_i^{(j)} - \bar \bx)  - \frac{1}{n_{j'}} \sum_{i=1}^{n_{j'}}(\bx_i^{(j')} - \bar \bx) \right) \\
  &= n\psi\lambda_1\lambda_2^2  \left(  2\bar \bx -  \bar\bx^{(j)}  -  \bar \bx^{(j')}  \right).
\end{align*}

The standard multilevel random effects assumption is that $(\beta_{0j},\boldsymbol{\beta}_{1j})^\top$ is normally distributed with mean zero and covariance matrix $\bPhi$ of dimensions $(p+1)\times(p+1)$.
In total, there are $p+1$ regression coefficients and $(p+1)(p+2)/2$ covariance parameters in $\Phi$ to be estimated.
In contrast, the I-prior model is parameterised by only two RKKS scale parameters and the error precision $\psi$.
While the estimation procedure for $\bPhi$ in the standard multilevel model can result in non-positive covariance matrices, the I-prior model has the advantage that positive definiteness is taken care of automatically.
This is seen from the calculations for $\Var \beta_{0j}$, $\Var \boldsymbol\beta_{1j}$ and the respective covariances.
An example of multilevel modelling using I-priors is given in \hltodo{Section 4.3.1}.

As a remark, the following regression functions are nested 
\begin{itemize}
  \item $f(\bx_i^{(j)}, j) = f_0 + f_1(\bx_i^{(j)}) + f_2(j)$ (random intercept model);  %\cF_0 \oplus \lambda_1\cF_1 \oplus \lambda_2\cF_2
  \item $f(\bx_i^{(j)}, j) = f_0 + f_1(\bx_i^{(j)})$ (linear regression model);
  \item $f(\bx_i^{(j)}, j) = f_0 + f_2(j)$ (ANOVA model);
  \item $f(\bx_i^{(j)}, j) = f_0 $ (intercept only model),
\end{itemize}
and thus one may compare likelihoods to ascertain the best fitting model.
In addition, one may add flexibility to the model in two possible ways:
\begin{enumerate}
  \item \textbf{More than two levels}. The model can be easily adjusted to reflect the fact that that the data is structured in a hierarchy containing three or more levels. For the three level case, let the indices $j\in\{1,\dots,m_1\}$ and $k\in\{1,\dots,m_2\}$ denote the two levels, and simply decompose the regression function accordingly.
  \begin{align*}
    \begin{gathered}
      f(\bx_i^{(j,k)}, j, k) = f_0 + f_1(\bx_i^{(j,k)}) + f_2(j) + f_3(k) + f_{12}(\bx_i^{(j,k)}, j) + f_{13}(\bx_i^{(j,k)}, k)\\ 
      + f_{23}(j, k) + f_{123}(\bx_i^{(j,k)}, j, k).
    \end{gathered}
  \end{align*}
  \item \textbf{Group-level covariates}. Suppose now we would like to add group-level covariates to the model, i.e., covariates $\bz_j$ that only vary across groups. The regression function would then be
  \begin{align*}
    \begin{gathered}
      f(\bx_i^{(j)}, j, \bz_j) = f_0 + f_1(\bx_i^{(j)}) + f_2(j) + f_3(\bz_j) + f_{12}(\bx_i^{(j)}, j) + f_{13}(\bx_i^{(j)}, \bz_j)\\ 
      + f_{23}(j, \bz_j) + f_{123}(\bx_i^{(j)}, j, \bz_j).
    \end{gathered}
  \end{align*}
  Not all of these terms need to be included.
  For example, excluding $f_{23}$ would imply that the regression coefficient for $\bz_j$ does not vary across groups.
\end{enumerate}

\subsection{Longitudinal modelling}

Longitudinal or panel data observes covariate measurements $x_i\in\cX$ and responses $y_i(t)\in\bbR$ for individuals $i=1,\dots,n$ across a time period $t \in \{1,\dots,T\} =: \cT$. 
Often, the time indexing set $\cT$ may be unique to each individual $i$, so measurements for unit $i$ happens across a time period $\{t_{i1},\dots,t_{iT_i} \} =: \cT_i$---this is known as an unbalanced panel.
It is also possible that covariate measurements vary across time too, so appropriately they are denoted $x_i(t)$.
For example, $x_i(t)$ could be repeated measurements of the variable $x_i$ at time point $t\in\cT_i$.
The relationship between the response variables $y_i(t)$ at time $t\in\cT_i$ is captured through the equation
\[
  y_i(t) = f\big(x_i, t \big) + \epsilon_{i}(t)
\]
where the distribution of $\bepsilon_i = \big(\epsilon_i(t_{i1}),\dots,\epsilon_i(t_{iT_i}) \big)^\top$ is Gaussian with mean zero and covariance matrix $\bPsi_i$.
Assuming $\bPsi_i=\psi_i\bI_{T_i}$ or even $\bPsi_i=\psi\bI_{T_i}$ are perfectly valid choices, even though this seemingly ignores any time dependence between the observations.
In reality, the I-prior induces time dependence of the observations via the kernels in the prior covariance matrix for $f$.
Additionally, the random vectors $\bepsilon_i$ and $\bepsilon_{i'}$ are assumed to be independent for any two distinct $i,i'\in\{1,\dots,n\}$.
%, in which case, $y_i(t)$ are viewed as multidimensional responses.

Using the functional ANOVA decomposition on the regression function, we obtain
\begin{align}\label{eq:longitudinalanova}
  f(x_i,t) = f_0 + f_1(x_i) + f_2(t) + f_{12}(x_i,t),
\end{align}
where $f_0$ is an overall constant, $f_1\in\cF_1$, $f_2\in\cF_2$, and $f_{12}\in\cF_1\otimes\cF_2$.
Choices for $\cF_1$ and $\cF_2$ are plentiful.
In fact, any of the RKHS/RKKS described in Chapter 3 can be used to either model a linear dependence (canonical RKHS), nominal dependence (Pearson RKHS), polynomial dependence (polynomial RKKS) or smooth dependence (fBm or SE RKHS) on the $x_i$'s and $t$'s on $f$.

\begin{remark}
  Although \eqref{eq:longitudinalanova} is a special case of the multilevel model decomposition \eqref{eq:anovamultilevel} for which $x_i = x_i(t)$ (time-varying covariates), it is different to how longitudinal models are normally treated in using random coefficients.
  As a multilevel model, longitudinal models treat the individuals as the groups or clusters (level two), and the time points as the various measurements within the clusters (level one).
\end{remark}

\subsection{Smoothing models}

Assume that, up to a constant, the regression function lies in the scaled, centred fBm RKHS $\cF$ of functions over $\cX \equiv \bbR$ with Hurst index $1/2$.
Thus, with a centring with respect to the empirical distribution of $\{x_1,\dots,x_n\}$ and using the absolute norm on $\bbR$, $\cF$ has kernel
\[
  h_\lambda(x,x') = \frac{\lambda}{2n^2} \sum_{i=1}^n\sum_{j=1}^n \big( \abs{x-x_i} + \abs{x'-x_j} - \abs{x-x'} - \abs{x_i-x_j} \big).
\]
According to \citet[Section 10]{van2008reproducing}, $\cF$ contains functions possessing a square integrable weak derivative.
The posterior mean of $f$ based on an I-prior is then a (one-dimensional) smoother for the data.

With $w_k\iid\N(0,\psi)$, functions in $\cF$ are of the form
\begin{align*}
  f(x) 
  &= \sum_{k=1}^n h(x,x_k)w_k \\
  &= \sum_{k=1}^n \left( \frac{\lambda}{2n^2} \sum_{i=1}^n\sum_{j=1}^n \big( \abs{x-x_i} + \abs{x_k-x_j} - \abs{x-x_k} - \abs{x_i-x_j} \big) \right) w_k.
\end{align*}
The derivative of $f(x)$ with respect to $x$ is
\begin{align*}
  \frac{\d }{\d x}f(x) 
  &= \frac{\lambda}{2n^2} \sum_{k=1}^n  \sum_{i=1}^n\sum_{j=1}^n \left( \frac{\d}{\d x}\abs{x-x_i}  - \frac{\d}{\d x}\abs{x-x_k}  \right) w_k \\
  &= \frac{\lambda}{2n^2} \sum_{k=1}^n  \sum_{i=1}^n\sum_{j=1}^n \left( \sign(x-x_i)  - \sign(x-x_k)  \right) w_k \\
  &= \frac{\lambda}{2n} \sum_{k=1}^n  \sum_{i=1}^n           \left( \sign(x-x_i)  - \sign(x-x_k)  \right) w_k \\
\end{align*}




