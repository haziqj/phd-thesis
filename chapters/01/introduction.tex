\documentclass[a4paper,showframe,11pt,draft]{report}
\usepackage{standalone}
\standalonetrue
\ifstandalone
  \usepackage{../../haziq_thesis}
  \usepackage{../../haziq_maths}
  \usepackage{../../haziq_glossary}
  \addbibresource{../../bib/haziq.bib}
\fi

\begin{document}
\hChapterStandalone{Introduction}

Regression analysis is undoubtedly one of the most important tools available at a practitioner's disposal to understand the relationship between one or more explanatory variables $x$, and the independent variable of interest, $y$.
This relationship is usually expressed as $y \approx f(x;\theta)$, where $f$ is called the \emph{regression function}, and this is   dependent on one or more parameters denoted by $\theta$.
Regression analysis concerns the estimation of said regression function, and once a suitable estimate $\hat f$ has been found, post-estimation procedures such as prediction, and inference surrounding $f$ or $\theta$, may be performed.

Estimation of the regression function may be done in many ways.
This thesis concerns the use of \emph{I-priors} \citep{bergsma2017}, in a semi-Bayesian manner, for regression modelling.
The I-prior is an objective, entropy-maximising prior for the regression function which makes use of its Fisher information.
The essence of regression modelling using I-priors is introduced briefly below, but as the development of I-priors is fairly recent, and we dedicate a full chapter (Chapter 2) to describe the concept fully.

This thesis has three main chapters which we hope to present as methodological innovations surrounding the use of I-priors for regression modelling.
Chapter 3 describes computational methods relating to the estimation of I-prior models.
Chapter 4 extends the I-prior methodology to fit discrete outcome models.
Chapter 5 discusses the use of I-priors for model selection. 
This short chapter provides an outline of the thesis, in addition to introducing the statistical model of interest.

\section{Regression models}

For subject $i \in \{1,\dots,n\}$, assume a real-valued response $y_i$ has been observed, as well as a row vector of $p$ covariates $x_i = (x_{i1},\dots,x_{ip})$, where each $x_{ik}$ belongs to some set $\cX_k$, for $k = 1,\dots,p$.
Let $\cS = \{(y_1, x_1), \dots, (y_n,x_n)\}$ denote this observed sample of size $n$.
Consider then the following regression model, which stipulates the dependence of the $y_i$ on the $x_i$:
\begin{align}\label{eq:model1}
  y_i = \alpha + f(x_i) + \epsilon_i,
\end{align}
where $f$ is some regression function to be estimated, and $\alpha$ is an intercept.
Additionally, it is assumed that the errors $\epsilon_i$ are normally distributed according to
\begin{align}\label{eq:model1ass}
  (\epsilon_1, \dots, \epsilon_n)^\top \sim \N_n(0, \bPsi^{-1}).
\end{align}
where $\bPsi = (\psi_{ij})_{i,j=1}^n$ is the precision matrix.
We shall often refer to model \eqref{eq:model1} subject to \eqref{eq:model1ass} as the \emph{normal regression model}.
The choice of multivariate normal errors is not only a convenient one (as far as distributional assumptions go), but one that is motivated by the principle of maximum entropy.

Interestingly, a wide variety of statistical models can be captured by the seemingly humble normal regression model, simply by varying the form of the regression function $f$.
For instance, when $f$ can be parameterised linearly as $f(x_i) = x_i \beta$, $\beta \in \bbR^p$, we then have the ordinary linear regression---a staple problem in statistics and other quantitative fields.

We might also have that the data is separated naturally into groups or levels by design, for example, data from stratified sampling, students within schools, or longitudinal measurements over time.
In such a case, we might want to consider a regression function with additive components
%
\[
  f(x_i^{(j)}, j) = f_1(x_i^{(j)}) + f_2(j) + f_{12}(x_i^{(j)}, j)
\]
%
where $x_i^{(j)}$ denotes the $p$-dimensional $i$th observation for group $j\in\{1,\dots,m\}$.
Again, assuming a linear parameterisation, this is recognisable as the multilevel or random-effects linear model, with $f_2$ representing the varying intercept via $f_2(j) = \alpha_{j}$, $f_{12}$ representing the varying slopes via $f_{12}(x_{ij},j) = x_{i} \beta_{j}$, with $\beta_j \in \bbR^p$, and $f_1$ representing the fixed-effects linear component $x_i\beta$ as above.

Moving on from linear models, smoothing models may be of interest as well.
A myriad of models exist for this type of problem, with most classed as nonparametric regression, and the more popular ones include LOcal regrESSion (LOESS), kernel regression, and smoothing splines.
Semiparametric regression models, on the other hand, combines the linear component of a regression model with a non-parameteric component.

Further, the regression problem is made more intriguing when the set of covariates $\cX$ is functional---in which case the linear regression model aims to estimate coefficient functions $\beta:\cT \to \bbR$ from the model
\[
  y_i = \int_\cT x_i(t)\beta(t)\d t + \epsilon_i.
\]
Nonparametric and semiparametric regression with functional covariates have also been widely explored.
Models of this nature still fall under the remit of the normal regression model by selecting a regression functional with domain over the functional covariates.

\section{Vector space of functions}

It would be beneficial to prescribe some sort of structure from which we may choose a regression function appropriately. 
This is given to us by assuming that our regression function for the normal model lies in some reproducing kernel Hilbert space (RKHS) $\cF$, with reproducing kernel $h:\cX \times \cX \to \bbR$.
Often, the reproducing kernel (or simply kernel, for short) is indexed by one or more parameters which we shall denote as $\eta$.
Correspondingly, the kernel is rightfully denoted as $h_\eta$ to indicate the dependence of the parameters on the kernels, though where this is seemingly obvious, might be omitted.

Due to the one-to-one mapping between the set of kernel functions and the set of RKHSs, choosing a regression function is equivalent to choosing a kernel function, and this is chosen according to the desired effects of the covariates on the regression function.
An in-depth discussion on kernels and RKHSs will be provided later in Chapter 2, but for now, it suffices to say that kernels which invoke a linear, smooth and categorical dependence, are of interest.
This would allow us to fit the various models described earlier within this RKHS framework.

\section{Estimating the regression function}

Tikhonov regularisation, Bayesian interpretation, priors

Having decided on a functional structure for $f$, we now turn to the task of estimating $f$.
%In doing so, we undertake a Bayesian approach of prescribing prior information for our regression function, and then work out the posterior regression function given the data.

\hltodo{Remark on dimensionality}.

\section{Regression using I-priors}

The definition of an RKHS entails that any function in $\cF$ can be approximated arbitrarily well by functions of the form
%
\begin{align}\label{eq:ipriorre}
  f(x) = f_0(x) + \sum_{i=1}^n h_\eta(x,x_i)w_i
\end{align}
%
where $w_1,\dots,w_n$ are real-valued\footnotemark.
Here, $f_0 \in \cF$ is some function chosen a priori which represents the `best guess' of the regression function.
The \emph{I-prior} for our regression function $f$ in \eqref{eq:model1} subject to \eqref{eq:model1ass} is defined as the distribution of a random function of the form \eqref{eq:ipriorre} when the $w_i$ are distributed according to 
%
\[
  (w_1,\dots,w_n)^\top \sim \N_n(\bzero,\bPsi),
\]
%
%where $\bff_0 = \big(f_0(x_1),\dots,f_0(x_n) \big)^\top$ is a vector of prior means for the regression function.
where $\bzero$ is a length $n$ vector of zeroes.
As a result, we may view the I-prior for $f$ as having the Gaussian process distribution
%
\begin{align}\label{eq:iprior}
  \bff := \big(f(x_1),\dots,f(x_n) \big)^\top \sim \N_n(\bff_0, \bH_\eta\bPsi\bH_\eta)
\end{align}
%
with $\bH_{\eta}$ an $n \times n$ matrix with $(i,j)$ entries equal to $h_\eta(x_i,x_j)$, and $\bff_0$ a vector containing the $f_0(x_i)$'s.
The covariance matrix of this multivariate normal prior is related to the Fisher information for $f$ \citep{bergsma2017}, and hence the name I-prior---the `I' stands for information.
Furthermore, the I-prior happens to be an entropy-maximising prior, subject to certain \hltodo[which are?]{constraints}.
More on the I-prior in Chapter 2.

\footnotetext{
That is to say, $\cF$ is spanned by the functions $h(\cdot,x)$. 
More precisely, $\cF$ is the completion of the space $\cG = \text{span}\{h(\cdot,x) | x \in \cX \}$ endowed with the squared norm $\norm{f}^2_\cG = \sum_{i=1}^n\sum_{i=1}^n w_i w_j h(x_i,x_j)$ for $f$ of the form \eqref{eq:ipriorre}. 
See, for example, \cite{berlinet2011reproducing} for details.
}

As with Gaussian process regression (GPR), the function $f$ is estimated by its posterior mean.
For the normal model, the posterior distribution for the regression function conditional on the responses $\by = (y_1,\dots,y_n)$,
%
\begin{align}
  p(\bff|\by) = \frac{p(\by|\bff)p(\bff)}{\int p(\by|\bff)p(\bff) \d \bff},
\end{align}
%
can easily be found, and it is in fact normally distributed.
The posterior mean for $f$ evaluated at a point $x \in \cX$ is given by
%
\begin{align}\label{eq:postmean}
  \E\big[f(x)\big|\by\big] = f_0(x) + \bh_\eta^\top(x) \cdot
  {\color{gray}
  \overbrace{\color{black} \bPsi\bH_\eta\big(\bH_\eta\bPsi\bH_\eta + \bPsi^{-1}\big)^{-1} (\by -\bff_0 )}^{\tilde \bw}
  }
\end{align}
%
where we have defined $\bh_\eta(x)$ to be the vector of length $n$ with entries $h_\eta(x,x_i)$ for $i=1,\dots,n$.
Incidentally, the elements of the $n$-vector $\tilde \bw$ defined in \eqref{eq:postmean} are the posterior means of the random variables $w_i$ in the formulation \eqref{eq:ipriorre}.
The point-evaluation posterior variance for $f$ is given by
%
\begin{align}\label{eq:postvar}
  \Var\big[f(x)\big|\by\big] = \bh_\eta^\top(x)\big( \bH_\eta\bPsi\bH_\eta + \bPsi^{-1} \big)^{-1} \bh_\eta^\top(x).
\end{align}
%
Prediction for a new data point $x_\new \in \cX$ then concerns obtaining the \emph{posterior predictive distribution}
%
\[
  p(y_\new|\by) = \int p(y_\new|f_\new,\by)p(f_\new|\by) \d f_\new,
\]
%
where we had defined $f_\new := f(x_\new)$.
This is again a normal distribution in the case of the normal model, with the same mean\footnotemark~as in \eqref{eq:postmean}, but a slightly different variance.
These are of course well-known results in Gaussian process literature---see, for example, \cite{rasmussen2006gaussian} for details.

\footnotetext{
The fact that it is the same is inconsequential.
It happens to be that the mean of the predictive distribution $\E[y_\new|\by]$ for a normal model is the same as \emph{prediction of the mean at the posterior}, $\E[f(x_\new)|\by]$.
\cite{rasmussen2006gaussian} points out that this is due to symmetries in the model and the posterior.
}

There is also the matter of optimising model parameters $\theta$.
In our case, $\theta$ collectively refers to the kernel parameters $\eta$ and the precision matrix of the errors $\bPsi$.
$\theta$ may be estimated in several ways, either by likelihood-based methods or fully Bayesian methods.
The former includes methods such as direct maximisation of the (marginal) likelihood, $L(\theta) = \int p(\by|\theta,\bff)p(\bff)\d\bff$, and the expectation-maximisation (EM) algorithm.
Both are seen as a form of \emph{empirical Bayes} estimation.
In a fully Bayesian setting on the other hand, Markov chain Monte Carlo methods may be employed, assuming prior distributions on the model parameters.

\section{Advantages and limitations of I-priors}

The I-prior methodology has the following advantages:

\begin{enumerate}
  \item \textbf{A unifying methodology for various regression models.}
  
  The I-prior methodology has the ability to fit a multitude of regression models simply by choosing the RKHS to which the regression function belongs.
  As such, it can be seen as a unifying methodology for various regression models. 
  
  \item \textbf{Simple estimation procedure.}
  
  Estimation of model parameters using the aforementioned methods are very 
  \hltodo[Why/How is it simple?]{simple} 
  to implement, barring any computational and numerical hurdles, which will be discussed.
  
  \item \textbf{Prevents over-fitting and under-smoothing.}
  
  When considering functions that minimise the squared loss function\footnote{For the normal model, this is in fact twice the negative log-likelihood of $f$, up to a constant.}
  \[
    \Lambda\big(y_i,f(x_i)\big) = \sum_{i=1}^n\sum_{j=1}^n \psi_{ij} \big(y_i - f(x_i)\big)\big(y_j - f(x_j)\big),
  \]
  over-fitting is a genuine concern.
  In fact, any function $f$ that passes through the data points minimises $\Lambda$.
  Regularising the problem with the use of I-priors prevents over-fitting.
  Though over-fitting can be solved using Tikhonov regularisation, the posterior solution under an 
  \hltodo[Cite or show later?]{I-prior does not tend to under-smooth as much as Tikhonov regularisation does}.
  Under-smoothing can adversely impact the estimate of $f$, and in real terms might even show features and artefacts that are not really there.
  
  \item \textbf{Better prediction.}
  
  Empirical studies and real-data examples show that small and large sample predictive performance of I-priors are comparative to, and often better than, other leading state-of-the-art models, including the closely related Gaussian process regression.

  \item \textbf{Straightforward inference.}
  
  Marginal likelihoods after integrating out the I-prior are easily obtained, making model selection via comparison of likelihood a viable option.
  This method of comparing marginal likelihood with maximum likelihood estimate plug-ins of the model parameters, is viewed as comparing empirical Bayes factors in the Bayesian literature.
  
\end{enumerate}

The main drawback of using I-prior models computational in nature, namely, the requirement of working with an $n \times n$ matrix and its inverse, as seen in Equations \eqref{eq:postmean} and \eqref{eq:postvar}, regardless of estimation method (ML or Bayes).
Analysis of data sets that are not more than a few thousand in size can be considered feasible; anything more than this is debilitatingly slow to compute.
In addition, care must be taken to avoid numerical instabilities when calculating the marginal log-likelihood during parameter estimation, which can affect gradient based optimisation or the EM algorithm.

Another issue when performing likelihood based methods is that the optimisation objective may be non-convex such that multiple local optima may exist. 
In such cases, multiple restarts from different initialisation may ultimately lead to a global maximum, although some difficulties may be faced when numerical instabilities occur.

Lastly, a remark on model assumptions, which are twofold: 1) Assumption of $f \in \cF$, some RKHS; and 2) normality of errors.
Of the two, the latter is more likely to be violated, especially when dealing with discrete responses, e.g. in classification.
Deviating from the normality assumption would require approximation techniques to be implemented in order to obtain the posterior distributions of interest.

\section{Outline of thesis}


\input{old.tex}





\hClosingStuffStandalone
\end{document}
