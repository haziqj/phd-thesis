Often times, there are unobserved, random variables $\bw=\{w_1,\dots,w_n\}$ that are assumed to make up the data generative process, prescribed in the statistical model through the \emph{joint pdf} $p(\by,\bw|\theta)$.
Examples of models that include latent variables are plenty: Gaussian mixture models, latent class analysis, factor models, random coefficient models, and so on.
In order to obtain ML estimates through a direct maximisation of the likelihood, it is necessary to first marginalise out the latent variables via
\begin{equation}\label{eq:varint}
  p(\by|\theta) 
  = \int 
  \myoverbrace{p(\by|\bw,\theta)p(\bw|\theta)}{p(\by,\bw|\theta)}
  \dint \bw
\end{equation}
and obtain the \emph{marginal likelihood}.
Note that the integral is replaced by a summation over all possible values in the case of discrete latent variables $\bw$.

Direct maximisation of the marginal (log-)likelihood might not be favourable due to intractability in obtaining ML solutions.
The form of the marginal likelihood might might not be conducive for closed-form estimates to be found, necessitating the use of numerical, gradient-based methods which is subject to its own undesirable quirks.
Moreover, when the evaluation of the (log-)likelihood, gradient and/or Hessian are expensive to compute, then numerical methods are burdensome to execute.

It is usually the case that if the latent variables $\bw$ were somehow known, estimation would be made simpler.
That is, the solution to $\argmax_\theta \log p(\by,\bw|\theta)$ can be obtained in a simple manner.
The expectation-maximisation algorithm \citep{dempster1977maximum}, commonly known as the EM algorithm, is an iterative procedure which exploits the fact that the so-called \emph{complete data likelihood} is easier to work with.
Correspondingly, in EM terminology, the marginal likelihood is referred to as the \emph{incomplete data likelihood}.

We describe a derivation of both a general EM algorithm and an EM algorithm for models whose data generative pdf belongs to an exponential family of pdfs. 
Interestingly, the EM algorithm can be modified to obtain maximum a posteriori estimates or penalised log-likelihood solutions.
As a note, the EM algorithm is not an algorithm per se, in that it does not provide exact instructions as to what the E- and M-steps should comprise of.
Rather, it is a generic device to obtain parameter estimates \citep{mclachlan2007algorithm}.

\section{Derivation of the EM algorithm}

For want of an iterative procedure to obtain maximum likelihood estimates, we seek a solution to 
\begin{equation}\label{eq:em1}
  \argmax_\theta \{ L(\theta|\by) - L(\theta^{(t)}|\by) \geq 0 \},
\end{equation}
where the solution to \cref{eq:em1} yields an improvement to the current $t$'th iteration of the log-likelihood value $L(\theta^{(t)}|\by)$.
Note that the objective function in \cref{eq:em1} forms an upper bound for the quantity $Q(\theta|\theta^{(t)})$, as shown below:
\begin{align*}
  L(\theta|\by) - L(\theta^{(t)}|\by)
  \phantom{:}={}& \log \int p(\by|\bw,\theta)p(\bw|\theta) \, \frac{p(\bw|\by,\theta^{(t)})}{p(\bw|\by,\theta^{(t)})}  \dint\bw - \log p(\by|\theta^{(t)}) \\
  \phantom{:}\geq{}& \int p(\bw|\by,\theta^{(t)}) \log \frac{p(\by|\bw,\theta)p(\bw|\theta)}{p(\bw|\by,\theta^{(t)})} \dint\bw \mycomment{(Jensen's inequality)} \\
  \phantom{:}& - \log p(\by|\theta^{(t)})\int p(\bw|\by,\theta^{(t)}) \dint \bw \\
  \phantom{:}={}& \int p(\bw|\by,\theta^{(t)}) \log \frac{p(\by|\bw,\theta)p(\bw|\theta)}{p(\bw|\by,\theta^{(t)})p(\by|\theta^{(t)})} \dint\bw\\
  =:{}& Q(\theta|\theta^{(t)}).
\end{align*}
Evidently, to maximise $L(\theta|\by)$, we can't do any worse than maximising $Q(\theta|\theta^{(t)})$ in $\theta$.
Denote by $\theta^{(t+1)}$ as the maximiser of $Q(\theta|\theta^{(t)})$. 
Then,
\begin{align*}
  \theta^{(t+1)}
  &= \argmax_\theta \int p(\bw|\by,\theta^{(t)}) \log \frac{p(\by|\bw,\theta)p(\bw|\theta)}{p(\bw|\by,\theta^{(t)})p(\by|\theta^{(t)})} \dint\bw \\
  &= \argmax_\theta \int p(\bw|\by,\theta^{(t)}) \log p(\by|\bw,\theta)p(\bw|\theta) \dint\bw \\
  &= \argmax_\theta \int p(\bw|\by,\theta^{(t)}) \log p(\by,\bw|\theta) \dint\bw \\
  &= \argmax_\theta \E_\bw \left[\log p(\bw,\by|\theta) | \by, \theta^{(t)} \right] 
\end{align*}
We arrive at an iterative procedure summarised succintly as the following:

\begin{algorithm}[H]
\caption{EM algorithm}\label{alg:EM4}
\begin{algorithmic}[1]
  \State \textbf{initialise} $\btheta^{(0)}$ and $t\gets 0$
  \While{not converged}
    \State E-step: compute $Q(\theta|\theta^{(t)}) = \E_\bw \left[\log p(\bw,\by|\theta) | \by, \theta^{(t)} \right]$
    \State M-step: $\theta^{(t+1)} \gets \argmax_\theta Q(\theta|\theta^{(t)})$
    \State $t \gets t + 1$
  \EndWhile
\end{algorithmic}
\end{algorithm}

\vspace{-0.9em}
Notice that the log-likelihood function satisfies
\begin{equation}
  L(\theta|\by) \geq L(\theta^{(t)}|\by) + Q(\theta|\theta^{(t)}),
\end{equation}
for which equality is achieved when $\theta = \theta^{(t)}$, since
\begin{align*}
  Q(\theta^{(t)}|\theta^{(t)})
  &= \int p(\bw|\by,\theta^{(t)}) \log \frac{p(\by|\bw,\theta^{(t)})p(\bw|\theta^{(t)})}{p(\bw|\by,\theta^{(t)})p(\by|\theta^{(t)})} \dint\bw \\
  &= \int p(\bw|\by,\theta^{(t)}) \cancelto{0}{\log \frac{p(\by,\bw|\theta^{(t)})}{p(\by,\bw|\theta^{(t)})}} \dint\bw\\
  &=0.
\end{align*}
This implies that the EM algorithm improves the log-likelihood values at each iteration, since
\begin{align*}
  L(\theta^{(t+1)}|\by) - L(\theta^{(t)}|\by) \geq  Q(\theta^{(t+1)}|\theta^{(t)}) \geq 0
\end{align*}
and $Q(\theta^{(t+1)}|\theta^{(t)}) \geq Q(\theta^{(t)}|\theta^{(t)}) = 0$ since $\theta^{(t+1)}$ maximises $Q(\cdot|\theta^{(t)})$.

The expectation in the E-step involves the conditional pdf $p(\bw|\by,\theta^{(t)})$.
Viewed through a Bayesian lens, this is the posterior density of the latent variables using the $t$'th iteration parameter values.
The success of the E-step is predicated on the availability of the conditional pdf for the expectation.
If not, approximations to the E-step can be explored, for example using Monte Carlo methods \citep{wei1990monte} or a variational approximation \citep{beal2003variational}.

The solution to the M-step usually, but not always, exists in closed form.
Maximising the $Q$ function over all possible values of $\theta$ may not be feasible \citep{mclachlan2007algorithm}.
In such situations, the generalised EM algorithm (as defined by \cite{dempster1977maximum}) requires only that $\theta^{(t+1)}$ be chosen in a way that
\[
  Q(\theta^{(t+1)}|\theta^{(t)}) \geq Q(\theta^{(t)}|\theta^{(t)}).
\]
That is, $\theta^{(t+1)}$ is chosen so as to increase the value of the $Q$ function at its current parameter value.
As seen in the argument above, this requirement is sufficient for a guaranteed increase in the log-likelihood function at each iteration.


\section{Exponential family EM algorithm}
\label{apx:expem}

Consider the density function $p(\cdot|\btheta)$ of the complete data $\bz = \{\by,\bw\}$, which depends on parameters $\btheta = (\theta_1,\dots,\theta_s)^\top \in\Theta\subseteq\bbR^s$, belonging to an exponential family of distributions.
This density takes the form $p(\bz|\btheta) = B(\bz) \exp \big( \ip{\bfeta(\btheta), \bT(\bz)} -  A(\btheta) \big)$, where $\bfeta:\bbR^s \mapsto \bbR$ is a link function,  $\bT(\bz) = \big(T_1(\bz),\dots,T_s(\bz)\big)^\top \in \bbR^s$ are the sufficient statistics of the distribution, and $\ip{\cdot,\cdot}$ is the usual Euclidean dot product.
It is often easier to work in the \emph{natural parameterisation} of the exponential family distribution
\begin{align}\label{eq:pdfexpfamnat}
  p(\bz|\bfeta) = B(\bz) \exp \big( \ip{\bfeta, \bT(\bz)} -  A^*(\bfeta) \big)
\end{align}
by defining $\bfeta := \big(\eta_1(\btheta),\dots,\eta_r(\btheta)\big) \in \cE$, and $\exp A^*(\bfeta) = \int B(\bz) \, \exp \, \ip{\bfeta, \bT(\bz)}  \dint \bz$ to ensure the density function normalises to one.
As an aside, the set $\cE := \big\{ \bfeta = (\eta_1,\dots,\eta_s) \,|\, \int  \exp A^*(\bfeta) < \infty \big\}$ is called the \emph{natural parameter space}.
If $\dim \cE = r < s = \dim \Theta$, then the the pdf belongs to the \emph{curved exponential family} of distributions.
If $\dim \cE = r = s = \dim \Theta$, then the family is a \emph{full exponential family}.

Assuming the latent $\bw$ variables are observed and working with the natural parameterisation, then the complete maximum likelihood (ML) estimate for $\bfeta$ is obtained by solving 
\begin{align}\label{eq:expEM1}
  \frac{\partial}{\partial\bfeta}\log p(\bz|\bfeta)
  &= \bT(\bz) - \frac{\partial}{\partial\bfeta} A^*(\bfeta) = 0.
\end{align}
Of course, the variable $\bw$ are never observed, so the ML estimate for $\bfeta$ can only be informed from what is observed.
Let $p(\by|\bfeta) = \int p(\by,\bw|\bfeta) \dint \bw$ represent the marginal density of the observations $\by$.
Now, the ML estimate for $\bfeta$  is obtained by solving
\begin{align}
  \frac{\partial}{\partial\bfeta}\log p(\by|\bfeta)
  &= \frac{1}{p(\by|\bfeta)} \cdot \frac{\partial}{\partial\bfeta}  p(\by|\bfeta) \nonumber \\
  &= \frac{1}{p(\by|\bfeta)} \cdot \frac{\partial}{\partial\bfeta} \left( \int p(\by,\bw|\bfeta) \dint \bw \right) \nonumber \\
  &= \frac{1}{p(\by|\bfeta)}  \int \left( \frac{\partial}{\partial\bfeta} p(\by,\bw|\bfeta) \right) \dint \bw \nonumber \\
  &= \frac{1}{p(\by|\bfeta)}  \int \left( p(\by,\bw|\bfeta) \frac{\partial}{\partial\bfeta} \log p(\by,\bw|\bfeta) \right) \dint \bw \nonumber \\
  &= \int \left( \bT(\by,\bw) - \frac{\partial}{\partial\bfeta} A^*(\bfeta) \right) p(\bw|\by,\bfeta) \dint \bw \nonumber \\
  &= \E_\bw \big[ \bT(\by,\bw) | \by \big] - \frac{\partial}{\partial\bfeta} A^*(\bfeta) \label{eq:expEM2}
\end{align}
equated to zero.
Note that we are allowed to change the order of integration and differentation provided the integrand is continuously differentiable.
So the only difference between the first order condition of \cref{eq:expEM1} and that of \cref{eq:expEM2} is that the sufficient statistics involving the unknown $\bw$ are replaced by their conditional or posterior expectations.
%but this is not an issue: by the law of total expectations, $\E\bT(\bz) = \E \bT(\by,\bw) = \E \big[ \E_\bw[\bT(\by,\bw)|\by] \big]$
%so solving $\bT(\bz) = \E \bT(\bz)$ for $\btheta$ is not possible without some manipulation.

A useful identity to know is that $\frac{\partial}{\partial\bfeta} A^*(\bfeta) = \E_\bz[ \bT(\bz)]$ \citep[Thm. 3.4.2 \& Exer. 3.32(a)]{casella2002statistical}, which can be expressed in terms of the original parameters $\btheta$.
As a consequence, solving for the ML estimate for $\btheta$ from the FOC equations \cref{eq:expEM2} is possible without having to deal with the derivative of $A^*$ with respect to the natural parameters.
Having said this, an analytical solution in $\btheta$ may not exist, because the relationship of $\btheta$ could be implicit in the set of equations $\E_\bw \big[ \bT(\bw,\by) | \by, \btheta \big] = \E_{\by,\bw}\left[ \bT(\by,\bw) | \btheta \right]$.
One way around this is to employ an iterative procedure, as detailed in \cref{alg:EM3}.

\begin{algorithm}[hbt]
\caption{Exponential family EM}\label{alg:EM3}
\begin{algorithmic}[1]
  \State \textbf{initialise} $\btheta^{(0)}$ and $t\gets 0$
  \While{not converged}
    \State E-step: $\tilde\bT^{(t+1)}(\by,\bw) \gets \E_\bw \big[ \bT(\bw,\by) | \by, \btheta^{(t)} \big]$
    \State M-step: $\btheta^{(t+1)} \gets$ solution to $\tilde\bT^{(t+1)}(\by,\bw) = \E_{\by,\bw}\left[ \bT(\by,\bw) | \btheta \right]$
    \State $t \gets t + 1$
  \EndWhile
\end{algorithmic}
\end{algorithm}

To see how \cref{alg:EM3} motivates the EM algorithm, consider the following argument.
Recall that for the EM algorithm, the function $Q_t(\bfeta) = \E_\bw[\log p(\by,\bw|\bfeta) | \by,\bfeta^{(t)}]$ is maximised at each iteration $t$.
For exponential families of the form \cref{eq:pdfexpfamnat}, the $Q_t$ function turns out to be
\[
 Q_t(\bfeta) = \E_\bw \big[ \ip{\bfeta, \bT(\bz)} | \by,\bfeta^{(t)} \big] -  A^*(\bfeta) + \log B(\bz),
\]
and this is maximised at the value of $\bfeta$ satisfying
\begin{align*}
  \frac{\partial}{\partial\bfeta} Q_t(\bfeta)
  &= \E_\bw \big[ \bT(\by,\bw) | \by,\bfeta^{(t)} \big] - \frac{\partial}{\partial\bfeta}A^*(\bfeta) = 0,
\end{align*}
a similar condition to \cref{eq:expEM2} when obtaining ML estimate of $\bfeta$.
Thus, $Q_t$ is maximised by the solution to line 4 in  \cref{alg:EM3}.

\section{Bayesian EM algorithm}

A simple modification of the EM algorithm can be done to obtain maximum a posteriori estimates, or maximum penalised likelihood estimates.
Under a Bayesian framework, a prior is assigned on the model parameters, $\theta\sim p(\theta)$.
Recall that the MAP estimate is obtained as the maximiser of the log-density $\log p(\by|\theta) + \log p(\theta)$.
%\begin{equation}
%  \log p(\by|\theta) + \log p(\theta).
%\end{equation}

The EM algorithm works as before, but replaces the E-step with 
\begin{equation}\label{eq:bayesem}
  \E_\bw \left[\log p(\bw,\by|\theta) + \log p(\theta) | \by, \theta^{(t)} \right]
  = Q(\theta|\theta^{(t)}) + \log p(\theta)
\end{equation}
since $\log p(\theta)$ has no terms involving the latent variables $\bw$.
The M-step now maximises \cref{eq:bayesem} with respect to $\theta$, which includes the log prior density (or a penalty term).
It would seem that the regular EM algorithm maximises \cref{eq:bayesem} such that $p(\theta) \propto \const$ is a diffuse prior for $\theta$.
\citet{beal2003} discuss a more Bayesian extension of EM, in which the output of the so-called \emph{variational Bayes EM} algorithm are (approximate) posterior distributions of the parameters, rather than MAP estimates discussed here.

