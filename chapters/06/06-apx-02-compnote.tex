\section{Computational note}

From \cref{apx:gibbsbvs}, we see that all of the Gibbs conditionals are of recognisable form, making Gibbs sampling a straightforward MCMC method to implement.
We built an \proglang{R} \hltodo[Cite this with correct version.]{package} \pkg{ipriorBVS} that uses \proglang{JAGS} \citep{plummer2003jags}, a variation of \proglang{WinBUGS}, internally for the Gibbs sampling, and wrote a wrapper function which takes formula based inputs for convenience.
The \pkg{ipriorBVS} also performs two-stage BVS, and supported priors are the I-prior, $g$-prior, and independent prior, as used in this thesis.
Although a Gibbs sampler could be coded from scratch, \proglang{JAGS} has the advantage of being tried and tested and has simple controls for tuning (burn-in, adaptation, thinning, etc.).
Furthermore, the output from \proglang{JAGS} can be inspected using a myriad of multipurpose MCMC tools to diagnose convergence problems.
The \pkg{ipriorBVS} package is available at \url{https://github.com/haziqj/ipriorBVS}.

In all examples, a default setting of 4,000 burn-in samples, 1,000 adaptation size, and 10,000 samples with no thinning seemed adequate.
There were no major convergence issues encountered.

Computational complexity is dominated by the inversion of a $p \times p$ matrix, and matrix multiplications of order $O(np^2)$.
These occur in the conditional posterior for $\bbeta$.
Overall, if $n \gg p$, then time complexity is $O(np^2)$.
Storage requirements are $O(np)$.