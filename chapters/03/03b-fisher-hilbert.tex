We extend the idea beyond thinking about parameters as merely numbers in the usual sense, to abstract objects in Hilbert spaces. 
This generalisation allows us to extend the concept of Fisher information to regression functions in RKHSs later.
The score and Fisher information is derived in a familiar manner, but extra care is required when taking derivatives with respect to Hilbert space objects.  
We discuss a generalisation of the concept of differentiability from real-valued functions of a single, real variable, as is common in calculus, to functions between Hilbert spaces.

\begin{definition}[Fréchet derivative]\label{def:frechet}
  Let $\cV$ and $\cW$ be two Hilbert spaces, and $\cU \subseteq \cV$ be an open subset.
  A function $f:\cU\to\cW$ is called \emph{Fréchet differentiable} at $x \in \cU$ if there exists a bounded, linear operator $T:\cV\to\cW$ such that 
  \[
    \lim_{v\to 0} \frac{\big\Vert f(x+v) - f(x) - T v \big\Vert_{\cW}}{\norm{v}_\cV} = 0
  \]
  If this relation holds, then the operator $T$ is unique, and we write $\d f(x) := T$ and call it the \emph{Fréchet derivative} or \emph{Fréchet differential} of $f$ at $x$.
  If $f$ is differentiable at every point $\cU$, then $f$ is said to be \emph{(Fréchet) differentiable} on $\cU$.
\end{definition}

\begin{remark}
  Since $\d f(x)$ is a bounded, linear operator, by Lemma \cref{thm:boundcont}, it is also continuous. 
\end{remark}

\begin{remark}
  While the Fréchet derivative is most commonly defined as derivatives of functions between Banach spaces,  the definition itself also applies to Hilbert spaces.
  Since our main focus are RKHSs, it is presented as such, and we follow the definitions supplied in \citet[Definition 3.6.5]{balakrishnan1981applied} and \citet[Section 6]{bouboulis2011extension}.
\end{remark}

\begin{remark}
  The use of the open subset $\cU$ in the definition above for the domain of the function $f$ is so that the notion of $f$ being differentiable is possible even without having it defined on the entire space $\cV$.
\end{remark}

The intuition here is similar to that of regular differentiability, in that the linear operator $T$ well approximates the change in $f$ at $x$ (the numerator), relative to the change in $x$ (the denominator)---the fact that the limit exists and is zero, it must mean that the numerator converges faster to zero than the denominator does.
In Landau notation, we have the familiar expression $f(x+v) = f(v) + \d f(x)(v) + o(v)$, that is, the derivative of $f$ at $x$ gives the best linear approximation to $f$ near $x$.
Note that the limit in the definition is meant in the usual sense of convergence of functions with respect to the norms of $\cV$ and $\cW$.

For the avoidance of doubt, $\d f(x)$ is not a vector in $\cW$, but is an element of the set of bounded, linear operators from $\cV$ to $\cW$, denoted $\L(\cV;\cW)$.
That is, if $f:\cU\to\cW$ is a differentiable function at all points in $\cU\subseteq\cV$, then its derivative is a linear map
\begin{align*}
  \d f: \cU &\to \text{L}(\cV;\cW) \\
  x &\mapsto \d f(x).
\end{align*}
It follows that this function may also have a derivative, which by definition will be a linear map as well.
This is the \emph{second Fréchet derivative} of $f$, defined by
\begin{align*}
  \d^2 f: \cU &\to \text{L}\big(\cV;\text{L}(\cV;\cW)\big) \\
  x &\mapsto \d^2f(x).
\end{align*}
To make sense of the space on the right-hand side, consider the following argument.
\begin{itemize}
  \item Take any $\phi(\cdot) \in \text{L}\big(\cV;\text{L}(\cV;\cW)\big)$. For all $v\in\cV$, $\phi(v)\in\text{L}(\cV;\cW)$, and $\phi(v)$ is linear in $v$.
  \item Since $\phi(v)\in\text{L}(\cV;\cW)$, it is itself a linear operator taking elements from $\cV$ to $\cW$. We can write it as $\phi(v)(\cdot)$ for clarity.
  \item So, for any $v'\in\cV$, $\phi(v)(v')\in\cW$, and it depends linearly on $v'$ too. Thus, given any two $v,v'\in\cV$, we obtain an element $\phi(v)(v')\in\cW$ which depends linearly on both $v$ and $v'$.
  \item It is therefore possible to identify $\phi \in \text{L}\big(\cV;\text{L}(\cV;\cW)\big)$ with an element $\psi \in \text{L}(\cV \times \cV, \cW)$ such that for all $v,v'\in\cV$, $\phi(v)(v') = \psi(v,v')$.
\end{itemize}
To summarise, there is an isomorphism between the space on the right-hand side and the space $\L(\cV \times \cV, \cW)$ of all continuous bilinear maps from $\cV$ to $\cW$.
The second derivative $\d^2 f(x)$ is therefore a bounded, bilinear operator from $\cV\times\cV$ to $\cW$.

Another closely related type of differentiability is the concept of \emph{Gâteaux differentials}, which is the formalism of the functional derivative in calculus of variations.
Let $\cV$, $\cW$ and $\cU$ be as before, and consider the function $f:\cU\to\cW$.

\begin{definition}[Gâteaux derivative]
  The \emph{Gâteaux differential} or the \emph{Gâteaux derivative} $\partial_v f(x)$ of $f$ at $x \in \cU$ in the direction $v\in\cV$ is defined as
  \[
    \partial_v f(x) = \lim_{t \to 0} \frac{f(x + t v) - f(x)}{t},  % = \frac{\partial}{\partial t}f(x+tv)\bigg|_{t=0}.
  \]  
  for which this limit is taken relative to the topology of $\cW$.
  The function $f$ is said to be \emph{Gâteaux differentiable} at $x\in\cU$ if $f$ has a directional derivative along all directions at $x$.
  We name the operator $\partial f(x):\cV\to\cW$ which assigns $v \mapsto \partial_v f(x) \in \cW$ the \emph{Gâteaux derivative} of $f$ at $x$, and the operator $\partial f:\cU\to(\cV;\cW) = \{A \,|\, A:\cV\to\cW \}$ which assigns $x \mapsto \partial f(x)$ simply the \emph{Gâteaux derivative} of $f$.
  
\end{definition}

\begin{remark}
  For Gâteaux derivatives, $\cV$ need only be a vector space, while $\cW$ a topological space.
  \citet[p. 55]{tapia1971diff} wrote that for quite some time analysis was simply done using the topology of the real line when dealing with functionals.
  As a result, important concepts such as convergence could not be adequately discussed.
%  For continuous linear functionals on $\bbR$ then this is fine.  
\end{remark}

\begin{remark}[\color{colblu}Tapia, 1971, p. 52]
  The space $(\cV;\cW)$ of operators from $\cV$ to $\cW$ is not a topological space, and there is no obvious way to define a topology on it.
  Consequently, we cannot consider the Gâteaux derivative of the Gâteaux derivative.
\end{remark}

Unlike the Fréchet derivative, which is by definition a linear operator, the Gâteaux derivative may fail to satisfy the additive condition of linearity\footnote{Although, for all scalars $\lambda \in \bbR$, the Gâteaux derivative is homogenous: $\partial_{\lambda v}f(x) = \lambda \partial_v f(x)$.}.
Even if it is linear, it may fail to depend continuously on some $v'\in\cV$ if $\cV$ and $\cW$ are infinite dimensional.
In this sense, Fréchet derivatives are more demanding than Gâteaux derivatives.
Nevertheless, the reasons we bring up Gâteaux derivatives is because it is usually simpler to calculate Gâteaux derivatives than Fréchet derivatives, and the two concepts are connected by the lemma below.

\begin{lemma}[Fréchet differentiability implies Gâteaux differentiability]
  If $f$ is Fréchet differentiable at $x\in\cU$, then $f:\cU\to\cW$ is Gâteaux differentiable at that point too, and $\d f(x) = \partial f(x)$.
\end{lemma}

\begin{proof}
  Since $f$ is Fréchet differentiable at $x\in\cU$, we can write $f(x+v) \approx f(x) + \d f(x)(v)$ for some $v\in\cV$.
  Then, 
  \begin{align}\label{eq:frecimplygat}
    \lim_{t\to 0}  & \left\Vert \frac{f(x + t v) - f(x)}{t} - \d f(x)(v) \right\Vert_\cW \\
    &\hspace{3cm}= \lim_{t\to 0} \frac{1}{t} \big\Vert f(x + t v) - f(x) - \d f(x)(tv)  \big\Vert_\cW \nonumber \\
    &\hspace{3cm}= \lim_{t\to 0} \frac{\big\Vert f(x + t v) - f(x) - \d f(x)(tv) \big\Vert_\cW }{\norm{tv}_\cV}\cdot \norm{v}_\cV  \nonumber
  \end{align}
  converges to 0 since $f$ is Fréchet differentiable at $x$, and  $t\to 0$ if and only if $\norm{tv}_\cV \to 0$.
  Thus, $f$ is Gâteaux differentiable at $x$, and the Gâteaux derivative $\partial_v f(x)$ of $f$ at $x$ in the direction $v$ coincides with the Fréchet derivatiave of $f$ at $x$ evaluated at $v$.
\end{proof}

On the other hand, Gâteaux differentiability does not necessarily imply Fréchet differentiability.
A sufficient condition for Fréchet differentiability is  that the Gâteaux derivative is continuous at the point of differentiation, i.e., the map $\partial f: \cU \to (\cV;\cW)$ is continuous at $x\in\cU$.
In other words, if $\partial f(x)$ is a bounded linear operator and the convergence in \eqref{eq:frecimplygat} is uniform with respect to all $v$ such that $\norm{v}_\cV=1$, then $\d f(x)$ exists and $\d f(x) = \partial f(x)$ \citep[p. 57 \& 66]{tapia1971diff}.

Consider now the function $\d f(x):\cV\to\cW$ and suppose that $f$ is twice Fréchet differentiable at $x\in\cU$, i.e. $\d f(x)$ is Fréchet differentiable at $x\in\cU$ with derivative $\d^2 f(x):\cV\times\cV \to \cW$.
Then, $\d f(x)$ is also Gâteaux differentiable at the point $x$ and the two differentials coincide.
In particular, we have
\begin{align}\label{eq:frech2gat}
  \left\Vert \frac{\d f(x + t v)(v') - \d f(x)(v')}{t} - \d^2 f(x)(v,v') \right\Vert_\cW \to 0 \text{ as } t \to 0,
\end{align}
by a similar argument in the proof above.
We will use this fact when we describe the Hessian in a little while.
%Again, if the convergence in \eqref{eq:frech2gat} is uniform with respect to all $v,v'\in\cV$ such that $\norm{v}_\cV = \norm{v'}_\cV = 1$, then $\d^2 f(x) $ exists and equals 

There is also the concept of \emph{gradients} in Hilbert space.
Recall that the Riesz representation theorem says that the mapping $A:\cV\to\cV'$ from the Hilbert space $\cV$ to its continuous dual space $\cV'$ defined by $A = \ip{\cdot,v}_\cV$ for some $v\in\cV$ is an isometric isomorphism.
Again, let $\cU \subseteq \cV$ be an open subset, and let $f:\cU\to\bbR$ be a (Fréchet) differentiable function with derivative $\d f: \cU \to \L(\cV,\bbR) \equiv \cV'$.
We define the gradient as follows.

\begin{definition}[Gradients in Hilbert space]
  The \emph{gradient} of $f$ is the operator $\nabla f: \cU \to \cV$ defined by $\nabla f = A^{-1} \circ \d f$.
  Thus, for $x \in \cU$, the gradient of $f$ at $x$, denoted $\nabla f(x)$, is the unique element of $\cV$ satisfying
  \[
    \ip{\nabla f(x), v}_\cV = \d f(x)(v)
  \]
  for any $v \in \cV$.
  Note that $\nabla f$ being a composition of two continuous functions, is itself continuous.
\end{definition}

\begin{remark}
  Alternatively, the gradient can be motivated using the Riesz representation theorem in Definition \ref{def:frechet}3 of the Fréchet derivative.
  Since $\cV'\ni T:\cV\to\bbR$, there is a unique element $v^*\in\cV$ such that $T(v)=\ip{v^*,v}_\cV$ for any $v\in\cV$.
  The element $v^*\in\cV$ is called the gradient of $f$ at $x$.
\end{remark}

Since the gradient of $f$ is an operator on $\cU$ to $\cV$, it may itself have a (Fréchet) derivative.
Assuming existence, i.e., $f$ is twice Fréchet differentiable at $x \in \cU$, we call this derivative the \emph{Hessian} of $f$.
From \eqref{eq:frech2gat}, it must be that
\begin{align*}
  \d^2 f(x)(v,v') &= \lim_{t\to 0} \frac{\d f(x + t v)(v') - \d f(x)(v')}{t} \\
  &= \lim_{t\to 0} \frac{\ip{\nabla f(x+tv), v'}_\cV - \ip{\nabla f(x), v'}_\cV}{t} \\
  &= \left\langle \lim_{t\to 0} \frac{\nabla f(x+tv) - \nabla f(x)}{t} , v' \right\rangle_\cV \\%\hspace{10pt} \rlap{\color{gray} \text{by linearity}} \\
  &= \left\langle \partial_v \nabla f(x) , v' \right\rangle_\cV.
\end{align*}
The second line follows from the definition of gradients, and the third line follows by linearity of inner products.
Note that since the Fréchet and Gâteaux differentials coincide, we have that $\partial_v \nabla f(x) = \d \nabla f(x) (v)$.
Letting $\cV$, $\cW$ and $\cU$ be as before, we now define the Hessian for the function $f:\cU \to \cW$.

\begin{definition}[Hessian]
  The Fréchet derivative of the gradient of $f$ is known as the \emph{Hessian} of $f$.
  Denoted $\nabla^2 f$, it is the mapping $\nabla^2 f: \cU \to \L(\cV,\cV)$ defined by $\nabla^2 f  = \d \nabla f$, and it satisfies
  \[
    \left\langle \nabla^2 f(x)(v) , v' \right\rangle_\cV = \d^2 f(x)(v,v').
  \]
  for $x\in\cU$ and $v,v'\in\cV$.
\end{definition}

\begin{remark}
  Since $\d^2 f(x)$ is a bilinear form in $\cV$, we can equivalently write
  \[
    \d^2 f(x)(v,v') = \ip{\d^2 f(x), v \otimes v'}_{\cV\otimes\cV}
  \]
  following the correspondence between bilinear forms and tensor product spaces.  
\end{remark}

With the differentiation tools above, we can now derive the Fisher information that we set out to derive at the beginning of this section.
Let $Y$ be a random variable with density in the parametric family $\{p(\cdot|\theta) \,|\, \theta \in \Theta \}$, where $\Theta$ is now assumed to be a Hilbert space with inner product $\ip{\cdot,\cdot}_\Theta$.
\hltodo[Why wouldn't it be >0 ?]{If $p(Y|\theta) > 0$}, the log-likelihood function of $\theta$ is the real-valued function $L(\cdot|Y):\Theta\to\bbR$ defined by $\theta \mapsto \log p(Y|\theta)$. 
The score $S$, assuming existence, is defined to be the (Fréchet) derivative of $L(\cdot|Y)$ at $\theta$, i.e. $S:\Theta \to \L(\Theta,\bbR) \equiv \Theta'$ defined by $S = \d L(\cdot | Y)$.
The second (Fréchet) derivative of $L(\cdot|Y)$ at $\theta$ is then $\d^2 L(\cdot|Y): \Theta \to \L(\Theta \times \Theta,\bbR)$.
We now prove the following proposition.

\begin{proposition}[Fisher information in Hilbert space]\label{thm:fisherinfohilbert}
  Assume that $p(Y|\cdot)$ and $\log p(Y|\cdot)$ are both Fréchet differentiable at $\theta$.
  Then, the Fisher information for $\theta\in\Theta$ is the element in the tensor product space $\Theta \otimes \Theta$ defined by
  \[
    \cI(\theta) = \E [\nabla L(\theta|Y) \otimes \nabla L(\theta|Y)].
  \]  
  Equivalently, assuming further that $\log p(Y|\cdot)$ is twice Fréchet differentiable at $\theta$, the Fisher information can be written as
  \[
    \cI(\theta) = \E[-\nabla^2 L(\theta|Y)].
  \]
  Note that both expectations are taken under the true distribution of random variable $Y$.
\end{proposition}

\begin{proof}
  


The Gâteaux derivative of $L(\cdot|Y) = \log p(Y|\cdot)$ at $\theta\in\Theta$ in the direction $b\in\Theta$, which is also its Fréchet derivative, is
\begin{align*}
  \partial_b L(\theta|Y) 
  &= \frac{\d}{\d t} \log p(Y|\theta + tb) \Bigg|_{t=0} \\
  &= \frac{\frac{\d}{\d t}p(Y|\theta + tb)\big|_{t=0}}{p(Y|\theta)} \\
  &= \frac{\partial_b p(Y|\theta)}{p(Y|\theta)}.
\end{align*}
Since it assumed that $p(Y|\cdot)$ is Fréchet differentiable at $\theta$, $\d p(Y|\theta)(b) = \partial_b p(Y|\theta)$.
The expectation of the score for any $b\in\Theta$ is shown to be
\begin{align*}
  \E[\d L(\theta|Y)(b)] 
  &= \E \left[ \frac{\d p(Y|\theta)(b)}{p(Y|\theta)} \right] \\
  &= \int \frac{\d p(Y|\theta)(b)}{\cancel{p(Y|\theta)}} \cancel{p(Y|\theta)} \dint Y \\
  &= \left(\d\int p(Y|\cdot) \dint Y \right)(\theta)(b) \\
  &= \left\langle \left(\nabla \int p(Y|\cdot) \dint Y\right)(\theta)  , b \right\rangle_\Theta \\
  &=0.
\end{align*}
The interchange of the integration and the Fréchet differential is allowed under certain conditions \citep{kammar2016}.
The derivative of $\int p(Y|\cdot)\dint Y$ at any value of $\theta\in\Theta$ is the zero vector as it is the derivative of a constant (i.e., 1).

Using the classical notion that the Fisher information is the variance of the score function, then, for fixed $b,b'\in\Theta$, combined with the fact that $\E[\d L(\theta|Y)]$ is a zero mean function, we have that 
\begin{align*}
  \cI(\theta)(b,b') 
  &= \E [ \d L(\theta|Y)(b) \cdot \d L(\theta|Y)(b') ] \\
  &= \E \big[ \left\langle \nabla L(\theta|Y) , b \right\rangle_\Theta \left\langle \nabla L(\theta|Y) , b' \right\rangle_\Theta \big] \\
  &= \left\langle \E [\nabla L(\theta|Y) \otimes \nabla L(\theta|Y)] , b \otimes b' \right\rangle_{\Theta\otimes\Theta}.
\end{align*}
Hence, $\cI(\theta)$ as a bilinear form corresponds to the element $\E [\nabla L(\theta|Y) \otimes \nabla L(\theta|Y)] \in \Theta\otimes\Theta$.

The Gâteaux derivative of the Fréchet differential is the second Fréchet derivative, since $L(\cdot|Y)$ is assumed to be twice differentiable at $\theta\in\Theta$:
\begin{align*}
  \d^2 L(\theta|Y)(b,b') 
  &= \partial_{b'} \d L(\theta|Y)(b) \\
  &= \partial_{b'} \left( \frac{\d p(Y|\theta)(b)}{p(Y|\theta)} \right) \\
  &= \frac{\d}{\d t} \left( \frac{\d p(Y|\theta + tb')(b)}{p(Y|\theta + tb')} \right) \Bigg|_{t=0} \\
  &= \frac{p(Y|\theta)\d^2 p(Y|\theta)(b,b') - \d p(Y|\theta)(b)\d p(Y|\theta)(b') }{p(Y|\theta)^2} \\
  &= \frac{\d^2 p(Y|\theta)(b,b')  }{p(Y|\theta)} 
  - \d L(\theta|Y)(b) \d L(\theta|Y)(b').
\end{align*}
Taking expectations of the first term in the right-hand side, we get that
\begin{align*}
  \E\left[ \frac{\d^2 p(Y|\theta)(b,b')}{p(Y|\theta)} \right] 
%  &= \frac{1}{p(Y|\theta)} \E \left\langle \big(\d \nabla p(Y|\theta)\big)(b) , b' \right\rangle_\Theta \\
%  &= \left\langle \int \big(\d \nabla p(Y|\theta)\big)(b)  \dint Y , b' \right\rangle_\Theta \\
%  &= \left\langle \d \left( \int \nabla p(Y|\theta) \dint Y \right)(b)   , b' \right\rangle_\Theta \\
  &= \int \frac{\d \big( \d p(Y|\theta) \big)(b,b')}{\cancel{p(Y|\theta)}} \cancel{p(Y|\theta)} \dint Y   \\
  &= \left(\d^2 \int p(Y|\cdot) \dint Y \right)(\theta)(b,b') \\
  &= \left\langle \left(\nabla^2 \int p(Y|\cdot) \dint Y\right)(\theta)(b)  , b' \right\rangle_\Theta \\
  &= 0.
\end{align*}
Thus, we see that from the first result obtained, 
\begin{align*}
  \E [-\d^2 L(\theta|Y)(b,b') ]
  &= \E[\d L(\theta|Y)(b) \d L(\theta|Y)(b')] \\
  &= \cI(\theta)(b,b'),
\end{align*}
while
\begin{align*}
  \E [-\d^2 L(\theta|Y)(b,b') ]
  &= -\E\ip{\nabla^2 L(\theta|Y)(b), b'}_\Theta \\
  &= \ip{-\E \nabla^2 L(\theta|Y)(b), b'}_\Theta.
\end{align*}
It would seem that $\E[- \nabla^2 L(\theta|Y)(b)]$ is an operator from $\Theta$ onto itself which also induces a bilinear form equivalent to $\E [-\d^2 L(\theta|Y)]$.
Therefore, $\cI(\theta) = \E[- \nabla^2 L(\theta|Y)]$.
\end{proof}

There are three equivalent interpretations of the Fisher information $\cI(\theta)$ for $\theta$, much like the covariance operator, which are 
\begin{enumerate}
  \item As its general form, i.e. an element in $\Theta\otimes\Theta$;
  \item As an operator $\cI(\theta):\Theta\to\Theta$ defined by $\cI(\theta)(b) = \E[- \nabla^2 L(\theta|Y)](b)$; and finally
  \item As a bilinear form $\cI(\theta):\Theta\times\Theta\to\bbR$ defined by $\cI(\theta)(b,b') = \ip{-\E \nabla^2 L(\theta|Y)(b), b'}_\Theta$ $= \E [-\d^2 L(\theta|Y)(b,b') ]$.
\end{enumerate}
In particular, viewed as a bilinear form, the evaluation of the Fisher information for $\theta$ at two points $b$ and $b'$ in $\Theta$ is seen as the Fisher information 
\hltodo[Is it really between two things?]{between} 
two continuous, linear functionals of $\theta$.
\hltodo{REASONS? linear isometry?}
%This is because the continuity of the linear differential operator ensures that the Fisher information is an element of the continuous dual $\Theta'$.
For brevity, we denote this $\cI(\theta_b,\theta_{b'})$.
The natural isometry between $\Theta$ and $\Theta'$ then allows us to write 
\begin{align}\label{eq:fisher-linear-functional}
  \cI(\theta_b,\theta_{b'}) = \ip{\cI(\theta), b \otimes b'}_{\Theta\otimes\Theta} = \big\langle\cI(\theta), \ip{\cdot,b}_\Theta \otimes \ip{\cdot,b'}_\Theta \big\rangle_{\Theta'\otimes\Theta'}.
\end{align}


%\begin{example}[Fisher information in Euclidean space]
%  In this example, we will derive the Fisher information for the parameters of a normal distribution using the formulae in Proposition \ref{thm:fisherinfohilbert} and the differentiation techniques in Hilbert space.
%  Let $Y \sim \N(\mu,\psi^{-1})$. 
%  Using the exponential family natural parameterisation, denote by $\theta = (\mu\psi,-\psi/2)^\top \in \Theta \subset \bbR^2$, and this space is equipped with the usual dot product inner product.
%  Also define the sufficient statistic $T(Y) = (Y, Y^2)^\top$
%  The log-likelihood function for $\theta$ is $L(\theta) = \const + \ip{\theta,T(Y)}$
%\end{example}


%\subsection{Tensor product spaces}
%\hltodo{Move this to Chapter 2}
%
%\begin{definition}[Tensor products]
%  Let $x_1\in\cH_1$ and $x_2\in\cH_2$ be two elements of two real Hilbert spaces.
%  Then, the tensor product $x_1 \otimes x_2:\cH_1\times\cH_2\to\bbR$, is a bilinear form defined as
%  \[
%    (x_1\otimes x_2)(y_1,y_2) = \ip{x_1,y_1}_{\cH_1}\ip{x_2,y_2}_{\cH_2}
%  \]
%  for any $(y_1,y_2)\in\cH_1\times\cH_2$.
%\end{definition}
%
%\begin{definition}[Tensor product space]
%  The tensor product space $\cH_1\otimes\cH_2$ is the completion of the space
%  \[
%    \cA = \left\{ \sum_{j=1}^J x_{1j} \otimes x_{2j} \,\Bigg\vert\, x_{1j}\in\cH_1, x_{2j}\in\cH_2, J \in \bbN \right\}
%  \]
%  with respect to the norm induced by the inner product
%  \[
%    \left\langle  \sum_{j=1}^J x_{1j} \otimes x_{2j},  \sum_{k=1}^K y_{1k} \otimes y_{2k} \right\rangle_\cA = \sum_{j=1}^J\sum_{k=1}^K \ip{x_{1j},y_{1k}}_{\cH_1} \ip{x_{2j},y_{2k}}_{\cH_2}.
%  \]
%\end{definition}
%
%An operator interpretation of the tensor product.
%For each pair of elements $(x_1,x_2) \in \cH_1\times\cH_2$, we define the operator $A:\cH_1\to\cH_2$ in the following way:
%\begin{align*}
%  A_{x_1,x_2}:\cH_1&\to\cH_2 \\
%  y_1&\mapsto \ip{x_1,y_1}_{\cH_1}x_2
%\end{align*}
%For some $y_1\in\cH_1$ and $y_2\in\cH_2$, we have that
%\begin{align*}
%  \ip{A_{x_1,x_2}(y_1),y_2}_{\cH_2} 
%  &= \big\langle \ip{x_1,y_1}_{\cH_1}x_2 , y_2 \big\rangle_{\cH_2} \\
%  &= \ip{x_1,y_1}_{\cH_1}\ip{x_2,y_2}_{\cH_2} \\
%  &= (x_1\otimes x_2)(y_1,y_2).
%\end{align*}
%It is seen that the tensor product $x_1\otimes x_2$ is is associated with the rank one operator $B:\cH_1'\to\cH_2$ defined by $z \mapsto z(x_1)x_2$ with $z = \ip{x_1,\cdot}_{\cH_1}$.
%We write $B = x_1 \otimes x_2$.
%\hltodo[From Wikipedia. But don't really get it, although it might explain the Fisher information between linear functionals.]{Therefore, this extends a linear identification between $\cH_1\otimes\cH_2$ and the space of finite-rank operators from $\cH_1'$ to $\cH_2$.}
%We now have three distinct interpretations of the tensor product:
%\begin{itemize}
%  \item \textbf{Bilinear form} (as defined in Definition 3.5). 
%  \begin{align*}
%    x_1 \otimes x_2:\cH_1 \times \cH_2 &\to \bbR \\
%    (y_1,y_2) &\mapsto \ip{x_1,y_1}_{\cH_1}\ip{x_2,y_2}_{\cH_2}
%  \end{align*}
%  for $x_1,y_1\in\cH_1$ and $x_2,y_2\in\cH_2$.
%  \item \textbf{Operator}.
%  \begin{align*}
%    x_1 \otimes x_2:\cH_1 &\to \cH_2 \\
%    y_1 &\mapsto \ip{x_1,y_1}_{\cH_1}x_2
%  \end{align*}  
%  \item \textbf{General form} (as an element in the tensor space).
%  \[
%    x_1 \otimes x_2 \in \cH_1 \otimes \cH_2.
%  \]
%\end{itemize}
%
%\subsection{Random elements in a Hilbert space}
%
%\hltodo{Move this to Chapter 2}
%
%Let $\cH$ be a real Hilbert space. 
%We can define a metric on $\cH$ using $D(x,x') = \norm{x-x'}_\cH$, where the norm on $\cH$ is the norm induced by the inner product.
%A collection $\Sigma$ of subsets of $\cH$ is called a \emph{$\sigma$-algebra} if $\emptyset \in \Sigma$, $S \in \Sigma$ implies its complement $S^c \in \Sigma$, and $S_j\in\Sigma$, $j\geq 1$ implies $\bigcup_{j=1}^\infty S_j \in \Sigma$.
%The smallest $\sigma$-algebra containing all open subsets of $\cH$ is called the \emph{Borel $\sigma$-algebra}, and its members the Borel sets.
%Denote by $\cB(\cH)$ the Borel $\sigma$-algebra of $\cH$.
%The metric space $(\cH,D)$ is called \emph{separable} if it has a countable dense subset, i.e., there are $x_1,x_2,\cdots$ in $\cH$ such that the closure $\overline{\{x,_1,x_2,\cdots\}} = \cH$.
%
%Recall that a function $\nu:\Sigma\to[0,\infty]$ is called a \emph{measure} if it satisfies
%\begin{itemize}
%  \item \textbf{Non-negativity:} $\nu(S) \geq 0$ for all $S$ in $\Sigma$;
%  \item \textbf{Null empty set:} $\nu(\emptyset) = 0$; and
%  \item \textbf{$\sigma$-additivity:} for all countable, mutually disjoint sets $\{S_i\}_{i=1}^\infty$,
%  \[
%    \nu\left(\bigcup_{i=1}^\infty S_i \right) = \sum_{i=1}^\infty \nu(S_i).
%  \] 
%\end{itemize}
%A measure $\nu$ on $\big(\cH,\cB(\cH)\big)$ is called a \emph{Borel measure} on $\cH$.
%We shall only concern ourselves with finite Borel measures. 
%In addition, if $\nu(\cH) = 1$ then $\nu$ is a \emph{(Borel) probability measure} and the measure space $\big(\cH,\cB(\cH),\nu\big)$ is a \emph{(Borel) probability space}.
%
%Let $(\Omega,\cE,\Prob)$ be a probability space.
%We say that a mapping $X:\Omega\to\cH$ is a \emph{random element} in $\cH$ if $X^{-1}(B)\in\cE$ for every Borel set, i.e., $X$ is a function such that for every $B\in\cB(\cH)$, its preimage $X^{-1}(B) = \{\omega \in \Omega \,|\, X(\omega) \in B \}$ lies in $\Sigma$.
%This is simply a generalisation of the definition of random variables in regular Euclidean space.
%From this definition, we can also properly define random functions $f$ in a Hilbert space of functions $\cF$.
%In any case, every random element $X$ induces a probability measure on $\cH$ defined by
%\[
%  \nu(B) = \Prob\big(X^{-1}(B)\big) = \Prob\big( \omega \in \Omega | X(\omega) \in B  \big) = \Prob(X \in B).
%\]
%The measure $\nu$ is called the \emph{distribution} of $X$.
%The \emph{density} $p$ of $X$ is a measurable function with the property that
%\[
%  \Prob(X \in B) = \int_{X^{-1}(B)} \omega \dint\Prob(\omega) = \int_B p(x) \dint\nu(x).
%\]
%
%\begin{definition}[Mean vector]
%  Let $\nu$ be a Borel probability measure on a real Hilbert space $\cH$.
%  Supposing that a random element $X$ of $\cH$ is \emph{integrable}, that is to say
%  \[
%    \E \norm{X}_\cH = \int_\cH \norm{x}_\cH \dint\nu(x) < \infty,
%  \]
%  then the unique element $\mu\in\cH$ satisfying 
%  \[
%    \ip{\mu,x'} = \int_\cX \ip{x,x'}_\cX \dint\nu(x) = \E\ip{X,x'}_\cH
%  \]
%  for all $x' \in \cH$ is called the \emph{mean vector}. 
%\end{definition}
%
%\begin{definition}[Covariance operator]
%  Let $\nu$ be a Borel probability measure on a real Hilbert space $\cH$.
%  Suppose that a random element $X$ of $\cH$ is \emph{square integrable}, i.e., $\E \norm{X}_\cH^2 < \infty$, and let $\mu$ be the mean vector of $X$.
%  Then the \emph{covariance operator} $C$ is defined by the mapping
%  \begin{align*}
%    C:\cH &\to \cH \\
%    x &\mapsto \E\big[\ip{X - \mu, x}_{\cH}(X - \mu) \big].
%  \end{align*}
%  The covariance operator $C$ is also an element of $\cH\otimes\cH$ that satisfies
%  \begin{align*}
%    \ip{C,x\otimes x'}_{\cH\otimes\cH} 
%    &= \int_\cH \ip{z-\mu, x}_\cH\ip{z-\mu,x'}_\cH \dint\nu(z) \\
%    &= \E\big[\ip{X-\mu,x}_\cH \ip{X-\mu,x'}_\cH\big]
%  \end{align*}
%  for all $x,x'\in\cH$.
%\end{definition}
%
%From the definition of the covariance operator, we see that it induces a symmetric, bilinear form, which we shall denote by $\Cov:\cH\times\cH\to\bbR$, through
%\begin{align*}
%  \ip{Cx,x'}_\cH 
%  &= \big\langle\E\big[\ip{X - \mu, x}_{\cH}(X - \mu) \big], x' \big\rangle_\cH \\
%  &= \E\big[\ip{X-\mu,x}_\cH \ip{X-\mu,x'}_\cH\big] \\
%  &=: \Cov(x,x').
%\end{align*}
%
%\begin{definition}[Gaussian vectors]
%  A random element $X$ is called \emph{Gaussian} if $\ip{X,x}_\cH$ has a normal distribution for all fixed $x\in\cH$.
%  A Gaussian vector $X$ is characterised by its mean element $\mu\in\cH$ and its covariance $C\in\cH\otimes\cH$.
%\end{definition}
%
