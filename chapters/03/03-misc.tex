\subsection{Total Fisher information}

For many applications, it is of interest to evaluate the (total) Fisher information at the maximum likelihood estimate under a sampling scenario.
However, the expectation required to calculate the Fisher information above cannot be done without knowing the true value of $\theta$.
As a point of clarification, we ought to make the distinction between the \emph{expected} Fisher information and the \emph{observed} Fisher information under a sampling scenario.
There are two quantities that are typically used as an approximation, and these are explained below.
Let $y = \{y_1,\dots,y_n\}$ represent an independent and identically distributed observed sample from $p(\cdot|\theta)$.
The maximum likelihood (ML) estimator $\hat\theta = \argmax_\theta L(\theta)$ for $\theta$ satisfies the first order conditions $S(\hat\theta) = 0$, where the log-likelihood function and the score function makes use of all of the observed samples, i.e. $L(\theta) = \sum_{i=1}^n \log p(y_i|\theta)$.
In a sampling experiment, the total Fisher information (denoted $\cI_n(\theta)$) is just $n$ times the unit Fisher information, i.e. $\cI_n(\theta) = n\cI(\theta)$.
Following \citet{efron1978assessing}, the expected Fisher information is defined to be $\cI_n(\hat\theta)$, while the observed Fisher information is
\[
  \hat\cI_n = -\sum_{i=1}^n \frac{\partial ^ 2}{\partial\theta^2} \log p(y_i|\theta) \bigg|_{\theta = \hat\theta} \ .
\]
which is also by definition the negative Hessian. 
Note that 
\[
  \cJ(\theta) = -\frac{1}{n} \sum_{i=1}^n \frac{\partial ^ 2}{\partial\theta^2} \log p(y_i|\theta) 
\]
$\frac{1}{n}\hat\cI_n \to \cI(\hat\theta)$ in probability as $n\to\infty$ by the weak law of large numbers.
Both of these quantities are used as replacements of the actual Fisher information about the ``true'' parameter.
In the context of measuring curvatures, the expected Fisher information would be used \citep{pawitan2001all}, but in the context of efficient variance for ML estimates, the observed Fisher information is favoured \citep{efron1978assessing}.
which by the law of large numbers, converges in probability to the expected Fisher information $\cI(\theta)$ as defined above.
In practice, one would not be able to calculate $\cI$ without knowing the true value for $\theta$, so replacing occurrences of $\theta$ with  (the MLE)

In particular, near the MLE, low Fisher information indicates a shallow maxima, while high observed information indicates a ``sharp'' maxima.
A shallow maxima is an indication that many nearby values have similar log-likelihood, but a sharp maxima is indicative of a high confidence surrounding the MLE.

We used the true Fisher information. \citet{efron1978assessing} say favour the observed information instead. Does this change if we use MLE $\hat f$ instead? Probably not... we don't use MLE anyway!

\url{https://stats.stackexchange.com/questions/179130/gaussian-process-proofs-and-results}

\url{https://stats.stackexchange.com/questions/268429/do-gaussian-process-regression-have-the-universal-approximation-property}

\subsection{Functional derivatives}

\begin{definition}[Directional derivative and gradient]
  Let ($\cH$, $\ip{\cdot,\cdot}_\cH$) be an inner product space, and consider a function $g:\mathcal H \rightarrow \mathbb R$. 
  Denote the directional derivate of $g$ in the direction $z$ by $\nabla_z g$, that is, 
	\[
		\nabla_z g(x) = \lim_{\delta \rightarrow 0} \frac{g(x + \delta z) - g(x)}{\delta}.
	\]
	The gradient of $g$, denoted by $\nabla g$, is the unique vector field satisfying 
	\[
		\langle \nabla g(x), z \rangle_{\mathcal H} = \nabla_z g(x), \ \ \ \forall x,z \in \mathcal H.
	\]
\end{definition}

\begin{definition}[Functional derivative]
  Given a manifold $M$ representing continuous/smooth functions $\rho$ with certain boundary conditions, and a functional $F:M\to\bbR$, the functional derivative of $F[\rho]$ with respect to $\rho$, denoted $\partial F/\partial\rho$, is defined by
  \begin{align*}
    \int \frac{\partial F}{\partial\rho}(x)\phi(x)\d x
    &= \lim_{\epsilon\to 0} \frac{F[\rho + \epsilon\phi] - F[\rho]}{\epsilon} \\
    &= \left[ \frac{\d}{\d \epsilon} F[\rho + \epsilon\phi] \right]_{\epsilon=0},
  \end{align*}
  where $\phi$ is an arbitrary function.
  The function $\partial F/\partial\rho$ as the gradient of $F$ at the point $\rho$, and
  \[
    \partial F(\rho,\phi) = \int \frac{\partial F}{\partial\rho}(x)\phi(x) \d x
  \]
  as the directional derivative at point $\rho$ in the direction of $\phi$.
  Analogous to vector calculus, the inner product with the gradient gives the directional derivative.
\end{definition}

\begin{example}[Functional derivative of entropy]
  Let $X$ be a discrete random variable with probability mass function $p(x) \geq 0$, for $\forall x \in \Omega$, a finite set.
  The entropy is a functional of $p$, namely
  \[
    \cE[p] = - \sum_{x\in\Omega} p(x)\log p(x).
  \]
  Equivalently, using the counting measure $\nu$ on $\Omega$, we can write
  \[
    \cE[p] = -\int_\Omega p(x) \log p(x) \d\nu(x).
  \]
  \begin{align*}
    \int_\Omega \frac{\partial\cE}{\partial p}(x)\phi(x) \dint x
    &= \left[ \frac{\d}{\d \epsilon} \cE[p +  \epsilon\phi] \right]_{\epsilon=0} \\
    &= \left[ -\frac{\d}{\d \epsilon} 
    \big( p(x) + \epsilon\phi(x) \big) 
    \log \big(p(x) + \epsilon\phi(x) \big) 
    \right]_{\epsilon=0} \\
    &= -\int_\Omega \left( 
    \frac{p(x)\phi(x)}{p(x)+\epsilon\phi(x)}
    + \frac{\epsilon\phi(x)}{p(x) + \epsilon\phi(x)}
    + \phi(x)\log\big( p(x) + \epsilon\phi(x) \big)
    \right) \d x \\
    &= -\int_\Omega \left( 1 + \log p(x) \right) \phi(x) \dint x.
  \end{align*}
  Thus, $(\partial\cE/\partial p)(x) = -1 -\log p(x)$.
\end{example}

\subsection{Data dependent priors}

Here we consider data dependent priors---seemingly data dependent (i.e. dependent on X) but the whole model is conditional on $X$ implicitly, so there is no issue.
If prior depended on $y$ then there is a problem, at least, violates Bayesian first principles (using the data twice such that a priori and a posteriori same amount of information).

