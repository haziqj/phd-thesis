\subsection{Component-wise derivative in infinite-dimensional vector spaces}
\label{misc:comwiseder}

Let $\cX$ be a topological vector space, and $g:\cX\to\bbR$ a real-valued function over $\cX$.
Also suppose that $\{e_1,e_2,\dots\}$ is an (algebraic) orthonormal basis for $\cX$, such that any $x\in\cX$ can be represented as $x = x_1e_1 + x_2e_2 + \cdots = (x_1,x_2,\dots)$.
We define the partial derivative of $g$ with respect to the $k$'th component of $x$ as
\[
  \frac{\partial g(x)}{\partial x_k}  = \lim_{t\to 0} \frac{g(x_1,\dots,x_k+t,\dots) - g(x_1,x_2,\dots)}{t}.
\]
Incidentally, this definition also coincides with the GÃ¢teaux derivative of $g$ at $x$ in the direction $e_k$, i.e., $\frac{\partial g(x)}{\partial x_k} = \partial_{e_k}g(x)$.
Denote by 
\[
  \nabla g(x) = \left(
  \frac{\partial g(x)}{\partial x_1}, 
  \frac{\partial g(x)}{\partial x_2},
  \dots
  \right)
\]
the total differential, or as it is more commonly known, the gradient of $g$ at $x$.
This is, in essence, a component-wise derivative of $g$ using the usual definition of derivatives.
The issue here is that the existence of the limits for each component of the differential does not guarantee existence of the gradient.
This is because component-wise convergence does not guarantee convergence with respect to the actual topology of the  space.

\begin{example}[Existence of partial derivatives does not imply differentiability]
Define $g:\bbR^2\to\bbR$ by
\[
  g(x) = 
  \begin{cases}
    \displaystyle\frac{x_1x_2}{x_1^2 + x_2^2} &\text{if } (x_1,x_2) \neq (0,0) \\
    0 &\text{if } x_1=x_2=0.
  \end{cases}
\]
We note that $\lim_{x\to 0} g(x) = \half \neq 0 = g(0,0)$ along the line $x_1 = x_2$.
Therefore, this function is discontinuous, and hence non-differentiable, at zero.
On the other hand, the partial derivatives of $g$ are 
\[
  \frac{\partial g(x)}{\partial x_1} = \frac{x_2(x_2^2-x_1^2)}{(x_1^2+x_2^2)^2}
  \hspace{0.5cm}\text{and}\hspace{0.5cm}
  \frac{\partial g(x)}{\partial x_2} = \frac{x_1(x_1^2-x_2^2)}{(x_1^2+x_2^2)^2},
\]
and thus along the $x_1$-axis, $\frac{\partial g(x_1,0)}{\partial x_1} = 0$, and similarly along the $x_2$-axis, $\frac{\partial g(0,x_2)}{\partial x_2} = 0$.
Existence of partial derivatives is not sufficient in this case for the existence of the gradient.

\end{example}


%\subsection{Expected versus observed Fisher information}
%
%For many applications, it is of interest to evaluate the (total) Fisher information at the maximum likelihood estimate under a sampling scenario.
%However, the expectation required to calculate the Fisher information above cannot be done without knowing the true value of $\theta$.
%As a point of clarification, we ought to make the distinction between the \emph{expected} Fisher information and the \emph{observed} Fisher information under a sampling scenario.
%There are two quantities that are typically used as an approximation, and these are explained below.
%Let $y = \{y_1,\dots,y_n\}$ represent an independent and identically distributed observed sample from $p(\cdot|\theta)$.
%The maximum likelihood (ML) estimator $\hat\theta = \argmax_\theta L(\theta)$ for $\theta$ satisfies the first order conditions $S(\hat\theta) = 0$, where the log-likelihood function and the score function makes use of all of the observed samples, i.e. $L(\theta) = \sum_{i=1}^n \log p(y_i|\theta)$.
%In a sampling experiment, the total Fisher information (denoted $\cI_n(\theta)$) is just $n$ times the unit Fisher information, i.e. $\cI_n(\theta) = n\cI(\theta)$.
%Following \citet{efron1978assessing}, the expected Fisher information is defined to be $\cI_n(\hat\theta)$, while the observed Fisher information is
%\[
%  \hat\cI_n = -\sum_{i=1}^n \frac{\partial ^ 2}{\partial\theta^2} \log p(y_i|\theta) \bigg|_{\theta = \hat\theta} \ .
%\]
%which is also by definition the negative Hessian. 
%Note that 
%\[
%  \cJ(\theta) = -\frac{1}{n} \sum_{i=1}^n \frac{\partial ^ 2}{\partial\theta^2} \log p(y_i|\theta) 
%\]
%$\frac{1}{n}\hat\cI_n \to \cI(\hat\theta)$ in probability as $n\to\infty$ by the weak law of large numbers.
%Both of these quantities are used as replacements of the actual Fisher information about the ``true'' parameter.
%In the context of measuring curvatures, the expected Fisher information would be used \citep{pawitan2001all}, but in the context of efficient variance for ML estimates, the observed Fisher information is favoured \citep{efron1978assessing}.
%which by the law of large numbers, converges in probability to the expected Fisher information $\cI(\theta)$ as defined above.
%In practice, one would not be able to calculate $\cI$ without knowing the true value for $\theta$, so replacing occurrences of $\theta$ with  (the MLE)
%
%In particular, near the MLE, low Fisher information indicates a shallow maxima, while high observed information indicates a ``sharp'' maxima.
%A shallow maxima is an indication that many nearby values have similar log-likelihood, but a sharp maxima is indicative of a high confidence surrounding the MLE.
%
%We used the true Fisher information. \citet{efron1978assessing} say favour the observed information instead. Does this change if we use MLE $\hat f$ instead? Probably not... we don't use MLE anyway!
%
%\url{https://stats.stackexchange.com/questions/179130/gaussian-process-proofs-and-results}
%
%\url{https://stats.stackexchange.com/questions/268429/do-gaussian-process-regression-have-the-universal-approximation-property}
%
\subsection{Functional derivatives}

Typically, the problem of finding the maxima of $H(p)$ in \cref{eq:entrop} is tackled using calculus of variations.
The following is an alternative route to obtain the functional derivative of the entropy.

\begin{definition}[Directional derivative and gradient]
  Let ($\cH$, $\ip{\cdot,\cdot}_\cH$) be an inner product space, and consider a function $g:\mathcal H \rightarrow \mathbb R$. 
  Denote the directional derivate of $g$ in the direction $z$ by $\nabla_z g$, that is, 
	\[
		\nabla_z g(x) = \lim_{\delta \rightarrow 0} \frac{g(x + \delta z) - g(x)}{\delta}.
	\]
	The gradient of $g$, denoted by $\nabla g$, is the unique vector field satisfying 
	\[
		\langle \nabla g(x), z \rangle_{\mathcal H} = \nabla_z g(x), \ \ \ \forall x,z \in \mathcal H.
	\]
\end{definition}

\begin{definition}[Functional derivative]
  Given a manifold $M$ representing continuous/smooth functions $\rho$ with certain boundary conditions, and a functional $F:M\to\bbR$, the functional derivative of $F[\rho]$ with respect to $\rho$, denoted $\partial F/\partial\rho$, is defined by
  \begin{align*}
    \int \frac{\partial F}{\partial\rho}(x)\phi(x)\d x
    &= \lim_{\epsilon\to 0} \frac{F[\rho + \epsilon\phi] - F[\rho]}{\epsilon} \\
    &= \left[ \frac{\d}{\d \epsilon} F[\rho + \epsilon\phi] \right]_{\epsilon=0},
  \end{align*}
  where $\phi$ is an arbitrary function.
  The function $\partial F/\partial\rho$ as the gradient of $F$ at the point $\rho$, and
  \[
    \partial F(\rho,\phi) = \int \frac{\partial F}{\partial\rho}(x)\phi(x) \d x
  \]
  as the directional derivative at point $\rho$ in the direction of $\phi$.
  Analogous to vector calculus, the inner product with the gradient gives the directional derivative.
\end{definition}

\begin{example}[Functional derivative of entropy]
  Let $X$ be a discrete random variable with probability mass function $p(x) \geq 0$, for $\forall x \in \Omega$, a finite set.
  The entropy is a functional of $p$, namely
  \[
    \cE[p] = - \sum_{x\in\Omega} p(x)\log p(x).
  \]
  Equivalently, using the counting measure $\nu$ on $\Omega$, we can write
  \[
    \cE[p] = -\int_\Omega p(x) \log p(x) \d\nu(x).
  \]
  \begin{align*}
    \int_\Omega \frac{\partial\cE}{\partial p}(x)\phi(x) \dint x
    &= \left[ \frac{\d}{\d \epsilon} \cE[p +  \epsilon\phi] \right]_{\epsilon=0} \\
    &= \left[ -\frac{\d}{\d \epsilon} 
    \big( p(x) + \epsilon\phi(x) \big) 
    \log \big(p(x) + \epsilon\phi(x) \big) 
    \right]_{\epsilon=0} \\
    &= -\int_\Omega \left( 
    \frac{p(x)\phi(x)}{p(x)+\epsilon\phi(x)}
    + \frac{\epsilon\phi(x)}{p(x) + \epsilon\phi(x)}
    + \phi(x)\log\big( p(x) + \epsilon\phi(x) \big)
    \right) \d x \\
    &= -\int_\Omega \left( 1 + \log p(x) \right) \phi(x) \dint x.
  \end{align*}
  Thus, $(\partial\cE/\partial p)(x) = -1 -\log p(x)$.
\end{example}

\subsection{Data dependent priors}

The I-prior for the regression model \cref{eq:model1} subject to \cref{eq:model1ass} is seemingly data dependent, which violates Bayesian first principles.
That is, an I-prior for $f$ as per \cref{eq:iprior2} makes use of the same data $\bx:=\{x_1,\dots,x_n\}$ in the covariance matrix for $f$ that appears in the model.
However, the whole model is implicitly conditional on $\bx$.
If the prior depended instead on the responses $\by$, then the state of knowledge a priori and a posteriori is exactly the same, and this violates Bayesian principles.

