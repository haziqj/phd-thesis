
We extend the idea beyond thinking about parameters as merely numbers in the usual sense, to abstract objects in Hilbert spaces. 
This generalisation allows us to extend the concept of Fisher information to regression functions in RKHSs later.
The score and Fisher information is derived in a familiar manner, but extra care is required when taking derivatives with respect to Hilbert space objects.  

Let $Y$ be a random variable with density in the parametric family $\{p(\cdot|\theta) \,|\, \theta \in \Theta \}$, where $\Theta$ is now assumed to be a Hilbert space with inner product $\ip{\cdot,\cdot}_\Theta$.
If $p(Y|\theta) > 0$\hltodo[Why wouldn't it be >0 ?]{}, the log-likelihood function of $\theta$ is the real-valued function $L(\cdot|Y):\Theta\to\bbR$ defined by $\theta \mapsto \log p(Y|\theta)$. 
To discuss derivatives of the log-likelihood function for $\theta\in\Theta$, we require a generalisation of the concept of differentiability from real-valued functions of a single, real variable, as is common in calculus, to functions between Banach spaces.

%\begin{definition}[Directional derivative and gradient]
%  Let ($\cH$, $\ip{\cdot,\cdot}_\cH$) be an inner product space, and consider a function $g:\mathcal H \rightarrow \mathbb R$. 
%  Denote the directional derivate of $g$ in the direction $z$ by $\nabla_z g$, that is, 
%	\[
%		\nabla_z g(x) = \lim_{\delta \rightarrow 0} \frac{g(x + \delta z) - g(x)}{\delta}.
%	\]
%	The gradient of $g$, denoted by $\nabla g$, is the unique vector field satisfying 
%	\[
%		\langle \nabla g(x), z \rangle_{\mathcal H} = \nabla_z g(x), \ \ \ \forall x,z \in \mathcal H.
%	\]
%\end{definition}

\begin{definition}[Fréchet derivative]
  Let $\cV$ and $\cW$ be two normed spaces, and $\cU \subseteq \cV$ be an open subset.
  A function $f:\cU\to\cW$ is called \emph{Fréchet differentiable} at $x \in \cU$ if there exists a bounded, linear operator $T:\cV\to\cW$ such that 
  \[
    \lim_{v\to 0} \frac{\big\Vert f(x+v) - f(x) - T v \big\Vert_{\cW}}{\norm{v}_\cV} = 0
  \]
  If this relation holds, then the operator $T$ is unique, and we write $\d f(x) := T$ and call it the \emph{Fréchet derivative} or \emph{Fréchet differential} of $f$ at $x$.
  If $f$ is differentiable at every point $\cU$, then $f$ is said to be \emph{differentiable} on $\cU$.
\end{definition}

\begin{remark}
  Since $\d f(x)$ is a bounded, linear operator, by Lemma X, it is also continuous. 
\end{remark}

\begin{remark}
  While many authors in the calculus of variations literature write the Fréchet derivative as derivative between Banach spaces, the definition also applies to Hilbert spaces.
  On the other hand, in the functional analysis literature, it is presented as derivatives in Hilbert spaces.
  A. V. Balakrishnan, Applied Functional Analysis. Springer, 1976.
  Extension of Wirtinger's Calculus to Reproducing Kernel Hilbert Spaces and the Complex Kernel LMS Pantelis Bouboulis, Sergios Theodoridis .
  For Gateaux derivative, $\cV$ need only be a vector space, while $\cW$ a topological space.
  For continuous linear functionals on $\bbR$ then this is fine.
\end{remark}


The intuition here is similar to that of regular differentiability, that the linear operator $T$ well approximates the change in $f$ at $x$ (the numerator), relative to the change in $x$ (the denominator)---the fact that the limit exists and is zero, it must mean that the numerator converges faster to zero than the denominator does.
In Landau notation, we have the familiar expression $f(x+h) = f(x) + \d f(x) h + o(h)$, that is, the tangent line to $f$ at $x$ gives the best linear approximation to $f$ near $x$.
The limit in the definition is meant in the usual sense of convergence of functions with respect to the norms of $\cV$ and $\cW$.
Of course, we may use Fréchet derivatives in Hilbert spaces too by using the inner product norm of the space.

For the avoidance of doubt, $\d f(x)$ is not a vector in $\cW$, but is an element of the set of bounded, linear operators from $\cV$ to $\cW$, denoted $\L(\cV,\cW)$.
That is, if $f:\cU\to\cW$ is a differentiable function at all points in $\cU\subseteq\cV$, then its derivative is a linear map
\begin{align*}
  \d f: \cU &\to \text{L}(\cV,\cW) \\
  x &\mapsto \d f(x).
\end{align*}
It follows that this function may also have a derivative, which by definition will be a linear map as well:
\begin{align*}
  \d^2 f: \cU &\to \text{L}\big(\cV,\text{L}(\cV,\cW)\big) \\
  x &\mapsto \d^2f(x).
\end{align*}
The space on the righthand side is identified with the Banach space $\L(\cV \times \cV, \cW)$ of all continuous bilinear maps from $\cV$ to $\cW$.
In other words, an element $\phi\in \text{L}\big(\cV,\text{L}(\cV,\cW)\big)$ is identified with $\psi \in \text{L}(\cV \times \cV, \cW)$ such that for all $x,y\in\cV$, $\phi(x)(y) = \psi(x,y)$.
Simply put, a function $\phi$ linear in $x$ with $\phi(x)$ linear in $y$ is the same as a bilinear function $\psi$ in $x$ and $y$.
The second derivative $\d^2 f(x)$ is therefore a bounded, bilinear operator from $\cV\times\cV$ to $\cW$.

Another closely related type of differentiability is the concept of \emph{Gâteaux differentials}, which is the formalism of functional derivatives in calculus of variations.
Let $\cV$, $\cW$ and $\cU$ be as before, and consider the function $f:\cU\to\cW$.

\begin{definition}[Gâteaux derivative]
  The \emph{Gâteaux differential} or the \emph{Gâteaux derivative} $\partial_v f(x)$ of $f$ at $x \in \cU$ in the direction $v\in\cV$ is defined as
  \[
    \partial_v f(x) = \lim_{t \to 0} \frac{f(x + t v) - f(x)}{t},  % = \frac{\partial}{\partial t}f(x+tv)\bigg|_{t=0}.
  \]  
  for which this limit is taken relative to the topology of $\cW$.
  The function $f$ is said to be \emph{Gâteaux differentiable} at $x\in\cU$ if $f$ has a directional derivative along all directions at $x$.
  We name the operator $\partial f(x):\cV\to\cW$ which assigns $v \mapsto \partial_v f(x) \in \cW$ the \emph{Gâteaux derivative} of $f$ at $x$, and the operator $\partial f:\cU\to(\cV,\cW) = \{A \,|\, A:\cV\to\cW \}$ which assigns $x \mapsto \partial f(x)$ simply the \emph{Gâteaux derivative} of $f$.
  
\end{definition}

\begin{remark}
  The space $(\cV,\cW)$ of operators from $\cV$ to $\cW$ is not a topological space, and there is no obvious way to define a topology on it.
  Consequently, we cannot consider the Gâteaux derivative of the Gâteaux derivative.
  Furthermore, unlike the Fréchet derivative, which is by definition a linear operator, the Gâteaux derivative may fail to satisfy the additive condition of linearity\footnote{Although, for all scalars $\lambda \in \bbR$, the Gâteaux derivative is homogenous: $\partial_{\lambda v}f(x) = \lambda \partial_v f(x)$.}.
  Finally, even if it is linear, it may fail to depend continuously on $v$ if $\cV$ and $\cW$ are infinite dimensional.
\end{remark}

Nevertheless, the reason we bring up Gâteaux differentials is that it may motivate higher-order Fréchet differentials.
First note the connection between the two, by again considering the function $f:\cU\to\cW$.

\begin{lemma}[Fréchet differentiability implies Gâteaux differentiability]
  If $f$ is Fréchet differentiable at $x\in\cU$, then $f$ is Gâteaux differentiable at that point too, and $\d f(x) = \partial f(x)$.
\end{lemma}

\begin{proof}
  Since $f$ is Fréchet differentiable at $x\in\cU$, we can write $f(x+v) \approx f(x) + \d f(x)(v)$ for some $v\in\cV$.
  Then, 
  \begin{align*}
    \left\Vert \frac{f(x + t v) - f(x)}{t} - \d f(x)(v) \right\Vert_\cW
    &= \frac{1}{t} \big\Vert f(x + t v) - f(x) - \d f(x)(tv)  \big\Vert_\cW \\
    &= \frac{\big\Vert f(x + t v) - f(x) - \d f(x)(tv) \big\Vert_\cW }{\norm{tv}_\cV}\cdot \norm{v}_\cV 
  \end{align*}
  which converges to 0 since $f$ is Fréchet differentiable at $x$, and  $t\to 0$ if and only if $\norm{tv}_\cV \to 0$.
  Thus, $f$ is Gâteaux differentiable at $x$, and the Gâteaux derivative $\partial_v f(x)$ of $f$ at $x$ in the direction $v$ coincides with the Fréchet derivatiave of $f$ at $x$ evaluated at $v$.
\end{proof}

Consider now the function $\d f(x):\cV\to\cW$ and suppose that $f$ is twice Fréchet differentiable at $x\in\cU$, i.e. $\d f(x)$ is Fréchet differentiable at $x\in\cU$ with derivative $\d^2 f(x):\cV\times\cV \to \cW$.
Then, $\d f(x)$ is also Gâteaux differentiable at the point $x$ and the two differentials coincide.
In particular, we have
\begin{align}\label{eq:frech2gat}
  \left\Vert \frac{\d f(x + t v)(v') - \d f(x)(v')}{t} - \d^2 f(x)(v,v') \right\Vert_\cW \to 0 \text{ as } t \to 0,
\end{align}
by a similar argument in the proof above.
We will use this fact when we describe the Hessian in a little while.

There is also the concept of \emph{gradients} in Hilbert space.
Recall that the Riesz representation theorem says that the mapping $A:\cV\to\cV'$ from the Hilbert space $\cV$ to its continuous dual space $\cV'$ defined by $A = \ip{\cdot,v}_\cV$ for some $v\in\cV$ is an isometric isomorphism.
Again, let $\cU \subseteq \cV$ be an open subset, and let $f:\cU\to\bbR$ be a (Fréchet) differentiable function with derivative $\d f: \cU \to \L(\cV,\bbR) \equiv \cV'$.
We define the gradient as follows.

\begin{definition}[Gradients in Hilbert space]
  The \emph{gradient} of $f$ is the operator $\nabla f: \cU \to \cV$ defined by $\nabla f = A^{-1} \circ \d f$.
  Thus, for $x \in \cU$, the gradient of $f$ at $x$, denoted $\nabla f(x)$, is the unique element of $\cV$ satisfying
  \[
    \ip{\nabla f(x), v}_\cV = \d f(x)(v)
  \]
  for any $v \in \cV$.
  Note that $\nabla f$ being a composition of two continuous functions, is itself continuous.
\end{definition}

Since the gradient of $f$ is an operator on $\cU$ to $\cV$, it may itself have a (Fréchet) derivative.
Assuming existence, i.e., $f$ is twice Fréchet differentiable at $x \in \cU$, we call this derivative the \emph{Hessian} of $f$.
From \eqref{eq:frech2gat}, it must be that
\begin{align*}
  \d^2 f(x)(v,v') &= \lim_{t\to 0} \frac{\d f(x + t v)(v') - \d f(x)(v')}{t} \\
  &= \lim_{t\to 0} \frac{\ip{\nabla f(x+tv), v'}_\cV - \ip{\nabla f(x), v'}_\cV}{t} \\
  &= \left\langle \lim_{t\to 0} \frac{\nabla f(x+tv) - \nabla f(x)}{t} , v' \right\rangle_\cV \\%\hspace{10pt} \rlap{\color{gray} \text{by linearity}} \\
  &= \left\langle \partial_v \nabla f(x) , v' \right\rangle_\cV.
\end{align*}
The second line follows from the definition of gradients, and the third line follows by linearity of inner products.
Note that since the Fréchet and Gâteaux differentials coincide, we have that $\partial_v \nabla f(x) = \d \nabla f(x) (v)$.
Letting $\cV$, $\cW$ and $\cU$ be as before, we now define the Hessian for the function $f:\cU \to \cW$.

\begin{definition}[Hessian]
  The Fréchet derivative of the gradient of $f$ is known as the \emph{Hessian} of $f$.
  Denoted $\nabla^2 f$, it is the mapping $\nabla^2 f: \cU \to \L(\cV,\cV)$ defined by $\nabla^2 f  = \d \nabla f$, and it satisfies
  \[
    \left\langle \nabla^2 f(x)(v) , v' \right\rangle_\cV = \d^2 f(x)(v,v').
  \]
  for $x\in\cU$ and $v,v'\in\cV$.
\end{definition}

\begin{remark}
  Since $\d^2 f(x)$ is a bilinear form in $\cV$, we can equivalently write
  \[
    \d^2 f(x)(v,v') = \ip{\d^2 f(x), v \otimes v'}_{\cV\otimes\cV}
  \]
  following the correspondence between bilinear forms and tensor product spaces.  
\end{remark}

We can now define the score $S$, assuming existence, as the (Fréchet) derivative of $L(\cdot|Y)$, i.e. $S:\Theta \to \L(\Theta,\bbR) \equiv \Theta'$ defined by $S = \d L(\cdot | Y)$.
The second (Fréchet) derivative of $L(\cdot|Y)$ is then $\d^2 L(\cdot|Y): \Theta \to \L(\Theta \times \Theta,\bbR)$.
The Fisher information $\cI(\theta)$ at $\theta\in\Theta$ is defined to be
\[
  \cI(\theta) = -\E[\d^2 L(\theta|Y)] \in \Theta \otimes \Theta.
\]
or \hltodo[Is this required?]{alternatively}
\begin{align*}
  \cI(\theta) 
  &= \E[\d L(\theta|Y) \otimes \d L(\theta|Y)] \\ %\in \Theta \otimes \Theta 
  &= \E[\ip{\nabla L(\theta|Y), \cdot}_{\Theta} \otimes \ip{\nabla L(\theta|Y), \cdot}_{\Theta}] \\
  &= \E\ip{\nabla L(\theta|Y) \otimes \nabla L(\theta|Y), \cdot}_{\Theta\otimes\Theta} \\
  &= \ip{\E[\nabla L(\theta|Y) \otimes \nabla L(\theta|Y)], \cdot}_{\Theta\otimes\Theta}.
\end{align*}

Since $\cI(\theta) \in \Theta \otimes \Theta$ we may view it also as a bilinear form. 
That is, for any $b,b'\in\Theta$, we have
\begin{align}\label{eq:fisher-linear-functional}
  \cI(\theta)(b,b') = \ip{\cI(\theta), b \otimes b'}_{\Theta\otimes\Theta}.
\end{align}
\hltodo[This part seems sketchy.]{We call this the Fisher information for $\theta$ evaluated at two points $b$ and $b'$ in $\Theta$}.
Setting $\theta_b = \ip{\theta,b}_\Theta$ for some $b\in\Theta$, we may view this also as the Fisher information between two continuous, linear functionals of $\theta$.
That is, $\cI(\theta)(x,\cdot)$ and $\cI(\theta)(\cdot,x')$ are both \hltodo[I think it is, because derivatives are continuous]{continuous}, linear functionals on $\Theta$, and thus belong to the continuous dual space $\Theta'$.
By the Riesz representation theorem, $\cI(\theta)(x,\cdot) = \ip{\cdot,b}$ for some $b\in\Theta$...

\subsection{Tensor product spaces}
\hltodo{Move this to Chapter 2}

\begin{definition}[Tensor products]
  Let $x_1\in\cH_1$ and $x_2\in\cH_2$ be two elements of two real Hilbert spaces.
  Then, the tensor product $x_1 \otimes x_2:\cH_1\times\cH_2\to\bbR$, is a bilinear form defined as
  \[
    (x_1\otimes x_2)(y_1,y_2) = \ip{x_1,y_1}_{\cH_1}\ip{x_2,y_2}_{\cH_2}
  \]
  for any $(y_1,y_2)\in\cH_1\times\cH_2$.
\end{definition}

\begin{definition}[Tensor product space]
  The tensor product space $\cH_1\otimes\cH_2$ is the completion of the space
  \[
    \cA = \left\{ \sum_{j=1}^J x_{1j} \otimes x_{2j} \,\Bigg\vert\, x_{1j}\in\cH_1, x_{2j}\in\cH_2, J \in \bbN \right\}
  \]
  with respect to the norm induced by the inner product
  \[
    \left\langle  \sum_{j=1}^J x_{1j} \otimes x_{2j},  \sum_{k=1}^K y_{1k} \otimes y_{2k} \right\rangle_\cA = \sum_{j=1}^J\sum_{k=1}^K \ip{x_{1j},y_{1k}}_{\cH_1} \ip{x_{2j},y_{2k}}_{\cH_2}.
  \]
\end{definition}

An operator interpretation of the tensor product.
For each pair of elements $(x_1,x_2) \in \cH_1\times\cH_2$, we define the operator $A:\cH_1\to\cH_2$ in the following way:
\begin{align*}
  A_{x_1,x_2}:\cH_1&\to\cH_2 \\
  y_1&\mapsto \ip{x_1,y_1}_{\cH_1}x_2
\end{align*}
For some $y_1\in\cH_1$ and $y_2\in\cH_2$, we have that
\begin{align*}
  \ip{A_{x_1,x_2}(y_1),y_2}_{\cH_2} 
  &= \big\langle \ip{x_1,y_1}_{\cH_1}x_2 , y_2 \big\rangle_{\cH_2} \\
  &= \ip{x_1,y_1}_{\cH_1}\ip{x_2,y_2}_{\cH_2} \\
  &= (x_1\otimes x_2)(y_1,y_2).
\end{align*}
It is seen that the tensor product $x_1\otimes x_2$ is is associated with the rank one operator $B:\cH_1'\to\cH_2$ defined by $z \mapsto z(x_1)x_2$ with $z = \ip{x_1,\cdot}_{\cH_1}$.
We write $B = x_1 \otimes x_2$.
\hltodo[From Wikipedia. But don't really get it, although it might explain the Fisher information between linear functionals.]{Therefore, this extends a linear identification between $\cH_1\otimes\cH_2$ and the space of finite-rank operators from $\cH_1'$ to $\cH_2$.}
We now have three distinct interpretations of the tensor product:
\begin{itemize}
  \item \textbf{Bilinear form} (as defined in Definition 3.5). 
  \begin{align*}
    x_1 \otimes x_2:\cH_1 \times \cH_2 &\to \bbR \\
    (y_1,y_2) &\mapsto \ip{x_1,y_1}_{\cH_1}\ip{x_2,y_2}_{\cH_2}
  \end{align*}
  for $x_1,y_1\in\cH_1$ and $x_2,y_2\in\cH_2$.
  \item \textbf{Operator}.
  \begin{align*}
    x_1 \otimes x_2:\cH_1 &\to \cH_2 \\
    y_1 &\mapsto \ip{x_1,y_1}_{\cH_1}x_2
  \end{align*}  
  \item \textbf{General form} (as an element in the tensor space).
  \[
    x_1 \otimes x_2 \in \cH_1 \otimes \cH_2.
  \]
\end{itemize}

\subsection{Random elements in a Hilbert space}

\hltodo{Move this to Chapter 2}

Let $\cH$ be a real Hilbert space. 
We can define a metric on $\cH$ using $D(x,x') = \norm{x-x'}_\cH$, where the norm on $\cH$ is the norm induced by the inner product.
A collection $\Sigma$ of subsets of $\cH$ is called a \emph{$\sigma$-algebra} if $\emptyset \in \Sigma$, $S \in \Sigma$ implies its complement $S^c \in \Sigma$, and $S_j\in\Sigma$, $j\geq 1$ implies $\bigcup_{j=1}^\infty S_j \in \Sigma$.
The smallest $\sigma$-algebra containing all open subsets of $\cH$ is called the \emph{Borel $\sigma$-algebra}, and its members the Borel sets.
Denote by $\cB(\cH)$ the Borel $\sigma$-algebra of $\cH$.
The metric space $(\cH,D)$ is called \emph{separable} if it has a countable dense subset, i.e., there are $x_1,x_2,\cdots$ in $\cH$ such that the closure $\overline{\{x,_1,x_2,\cdots\}} = \cH$.

A function $\nu:\Sigma\to[0,\infty]$ is called a \emph{measure} if it satisfies
\begin{itemize}
  \item \textbf{Non-negativity}. $\nu(S) \geq 0$ for all $S$ in $\Sigma$
  \item \textbf{Null empty set}. $\nu(\emptyset) = 0$
  \item \textbf{$\sigma$-additivity}. For all countable, mutually disjoint sets $\{S_i\}_{i=1}^\infty$,
  \[
    \nu\left(\bigcup_{i=1}^\infty S_i \right) = \sum_{i=1}^\infty \nu(S_i)
  \] 
\end{itemize}
A measure $\nu$ on $\big(\cH,\cB(\cH)\big)$ is called a \emph{Borel measure} on $\cH$.
We shall only concern ourselves with finite Borel measures. 
In addition, if $\nu(\cH) = 1$ then $\nu$ is a \emph{(Borel) probability measure} and the measure space $\big(\cH,\cB(\cH),\nu\big)$ is a \emph{(Borel) probability space}.

Let $(\Omega,\cE,\Prob)$ be a probability space.
We say that a mapping $X:\Omega\to\cH$ is a \emph{random element} in $\cH$ if $X^{-1}(B)\in\cE$ for every Borel set, i.e., $X$ is a function such that for every $B\in\cB(\cH)$, its preimage $X^{-1}(B) = \{\omega \in \Omega \,|\, X(\omega) \in B \}$ lies in $\Sigma$.
This is simply a generalised version of the definition of random variables in regular Euclidean space.
From this definition, we can also properly define random functions $f$ in some Hilbert space of functions $\cF$.
In any case, every random element $X$ induces a probability measure on $\cH$ defined by
\[
  \nu(B) = \Prob\big(X^{-1}(B)\big) = \Prob\big( \omega \in \Omega | X(\omega) \in B  \big) = \Prob(X \in B).
\]
The measure $\nu$ is called the \emph{distribution} of $X$.
The \emph{density} $\pi$ of $X$ is a measurable function with the property that
\[
  \Prob(X \in B) = \int_{X^{-1}(B)} \omega \dint\Prob(\omega) = \int_B \pi(x) \dint\nu(x).
\]

\begin{definition}[Mean vector]
  Let $\nu$ be a Borel probability measure on a real Hilbert space $\cH$.
  Supposing that a random element $X$ of $\cH$ is \emph{integrable}, that is to say
  \[
    \E \norm{X}_\cH = \int_\cH \norm{x}_\cH \dint\nu(x) < \infty,
  \]
  then the unique element $\mu\in\cH$ satisfying 
  \[
    \ip{\mu,x'} = \int_\cX \ip{x,x'}_\cX \dint\nu(x) = \E\ip{X,x'}_\cH
  \]
  for all $x' \in \cH$ is called the \emph{mean vector}. 
\end{definition}

\begin{definition}[Covariance operator]
  Let $\nu$ be a Borel probability measure on a real Hilbert space $\cH$.
  Suppose that a random element $X$ of $\cH$ is \emph{square integrable}, i.e., $\E \norm{X}_\cH^2 < \infty$, and let $\mu$ be the mean vector of $X$.
  Then the \emph{covariance operator} $C$ is defined by the mapping
  \begin{align*}
    C:\cH &\to \cH \\
    x &\mapsto \E\big[\ip{X - \mu, x}_{\cH}(X - \mu) \big].
  \end{align*}
  The covariance operator $C$ is also an element of $\cH\otimes\cH$ that satisfies
  \begin{align*}
    \ip{C,x\otimes x'}_{\cH\otimes\cH} 
    &= \int_\cH \ip{z-\mu, x}_\cH\ip{z-\mu,x'}_\cH \dint\nu(z) \\
    &= \E\big[\ip{X-\mu,x}_\cH \ip{X-\mu,x'}_\cH\big]
  \end{align*}
  for all $x,x'\in\cH$.
\end{definition}

From the definition of the covariance operator, we see that it induces a symmetric, bilinear form, which we shall denote by $\Cov:\cH\times\cH\to\bbR$, through
\begin{align*}
  \ip{Cx,x'}_\cH 
  &= \big\langle\E\big[\ip{X - \mu, x}_{\cH}(X - \mu) \big], x' \big\rangle_\cH \\
  &= \E\big[\ip{X-\mu,x}_\cH \ip{X-\mu,x'}_\cH\big] \\
  &=: \Cov(x,x').
\end{align*}

\begin{definition}[Gaussian vectors]
  A random element $X$ is called \emph{Gaussian} if $\ip{X,x}_\cH$ has a normal distribution for all fixed $x\in\cH$.
  A Gaussian vector $X$ is characterised by its mean element $\mu\in\cH$ and its covariance $C\in\cH\otimes\cH$.
\end{definition}

