In the introductory chapter \colp{\cref{chapter1}}, we discussed that unless the regression function $f$ is regularised (for instance, using some prior information), the ML estimator of $f$ is likely to be inadequate.
In choosing a prior distribution for $f$, we appeal to the principle of maximum entropy \citep{jaynes1957a,jaynes1957b,jaynes2003probability}, which states that the probability distribution which best represents the current state of knowledge is the one with largest entropy.
In this section, we aim to show the relationship between the Fisher information for $f$ and its maximum entropy prior distribution.
Before doing this, we recall the definition of entropy and derive the maximum entropy prior distribution for a parameter which has unrestricted support.
Let $(\Theta,D)$ be a metric space and let $\nu = \nu_D$ be a volume measure induced by $D$ (e.g. Hausdorff measure).
In addition, assume $\nu$ is a probability measure over $\Theta$ so that $(\Theta,\cB(\Theta),\nu)$ is a Borel probability space.

\begin{definition}[Entropy]\label{def:entropy}
  Denote by $p$ a probability density over $\Theta$ relative to $\nu$.
  Suppose that $\int p\log p \dint \nu < \infty$, i.e. $p \log p$ is Lebesgue integrable and belongs to the space $\text{L}^1(\Theta,\nu)$.
  The entropy of a distribution $p$ over $\Theta$ relative to a measure $\nu$ is defined as
  \begin{align}\label{eq:entrop}
    H(p) = - \int_\Theta p(\theta) \log p(\theta) \, \d\nu(\theta).
  \end{align}
\end{definition}

In deriving the maximum entropy distribution, we will need to maximise the functional $H$ with respect to $p$.
Typically, this is done using calculus of variations techniques, and standard calculations \colp{\cref{apx:funcder}} reveal that the functional derivative of $H(p)$ with respect to $p$, denoted $\partial H/\partial p$, is equal to $- 1 - \log p$. 
We now present another well known result from information theory, regarding the form of the maximum entropy distribution.

\begin{lemma}[Maximum entropy distribution]\label{thm:maxentr}
  Let $(\Theta,D)$ be a metric space, $\nu=\nu_D$ be a volume measure induced by $D$, and $p$ be a probability density function on $\Theta$.
  The entropy maximising density $\tilde p$, which satisfies
  \[
    \argmax_{p\in\L^2(\Theta,\nu)} H(p) = - \int_\Theta \tilde p(\theta) \log \tilde p(\theta) \dint\nu(\theta),
  \]
  subject to the constraints
  \begin{align*}
    \begin{gathered}
      \E\big[D(\theta,\theta_0)^2\big] = \int_\Theta D(\theta,\theta_0)^2 p(\theta) \dint \nu(\theta) = \const, \hspace{1cm} 
      \int_\Theta p(\theta) \dint \nu(\theta) = 1, \\
      \text{and} \hspace{0.5cm} p(\theta) \geq 0, \forall \theta \in \Theta,
    \end{gathered}
  \end{align*}
  is the density given by
  \[
    \tilde p(\theta) \propto \exp \left(-\half D(\theta,\theta_0)^2 \right),
  \]
  for some fixed $\theta_0\in\Theta$.
  If $(\Theta,D)$ is a Euclidean space and $\nu$ a flat (Lebesgue) measure then $\tilde p$ represents a (multivariate) normal density.
\end{lemma}

\begin{proof}[Sketch proof.]
  This follows from standard calculus of variations, though we provide a sketch proof here.
  Set up the Langrangian
  \begin{align*}
      \cL(p,\gamma_1,\gamma_2) 
      ={}& 
      - \int_\Theta p(\theta) \log p(\theta) \dint\nu(\theta) +
      \gamma_1 \left(\int_\Theta D(\theta,\theta_0)^2 p(\theta) \dint\nu(\theta) - \const \right) \\
      &+ \gamma_2 \left( \int_\Theta p(\theta) \dint\nu(\theta) - 1 \right).
  \end{align*}
  Taking derivatives with respect to $p$ (see \cref{apx:funcder} for definition of functional derivatives) yields
  \begin{align*}
    \frac{\partial}{\partial p} \cL(p,\gamma_1,\gamma_2)(\theta)
    = - 1 - \log p(\theta) + \gamma_1 D(\theta,\theta_0)^2 + \gamma_2.
  \end{align*}
  Set this to zero, and solve for $p(\theta)$:
  \begin{align*}
    p(\theta) &= \exp \left( \gamma_1 D(\theta,\theta_0)^2 + \gamma_2 - 1 \right) \\
    &\propto \exp \left( \gamma_1 D(\theta,\theta_0)^2 \right).
  \end{align*}
  This density is positive for any values of $\gamma_1$ (and $\gamma_2$), and it normalises to one if $\gamma_1 < 0$. 
  As $\gamma_1$ can take any value less than zero, we choose $\gamma_1=-1/2$.
  
  Now, if $\Theta \equiv \bbR^m$ and $\nu$ is the Lebesgue measure, then $D(\theta,\theta_0)^2 = \norm{\theta-\theta_0}_{\bbR^m}^2$, so $\tilde p$ is recognised as a multivariate normal density centred at $\theta_0$ with identity covariance matrix.
\end{proof}

Returning to the normal regression model of \cref{eq:model1} subject to \cref{eq:model1ass}, we shall now derive the maximum entropy prior for $f$ in some RKKS $\cF$.
One issue that we have is that the set $\cF$ is potentially ``too big'' for the purpose of estimating $f$, that is, for certain pairs of functions $\cF$, the data do not allow an assessment of whether one is closer to the truth than the other.
In particular, the data do not contain information to distinguish between two functions $f$ and $g$ in $\cF$ for which $f(x_i) = g(x_i), i=1,\dots,n$.
Since the Fisher information for a linear functional of a non-zero $f \in \cF_n$ is non-zero, there is information to allow a comparison between any pair of functions in $f_0 + \cF_n := \{f_0 + f_n \,|\, f_0 \in \cF, f_n \in \cF_n \}$.
A prior for $f$ therefore need not have support $\cF$, instead it is sufficient to consider priors with support $f_0 + \cF_n$, where $f_0 \in \cF$ is fixed and chosen a priori as a ``best guess'' of $f$.
We now state and prove the main I-prior theorem.

\begin{theorem}[The I-prior]
  Let $\cF$ be a RKKS with kernel $h$, and consider the finite dimensional subspace $\cF_n$ of $\cF$ equipped with an inner product as per \cref{eq:Fninnerprod}.
%  Suppose $\Theta$ is a finite dimensional affine subspace of a Hilbert space with norm $\norm{\cdot}_\Theta$. 
%  We have a metric space $(\cF_n,d)$, where $d(f,f')^2 = \ip{f-f',f-f'}$.
  Let $\nu$ be a volume measure induced by the norm $\norm{\cdot}_{\cF_n} = \sqrt{\ip{\cdot,\cdot}_{\cF_n}}$.
  With $f_0 \in \cF$, let $\cD_0$ be the class of distributions $p$ such that 
  \[
    \E\big[\norm{f-f_0}^2_{\cF_n}\big] = \int_{\cF_n} \norm{f-f_0}^2_{\cF_n} \, p(f) \dint \nu(f) = \const
  \]
  Denote by $\tilde p$ the density of the entropy maximising distribution among the class of distributions within $\cD_0$.
  Then, $\tilde p$ is Gaussian over $\cF$ with mean $f_0$ and covariance function equal to the reproducing kernel of $\cF_n$, i.e.
  \[
    \Cov\big[f(x),f(x')\big] = h_n(x,x').
  \]
  We call $\tilde p$ the \emph{I-prior} for $f$.
\end{theorem}

\begin{proof}
  Recall the fact that any $f \in \cF$ can be decomposed into $f = f_n + r$, with $f_n \in \cF_n$ and $r \in \cF_n^\bot$.
  Also recall that there is no Fisher information about any $r \in \cR_n$, and therefore it is not possible to estimate $r$ from the data.
  Therefore, $p(r) = 0$, and one needs only consider distributions over $\cF_n$ when building distributions over $\cF$.
  
  The norm on $\cF_n$ induces the metric $D(f,f') = \norm{f - f'}_{\cF_n}$.
  Consider functions in the set $f_0 + \cF_n$, i.e. functions of the form
  \[
    f = f_0 + \sum_{i=1}^n h(\cdot,x_i)w_i,
  \]
  such that $(f - f_0) \in \cF_n$. 
  Compute the squared distance between $f$ and $f_0$:
  \begin{align*}
    D(f,f_0)^2 
    &= \norm{f - f_0}_{\cF_n}^2 \\
    &= \left\Vert \sum_{i=1}^n h(\cdot,x_i)w_i \right\Vert_{\cF_n}^2 \\
    &= \bw ^\top\bPsi^{-1} \bw.
  \end{align*}
  Thus, by \cref{thm:maxentr}, the maximum entropy distribution for $f - f_0 = \sum_{i=1}^n h(\cdot,x_i)w_i$ is
  \[
    (w_1,\dots,w_n)^\top \sim \N_n(\bzero,\bPsi).
  \]
  This implies that $f$ is Gaussian, since
  \begin{align*}
    \langle f,f' \rangle_{\cF}
    &= \left\langle f_0 + \sum_{i=1}^n h(\cdot,x_i)w_i \,,\, f' \right\rangle_{\cF} \\
    &= \ip{f_0,f'}_\cF +  \sum_{i=1}^n w_i \left\langle  h(\cdot,x_i), f' \right\rangle_{\cF}  
  \end{align*}
  is a sum of normal random variables, and therefore $\langle f,f' \rangle_{\cF}$ is normally distributed for any $f' \in \cF$.
  The mean $\mu\in\cF$ of this random vector $f$ satisfies $\E\ip{f,f'}_{\cF}  = \ip{\mu,f'}_{\cF}$ for all $f'\in\cF_n$, but
  \vspace{-1.1em}
  \begin{align*}
    \E\ip{f,f'}_{\cF}  
    &= \langle f_0,f' \rangle_{\cF} + 
    \E \left[ \sum_{i=1}^n w_i \left\langle  h(\cdot,x_i), f' \right\rangle_{\cF} \right] \\
    &= \langle f_0,f' \rangle_{\cF} + 
     \sum_{i=1}^n \cancelto{0}{\E [w_i]} \left\langle  h(\cdot,x_i), f' \right\rangle_{\cF} \\
    &= \langle f_0,f' \rangle_{\cF},
  \end{align*}
  so $\mu \equiv f_0$. 
  
  Following \cref{def:covoper}, the covariance between two evaluation functionals of $f$ is shown to satisfy 
  \begin{align*}
    \Cov\big[f(x),f(x')\big] 
    &= \Cov\big[\ip{f,h(\cdot,x)}_{\cF}, \ip{f,h(\cdot,x')}_{\cF} \big] \\
    &= \E\big[\ip{f-f_0,h(\cdot,x)}_{\cF}\ip{f-f_0,h(\cdot,x')}_{\cF} \big].
  \end{align*}
  Then, making use of the reproducing property of $h$ for $\cF$, we have that
  \begin{align*}
    \Cov\big[f(x),f(x')\big]
    &= \E \left[ 
    \left\langle \sum_{i=1}^n h(\cdot,x_i)w_i, h(\cdot,x) \right\rangle_{\cF} 
    \left\langle \sum_{j=1}^n h(\cdot,x_j)w_j, h(\cdot,x') \right\rangle_{\cF} 
    \right] \\
    &= \E \left[ 
    \sum_{i=1}^n\sum_{j=1}^n w_iw_j \left\langle  h(\cdot,x), h(\cdot,x_i) \right\rangle_{\cF} 
     \left\langle h(\cdot,x'), h(\cdot,x_j)\right\rangle_{\cF} 
    \right] \\
    &= \sum_{i=1}^n\sum_{j=1}^n \psi_{ij} h(x,x_i) h(x',x_j),
  \end{align*}
  which is the reproducing kernel for $\cF_n$.
\end{proof}

In closing, we reiterate the fact that the I-prior for $f$ in the normal regression model subject to $f$ belonging to some RKKS $\cF$ with kernel $h_\eta$ has the simple representation
\begin{align}\label{eq:iprior2}
  \begin{gathered}
    f(x_i) = f_0(x_i) + \sum_{k=1}^n h_\eta(x_i,x_k)w_k \\
    (w_1,\dots,w_n)^\top \sim \N_n(\bzero,\bPsi).
  \end{gathered}
\end{align}
Equivalently, this may be written as a Gaussian process-like prior 
\begin{align}\label{eq:iprior3}
  \big(f(x_1),\dots,f(x_n) \big)^\top \sim \N(\bff_0,\bH_\eta\bPsi\bH_\eta),
\end{align}
where $\bff_0 = \big(f_0(x_1),\dots,f_0(x_n) \big)^\top$ is the vector of prior mean functional evaluations, and $\bH_\eta = \big(h_\eta(x_i,x_j)\big)_{i,j=1}^n$ is the kernel matrix.
