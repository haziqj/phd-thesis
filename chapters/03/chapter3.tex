\documentclass[a4paper,showframe,11pt]{report}
\usepackage{standalone}
\standalonetrue
\ifstandalone
  \usepackage{../../haziq_thesis}
  \usepackage{../../haziq_maths}
  \usepackage{../../haziq_glossary}
  \addbibresource{../../bib/haziq.bib}
  \externaldocument{../01/.texpadtmp/chapter1}
  \externaldocument{../02/.texpadtmp/chapter2}
\fi

\begin{document}
\hChapterStandalone[3]{Fisher information and the I-prior}
\label{chapter3}

Traditionally, Fisher information is calculated for unknown parameters $\theta$ of probability distribution from observable random variables.
In a similar light, we can treat the regression function $f$ in the model stated in \cref{eq:model1}, subject to \cref{eq:model1ass}, as the unknown ``parameter'' for which we would like information regarding.
Care must be taken when computing derivatives with respect to $f$, especially when $f$ belong to an infinite-dimensional vector space $\cF$.
If $\cF$ possesses an orthonormal basis, then one could define the derivative of the functional $g:\cF\to\bbR$ component-wise with respect to the orthonormal basis, as in the finite dimensional case.
This is analogous to the usual concept of   partial derivatives.

However, the notion of partial derivatives does not generalise to arbitrary topological vector spaces for two reasons.
Firstly, general spaces may not have an orthonormal basis \citep[§5, pp. 76]{tapia1971diff}.
Secondly, component-wise derivatives, which are in essence limits taken component-wise using the usual definition of derivatives, may not coincide with the overall limit taken with respect to the topology of the vector space (see note in \cref{misc:comwiseder}).
For these reasons, there is a need to consider a more rigorous concept of differentiation suitable for infinite-dimensional vector spaces provided by Fréchet and Gâteaux derivatives.
These concepts are introduced in \cref{sec:fihilbert}, prior to the actual derivation of the Fisher information of the regression function in \cref{sec:firegfun}.

In the remaining sections, we discuss the notion of prior distributions for regression functions, and how one might assign a suitable prior.
In our case, we choose an objective prior following \citep{jaynes1957a,jaynes1957b}: in the absence of any prior knowledge, a prior distribution which maximises entropy should be used.
As it turns out, the entropy maximising prior for $f$ is Gaussian with mean chosen a priori and covariance kernel proportional to the Fisher information.
We call such a distribution on $f$ an \emph{I-prior distribution} for $f$.
The I-prior has a simple, intuitive appeal: much information about $f$ corresponds to a larger prior covariance, and thus less influence of the prior mean, and more of the data, in informing the posterior, and vice versa.

\section{The traditional Fisher information}
\input{03a-traditional-fisher.tex}

\section{Fisher information in Hilbert space}
\label{sec:fihilbert}
\input{03b-fisher-hilbert.tex}
%\input{old-fisher-material.tex}

\section{Fisher information for regression functions}
\label{sec:firegfun}
\input{03c-fisher-regression}

\section{The induced Fisher information RKHS}
\label{sec:inducedFisherRKHS}
\input{03d-induced-fisher-RKHS}

\section{The I-prior}
\input{03e-iprior-derivation}

\section{Conclusion}

In estimating the regression function $f$ of the normal model in \cref{eq:model1} subject to \cref{eq:model1ass} and $f$ belonging to an RKHS $\cF$, we established that the entropy maximising prior distribution for $f$ is Gaussian with some chosen prior mean $f_0$, and covariance function proportional\footnote{Proportionality, rather than equality, is a consequence of any RKHS scale parameters that $\cF$ may have.} to the Fisher information for $f$.
We call this the I-prior for $f$.

The dimension of the function space $\cF$ could be huge, infinite-dimensional even, while the task of estimating $f\in\cF$ only relies on a finite amount of data point.
However, we are certain that the Fisher information for $f$ exists only for the finite subspace $\cF_n$ as defined in \cref{eq:subspaceFn}, and it is zero everywhere else.
This suggests that the data only allows us to provide an estimation to the function $f\in\cF$ by considering functions in an (at most) $n$-dimensional subspace instead.
In other words, it would be futile to consider functions in a space larger than this, and hence there is an element of dimension reduction here, especially when $\dim(\cF) \gg n$.

By equipping the subspace $\cF_n$ with the inner product \cref{eq:Fninnerprod}, $\cF_n$ is revealed to be a RKHS with reproducing kernel equal to the Fisher information for $f$.
Importantly, functions in the subspace $\cF_n$ are structurally similar to the functions in the parent space $\cF$.
The problem at hand then boils down to a Gaussian process regression using the kernel of the RKHS $\cF_n$, which is the Fisher information for $f$.

\hClosingStuffStandalone
\end{document}