\documentclass[a4paper,showframe,11pt]{report}
\usepackage{standalone}
\standalonetrue
\ifstandalone
  \usepackage{../../haziq_thesis}
  \usepackage{../../haziq_maths}
  \usepackage{../../haziq_glossary}
  \addbibresource{../../bib/haziq.bib}
  \externaldocument{../01/.texpadtmp/introduction}
  \externaldocument{../02/.texpadtmp/chapter2}
\fi

\begin{document}
\hChapterStandalone[3]{Fisher information and the I-prior}

\hltodo[Intro is unbalanced. Focus on: Fisher information, extension to infinite dimensions, derivation of I-prior, relation to F.I.]{}
Traditionally, Fisher information is calculated for unknown parameters $\theta$ of probability distribution from observable random variables.
In a similar light, we can treat the regression function $f$ in the model stated in \cref{eq:model1}, subject to \cref{eq:model1ass}, as the unknown ``parameter'' for which we would like information regarding.
In this chapter, we extend the notion of Fisher information to abstract objects in Hilbert spaces, and also to linear functionals of these objects.
This will allow us to achieve our aim of deriving the Fisher information for our regression function.

Following this, we shall discuss the notion of prior distributions for regression functions, and how one might assign a suitable prior.
In our case, we choose an objective prior following \citep{jaynes1957a,jaynes1957b}---in the absence of any prior knowledge, a prior distribution which maximises entropy should be used.
It turns out, the entropy maximising prior for $f$ is Gaussian with mean chosen a priori and covariance kernel proportional to the Fisher information.
Such a distribution on $f$ is called the I-prior distribution.

\section{The traditional Fisher information}
\input{03a-traditional-fisher.tex}

\section{Fisher information for Hilbert space objects}
\input{03b-fisher-hilbert.tex}
%\input{old-fisher-material.tex}

\section{Fisher information for regression functions}
\input{03c-fisher-regression}

\section{The induced Fisher information RKHS}
\label{sec:inducedFisherRKHS}
\input{03d-induced-fisher-RKHS}

\section{The I-prior}
\input{03e-iprior-derivation}

\section{Conclusion}

In estimating the regression function $f$ of the normal model in \cref{eq:model1} subject to \cref{eq:model1ass}, and $f$ belonging to an RKHS $\cF$, we established that the entropy maximising prior distribution for $f$ is Gaussian with some prior mean $f_0$ that needs to be chosen, and covariance function equal to the Fisher information for $f$.
We call this the I-prior for $f$.

The dimension of the function space $\cF$ could be huge, infinite-dimensional even, while the task of estimating $f\in\cF$ only relies on a finite amount of data point.
However, we are certain that the Fisher information for $f$ exists only for the finite subspace $\cF_n$ as defined in \cref{eq:subspaceFn}, and it is zero everywhere else.
This suggests that the data only allows us to provide an estimation to the function $f\in\cF$ by considering functions in an (at most) $n$-dimensional subspace instead.
In other words, it would be futile to consider functions in a space larger than this, and hence there is an element of dimension reduction here, especially when $\dim(\cF) \gg n$.

By equipping the subspace $\cF_n$ with the inner product \cref{eq:Fninnerprod}, $\cF_n$ is revealed to be a RKHS with reproducing kernel equal to the Fisher information for $f$.
Importantly, functions in the subspace $\cF_n$ are structurally similar to the functions in the parent space $\cF$.
The problem at hand then boils down to a Gaussian process regression using the kernel of the RKHS $\cF_n$, which is the Fisher information for $f$.

%\section{Miscellanea}
%\input{03-misc}

\hClosingStuffStandalone
\end{document}