\documentclass[showframe,11pt,twoside,openright]{report}
\usepackage{standalone}
\standalonetrue
\ifstandalone
  \usepackage{../../haziq_thesis}
  \usepackage{../../haziq_maths}
  \usepackage{../../haziq_glossary}
  \addbibresource{../../bib/haziq.bib}
  \externaldocument{../01/.texpadtmp/chapter1}
  \externaldocument{../02/.texpadtmp/chapter2}  
  \externaldocument{../03/.texpadtmp/chapter3}
  \externaldocument{../04/.texpadtmp/chapter4}
  \externaldocument{../05/.texpadtmp/chapter5}
  \externaldocument{../06/.texpadtmp/chapter6}
  \externaldocument{../07/.texpadtmp/chapter7}
  \externaldocument{../appendix/.texpadtmp/appendix}  
\fi

\begin{document}
\hChapterStandalone[3]{Fisher information and the I-prior}
\label{chapter3}
\thispagestyle{chapterthree}

\newcommand{\funder}{\rho}  % variable used for Fréchet/Gâteaux

We are interested in calculating the Fisher information for our unknown regression function $f$ (the parameter to be estimated) in \cref{eq:model1}, subject to \cref{eq:model1ass} and $f\in\cF$, a reproducing kernel Kreĭn space (RKKS).
Usually, the Fisher information pertains to finite-dimensional parameters, but as $\cF$ may be infinite dimensional, care must be taken when computing derivatives with respect to $f$.
For function spaces that possess an orthonormal basis, which all Hilbert spaces do, then one could define the derivative of the functional $\funder:\cF\to\bbR$ componentwise with respect to the orthonormal basis, as in the finite-dimensional case.
This is analogous to the usual concept of partial derivatives.

However, the notion of partial derivatives does not generalise to arbitrary topological vector spaces for two reasons.
Firstly, general spaces may not have an orthonormal basis \citep[Sec. 5, p. 76]{tapia1971diff}.
Secondly, componentwise derivatives, which are in essence limits taken componentwise using the usual definition of derivatives, may not coincide with the overall limit taken with respect to the topology of the vector space.
For these reasons, there is a need to consider the rigorous concepts of differentiation suitable for infinite-dimensional vector spaces provided by Fréchet and Gâteaux derivatives.
These concepts are introduced in \cref{sec:fihilbert}, prior to the actual derivation of the Fisher information of the regression function in \cref{sec:firegfun}.

\index{I-prior}
\index{maximum entropy}
In the remaining sections, we discuss the notion of prior distributions for regression functions, and how one might assign a suitable prior.
In our case, we choose an objective prior following \citep{jaynes1957a,jaynes1957b,jaynes2003probability}: in the absence of any prior knowledge, a prior distribution which maximises entropy should be used.
As it turns out, the entropy maximising prior for $f$ is Gaussian with mean chosen a priori and covariance kernel proportional to the Fisher information.
We call such a distribution on $f$ an \emph{I-prior distribution} for $f$.
The I-prior has a simple, intuitive appeal: much information about $f$ corresponds to a larger prior covariance, and thus less influence of the prior mean, and more of the data, in informing the posterior, and vice versa.

\section{The traditional Fisher information}
\input{03a-traditional-fisher.tex}

\section{Fisher information in Hilbert space}
\label{sec:fihilbert}
\input{03b-fisher-hilbert.tex}

\section{Fisher information for regression functions}
\label{sec:firegfun}
\input{03c-fisher-regression}

\section{The induced Fisher information RKHS}
\label{sec:inducedFisherRKHS}
\input{03d-induced-fisher-RKHS}

\section{The I-prior}
\input{03e-iprior-derivation}

\section{Conclusion}

In estimating the regression function $f$ of the normal model in \cref{eq:model1} subject to \cref{eq:model1ass} and $f$ belonging to an RKKS $\cF$, we established that the entropy maximising prior distribution for $f$ is Gaussian with some chosen prior mean $f_0$, and covariance function proportional\footnote{Proportionality, rather than equality, is a consequence of any RKHS scale parameters that $\cF$ may have.} to the Fisher information for $f$.
We call this the I-prior for $f$.

The dimension of the function space $\cF$ could be huge, infinite dimensional even, while the task of estimating $f\in\cF$ only relies on a finite amount of data point.
However, we are certain that the Fisher information for $f$ exists only for the finite subspace $\cF_n$ as defined in \cref{eq:subspaceFn}, and it is zero everywhere else.
This suggests that the data only allows us to provide an estimation to the function $f\in\cF$ by considering functions in an (at most) $n$-dimensional subspace instead.
In other words, it would be futile to consider functions in a space larger than this, and hence there is an element of dimension reduction here, especially when $\dim(\cF) \gg n$.

By equipping the subspace $\cF_n$ with the inner product \cref{eq:Fninnerprod}, $\cF_n$ is revealed to be an RKHS with reproducing kernel equal to the Fisher information for $f$.
Importantly, since $\cF_n$ as in \cref{eq:subspaceFn} is the pre-Hilbert space whose completion as $n\to\infty$ is $\cF$, functions in the subspace $\cF_n$ contain ``similarly shaped'' functions as in the parent space $\cF$.
The problem at hand then boils down to a Gaussian process regression using the kernel of the RKHS $\cF_n$, which is the Fisher information for $f$.

\hClosingStuffStandalone
\end{document}