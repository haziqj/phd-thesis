\documentclass[a4paper,showframe,11pt,draft]{report}
\usepackage{standalone}
\standalonetrue
\ifstandalone
  \usepackage{../../haziq_thesis}
  \usepackage{../../haziq_maths}
  \usepackage{../../haziq_glossary}
  \addbibresource{../../bib/haziq.bib}
  \externaldocument{../01/.texpadtmp/introduction}
\fi

\begin{document}
\hChapterStandalone[3]{Fisher information and the I-prior}

Traditionally, Fisher information is calculated for unknown parameters $\theta$ of probability distribution from observable random variables.
In a similar light, we can treat the regression function $f$ in the model stated in \eqref{eq:model1}, subject to \eqref{eq:model1ass}, as the unknown ``parameter'' for which we would like information regarding.
In this chapter, we extend the notion of Fisher information to abstract objects in Hilbert spaces, and also to linear functionals of these objects.
This will allow us to achieve our aim of deriving the Fisher information for our regression function.

Following this, we shall discuss the notion of prior distributions for regression functions, and how one might assign a suitable prior.
In our case, we choose an objective prior following \citep{jaynes1957a,jaynes1957b}---in the absence of any prior knowledge, a prior distribution which maximises entropy should be used.
It turns out, the entropy maximising prior for $f$ is Gaussian with mean chosen a priori and covariance kernel proportional to the Fisher information.
Such a distribution on $f$ is called the I-prior distribution.

\section{The traditional Fisher information}
%\input{traditional-fisher.tex}

\section{Fisher information for Hilbert space objects}
\input{fisher-hilbert.tex}
%\input{old-fisher-material.tex}

\section{Fisher information for regression functions}
\input{fisher-regression}

\section{The induced Fisher information RKHS}
\input{induced-fisher-RKHS}

\section{The I-prior}
%\input{iprior-derivation}

\section{Rate of convergence}

\section{Conclusion}
We used the true Fisher information. \citet{efron1978assessing} say favour the observed information instead. Does this change if we use MLE $\hat f$ instead? Probably not... we don't use MLE anyway!

https://stats.stackexchange.com/questions/179130/gaussian-process-proofs-and-results

https://stats.stackexchange.com/questions/268429/do-gaussian-process-regression-have-the-universal-approximation-property

\hClosingStuffStandalone
\end{document}