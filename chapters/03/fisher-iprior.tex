\documentclass[a4paper,showframe,11pt,draft]{report}
\usepackage{standalone}
\standalonetrue
\ifstandalone
  \usepackage{../../haziq_thesis}
  \usepackage{../../haziq_maths}
  \usepackage{../../haziq_glossary}
  \addbibresource{../../bib/haziq.bib}
  \externaldocument{../01/.texpadtmp/introduction}
\fi

\begin{document}
\hChapterStandalone[3]{Fisher information and the I-prior}

Traditionally, Fisher information is calculated for unknown parameters $\theta$ of probability distribution from observable random variables.
In a similar light, we can treat the regression function $f$ in the model stated in \eqref{eq:model1}, subject to \eqref{eq:model1ass}, as the unknown ``parameter'' for which we would like information regarding.
In this chapter, we extend the notion of Fisher information to abstract objects in Hilbert spaces, and also to linear functionals of these objects.
This will allow us to achieve our aim of deriving the Fisher information for our regression function.

Following this, we shall discuss the notion of prior distributions for regression functions, and how one might assign a suitable prior.
In our case, we choose an objective prior following \citep{jaynes1957a,jaynes1957b}---in the absence of any prior knowledge, a prior distribution which maximises entropy should be used.
It turns out, the entropy maximising prior for $f$ is Gaussian with mean chosen a priori and covariance kernel proportional to the Fisher information.
Such a distribution on $f$ is called the I-prior distribution.

\section{The traditional Fisher information}
%\input{traditional-fisher.tex}

\section{Fisher information for Hilbert space objects}
\input{fisher-hilbert.tex}
%\input{old-fisher-material.tex}

\section{Fisher information for regression functions}
\input{fisher-regression}

\section{The induced Fisher information RKHS}
\input{induced-fisher-RKHS}

\section{The I-prior}
\input{iprior-derivation}

\section{Rate of convergence}
\hltodo{Should I say something about this? Rates can be better than GPR?}

\section{Conclusion}

Goal is always to estimate $f \in \cF$ based on finite amount of data points.
We know MLE is not so good, so want regularise by some prior.
Unfortunately, $\cF$ might be huge such that data don't provide enough information for $f$ to be estimated sufficiently well.
We ask: What is the smallest subset for which there is full information coming from the data? 
Intuitively, it must be of $n$-dimensions, the sample size of the data.
Rather separately, we found out what the Fisher information for $f$ looks like, and deduced that there is Fisher information only on an orthogonal projection of $\cF$ on to $\cF_n$.
There is this flavour of dimension reduction---no need to consider the entire space, because this is futile, but just consider functions in the smaller subspace, as this is the best we can do anyway.
Therefore, we just look in this subspace $\cF_n$ for an appropriate approximation to $f$. 
In particular, what prior should I use? On the basis of maximum entropy principle, I figure out that the form of our I-prior.
The connection of $\cF_n$ to Fisher information is this: $\cF_n$ is the subspace of $\cF$ for which Fisher information exists. Equipping this space with a particular inner product reveals that $\cF_n$ is a RKHS with reproducing kernel equal to the Fisher information for $f$.



\section{Omitted}
\input{03-omitted}


\hClosingStuffStandalone
\end{document}