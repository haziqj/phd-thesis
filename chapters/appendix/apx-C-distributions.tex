This is a reference relating to the multivariate normal,  matrix normal, truncated univariate and multivariate normal, which are collated from various sources for convenience.
Of interest are probability density functions, first and second moments, and entropy \colp{\cref{def:entropy}, page \pageref{def:entropy}}.
Note that in this part of the appendix, boldface notation for matrix and vectors are not used.

\section{Multivariate normal distribution}
\label{apx:fishermultinormal}

Let $X\in\bbR^d$ be distributed according to a multivariate normal (Gaussian) distribution with mean $\mu \in \bbR^d$ and covariance matrix $\Sigma \in\bbR^d$ (a square, symmetric, positive-definite matrix).
We say that $X\sim\N_d(\mu,\Sigma)$.
Then,
\begin{itemize}
  \item \textbf{Pdf}. $p(X|\mu,\Sigma) = (2\pi)^{-d/2}|\Sigma|^{-1/2}\exp\big(-\half (X-\mu)^\top\Sigma^{-1}(X-\mu)\big)$.
  \item \textbf{Moments}. $\E X=\mu$, $\E [XX^\top] = \Sigma + \mu \mu^\top$.
  \item \textbf{Entropy}. $H(p) = \half \log \abs{2\pi e \Sigma} = \half[d](1 + \log 2\pi) + \half\log\abs{\Sigma}$.
\end{itemize}

For $d=1$, i.e. $X$ is univariate, then its pdf is $p(X|\mu,\sigma^2) = \frac{1}{\sigma}\phi \left( \frac{X-\mu}{\sigma} \right)$, and its cdf is $F(X|\mu,\sigma^2) = \Phi \left( \frac{X-\mu}{\sigma} \right)$, where $\phi(\cdot)$ and $\Phi(\cdot)$ are the pdf and cdf of a univariate standard normal distribution.
In the special case that $\Sigma = \diag(\sigma_1^2,\dots,\sigma_d^2)$, then the components of $X=(X_1,\dots,X_d)^\top$ are independently distributed according to $X_i\sim\N(\mu_i,\sigma_i^2)$.

\begin{lemma}[Properties of multivariate normal]
  Assume that $X \sim \N_d(\mu,\Sigma)$ and $Y \sim \N_{d}(\nu,\Psi)$, where
  \[
    X = 
    \begin{pmatrix}
      X_a \\ X_b
    \end{pmatrix},
    \hspace{0.5cm}
    \mu = 
    \begin{pmatrix}
      \mu_a \\ \mu_b
    \end{pmatrix},
    \hspace{0.25cm}\text{and}\hspace{0.25cm}
    \Sigma = 
    \begin{pmatrix}
      \Sigma_a    &\Sigma_{ab} \\
      \Sigma_{ab}^\top &\Sigma_b \\
    \end{pmatrix}.
  \]
  Then,
  \begin{itemize}
    \item \textbf{Marginal distributions}.
    \[
      X_a \sim \N_{\dim X_a}(\mu_a,\Sigma_a)
      \hspace{0.5cm}\text{and}\hspace{0.5cm}
      X_b \sim \N_{\dim X_b}(\mu_b,\Sigma_b).
    \]
    \item \textbf{Conditional distributions}.
    \[
      X_a|X_b \sim \N_{\dim X_a}(\tilde\mu_a,\tilde\Sigma_a)
      \hspace{0.5cm}\text{and}\hspace{0.5cm}
      X_b \sim \N_{\dim X_b}(\tilde\mu_b,\tilde\Sigma_b),
    \]
    where
    \begin{alignat*}{5}
      & \tilde\mu_a 
      &&= \mu_a + \Sigma_{ab}\Sigma_b^{-1}(X_b-\mu_b)
      &&\hspace{1cm}
      &&\tilde\mu_b 
      &&= \mu_b + \Sigma_{ab}^\top\Sigma_a^{-1}(X_a-\mu_a) \\
      &\tilde\Sigma_a 
      &&= \Sigma_a -  \Sigma_{ab}\Sigma_b^{-1}\Sigma_{ab}^\top
      &&\hspace{1cm}
      &&\tilde\Sigma_b 
      &&= \Sigma_b -  \Sigma_{ab}^\top\Sigma_a^{-1}\Sigma_{ab} 
    \end{alignat*}
    \item \textbf{Linear combinations}. 
    \[
      AX + BY + C \sim \N_{d}(A\mu + B\nu + C, A\Sigma A^\top + B\Psi B^\top)
    \]
    where $A$ and $B$ are appropriately sized matrices, and $C\in\bbR^d$.
    \item \textbf{Product of Gaussian densities}. 
    \[
      p(X|\mu,\Sigma)p(Y|\nu,\Psi) \propto p(Z|m,S)
    \]
    where $p(Z)$ is a Gaussian density, $m = S(\Sigma^{-1}\mu + \Psi^{-1}\nu)$ and $S= (\Sigma^{-1} + \Psi^{-1})^{-1}$.
    The normalising constant is equal to the density of $\mu\sim\N(\nu,\Sigma + \Psi)$.
  \end{itemize}
\end{lemma}

\begin{proof}
  Omitted---see \citet[sec. 8]{petersen2008matrix}.
\end{proof}

Frequently, in Bayesian statistics especially, the following identities will be useful in deriving posterior distributions involving multivariate normals.

\begin{lemma}
  Let $x,b\in\bbR^d$ be a vector, $X,B\in\bbR^{n\times d}$ a matrix, and $A \in \bbR^{d \times d}$ a symmetric, invertible matrix.
  Then,
  \begin{align*}
    -\half x^\top A x + b^\top x 
    &= -\half (x - A^{-1}b)^\top A (x - A^{-1}b) + \half b^\top A^{-1} b \\
    -\half \tr (X^\top A X) + \tr(B^\top X)
    &= -\half \tr\big((X - A^{-1}B)^\top A(X - A^{-1}B) \big) + \half\tr(B^\top A^{-1} B).
  \end{align*}
\end{lemma}

\begin{proof}
  Omitted---see \citet[sec. 8.1.6]{petersen2008matrix}.
\end{proof}

\newcommand{\dSi}{\frac{\partial\Sigma_\theta}{\partial \theta_i}}
\newcommand{\dSinv}{\frac{\partial\Sigma_\theta^{-1}}{\partial \theta_i}}
\newcommand{\dSj}{\frac{\partial\Sigma_\theta}{\partial \theta_j}}
\newcommand{\dSjnv}{\frac{\partial\Sigma_\theta^{-1}}{\partial \theta_j}}
\newcommand{\dStwo}{\frac{\partial^2\Sigma_\theta}{\partial \theta_i\theta_j}}
\newcommand{\Xmu}{(X - \mu_\theta)}
\newcommand{\dmui}{\frac{\partial\mu_\theta}{\partial\theta_i}}
\newcommand{\dmuj}{\frac{\partial\mu_\theta}{\partial\theta_j}}
\newcommand{\dmuitop}{\frac{\partial\mu_\theta^\top}{\partial\theta_i}}
\newcommand{\dmujtop}{\frac{\partial\mu_\theta^\top}{\partial\theta_j}}
\newcommand{\dmutwo}{\frac{\partial^2\mu_\theta}{\partial\theta_i\partial\theta_j}}


\begin{lemma}
  Let $X\sim\N_p(\mu_\theta,\Sigma_\theta)$, that is, the mean vector $\mu_\theta$ and covariance matrix $\Sigma_\theta$ depends on a real, $q$-dimensional vector $\theta$.
  The Fisher information matrix $U \in \bbR^{q\times q}$ for $\theta$ has $(i,j)$ entries given by
  \begin{equation}
    U_{ij} = \dmuitop \Sigma_\theta^{-1} \dmuj + \half \tr \left(\Sigma_\theta^{-1} \dSi \Sigma_\theta^{-1} \dSj \right)
  \end{equation}
  for $i,j=1,\dots,q$.
\end{lemma}

\begin{proof}
Define the derivative of a matrix $\Sigma \in \bbR^{p\times p}$ with respect to a scalar $z$, denoted $\partial\Sigma/\partial z \in \bbR^{p\times p}$, by $(\partial\Sigma/\partial z)_{ij} = \partial \Sigma_{ij}/\partial z$, i.e. derivatives are taken element-wise.
The two identities below are useful:
\begin{gather}
  \frac{\partial}{\partial z} \tr\Sigma = \tr \frac{\partial\Sigma}{\partial z} \\
  \frac{\partial}{\partial z} \log \abs{\Sigma} = \tr\left( \Sigma^{-1} \frac{\partial\Sigma}{\partial z} \right) \\
  \frac{\partial\Sigma^{-1}}{\partial z} = -\Sigma^{-1} \frac{\partial\Sigma}{\partial z}\Sigma^{-1}
%  \frac{\partial AB}{\partial z} = \frac{\partial A}{\partial z} B + A\frac{\partial B}{\partial z}
\end{gather}
A useful reference for these identities is \citet{petersen2008matrix}.

Differentiating the log-likelihood for $\theta$ with respect to the $i$'th component of $\theta$ yields
\begin{align*}
\frac{\partial}{\partial \theta_i} L(\theta|X) 
  ={}& -\half \frac{\partial}{\partial \theta_i} \log \abs{\Sigma_\theta} - \half \frac{\partial}{\partial \theta_i} \tr(\Sigma_\theta^{-1}\Xmu\Xmu^\top) \\
  ={}& -\half \tr \left(\Sigma_\theta^{-1} \frac{\partial\Sigma_\theta}{\partial \theta_i}  \right) - \half \tr \left( \frac{\partial\Sigma_\theta^{-1}}{\partial \theta_i} \Xmu\Xmu^\top \right)   \\
  & - \half \tr \left( \Sigma_\theta^{-1} \frac{\partial}{\partial \theta_i}\big( \Xmu\Xmu^\top \big) \right) \\
  ={}& -\myoverbrace{\half \tr \left( \Sigma_\theta^{-1} \frac{\partial\Sigma_\theta}{\partial \theta_i}\right)}{(A)}
  - \myoverbrace{\half \tr \left( \Sigma_\theta^{-1} \frac{\partial\Sigma_\theta}{\partial \theta_i}\Sigma_\theta^{-1} \Xmu\Xmu^\top \right)}{(B)} \\
  ={}&  + \myoverbrace{\tr \left( \Sigma_\theta^{-1}\Xmu\dmuitop \right)}{(C)}.
\end{align*}

Taking derivatives again, this time with respect to $\theta_j$, of the three parts (A), (B) and (C) above, we get:

\begin{itemize}
  \item \textbf{(A)}
  \begin{align*}
    \half \frac{\partial}{\partial \theta_j}  \tr \left( 
    \Sigma_\theta^{-1} \frac{\partial\Sigma_\theta}{\partial \theta_i}
    \right) 
    &= \half \tr \left( \dSjnv \dSi + \Sigma_\theta^{-1} \dStwo \right)
  \end{align*} 
  
  \item \textbf{(B)}
  \begin{align*}
    \half \frac{\partial}{\partial \theta_j}  
    &\tr \left( 
    \Sigma_\theta^{-1} \dSi \Sigma_\theta^{-1} \Xmu\Xmu^\top 
    \right)  \\
    ={}& \half \tr \left( 
    \dSjnv \dSi \Sigma_\theta^{-1} \Xmu\Xmu^\top
    \right) \\
    & +\half \tr \left( 
    \Sigma_\theta^{-1} \dStwo \Sigma_\theta^{-1} \Xmu\Xmu^\top
    \right) \\
    & +\half \tr \left( 
    \Sigma_\theta^{-1} \dSi \dSjnv \Xmu\Xmu^\top
    \right) \\
    & - \tr \left( 
    \Sigma_\theta^{-1} \dSi \Sigma_\theta^{-1} \dmuj\Xmu^\top
    \right)
  \end{align*}
  
  \item \textbf{(C)}
  \begin{align*}
    \frac{\partial}{\partial \theta_j}  \tr \left( 
    \Sigma_\theta^{-1}\Xmu\dmuitop 
    \right) 
    &= \tr \Bigg( 
    \dSjnv \Xmu\dmuitop 
    -
    \Sigma_\theta^{-1} \dmuj \dmuitop \\
    &\hspace{2cm} - \Sigma_\theta^{-1}\Xmu\dmutwo \Bigg)
  \end{align*}
\end{itemize}

The Fisher information matrix $U$ contains $(i,j)$ entries equal to the expectation of $-\frac{\partial^2}{\partial \theta_i\theta_j} L(\theta|X)$. 
Using the fact that 1) $\E[X-\mu_\theta]=0$; 2) $\E [\tr \Sigma] = \tr (\E \Sigma)$; 3) $\E [XX^\top] = \Sigma_\theta$; and 4) the trace is invariant under cyclic permutations, we get
\begin{align*}
  U_{ij} 
  ={}& \tr \left( \Sigma_\theta^{-1} \dmuj \dmuitop \right)   \\
  & + \half \tr \left( 
  \cancel{\dSjnv \dSi} + \cancel{\Sigma_\theta^{-1}\dStwo}
  - \cancel{\dSjnv \dSi}  - \cancel{\Sigma_\theta^{-1}\dStwo} -  \dSi\dSjnv \right) \\
  ={}&  \dmuitop \Sigma_\theta^{-1} \dmuj + \half \tr \left(\Sigma_\theta^{-1} \dSi \Sigma_\theta^{-1} \dSj \right)
\end{align*}
as required.  \qedhere

\end{proof}

\section{Matrix normal distribution}
\label{apx:matrixnormal}

The matrix normal distribution is an extension of the Gaussian distribution to matrices.
Let $X\in\bbR^{n \times m}$ matrix, and let $X$ follow a matrix normal distribution with mean $\mu\in\bbR^{n \times m}$ and row and column variances $\Sigma \in \bbR^{n \times n}$ and $\Psi \in \bbR^{m \times m}$ respectively, which we denote by $X\sim\MN_{n,m}(\mu,\Sigma,\Psi)$.
Then,
\begin{itemize}
  \item \textbf{Pdf}. $p(X|\mu,\Sigma,\Psi) = (2\pi)^{-nm/2}|\Sigma|^{-m/2}|\Psi|^{-n/2} e^{-\half \tr \big(\Psi^{-1}(X-\mu)^\top\Sigma^{-1}(X-\mu)\big)}$.
  \item \textbf{Moments}. $\E X=\mu$, $\Var(X_{i \bigcdot }) = \Psi$ for $i=1,\dots,n$, and $\Var(X_{\bigcdot j}) = \Sigma$ for $j=1,\dots,m$. 
  \item \textbf{Entropy}. $H(p) = \half \log \abs{2\pi e (\Psi \otimes \Sigma)} = \half[nm](1 + \log 2\pi) + \half\log\abs{\Sigma}^m\abs{\Psi}^n$.
\end{itemize}

\begin{lemma}[Equivalence between matrix and multivariate normal]
  $X\sim\MN_{n,m}(\mu,\Sigma,\Psi)$ if and only if $\vecc X\sim \N_{nm}(\vecc\mu,\Psi \otimes \Sigma)$.
\end{lemma}

\begin{proof}
  In the exponent of the matrix normal pdf, we have
  \begin{align*}
    -\half \tr \big(\Psi^{-1}(X-\mu)^\top &\Sigma^{-1}(X-\mu)\big) \\
    &= -\half \vecc(X-\mu)^\top \vecc(\Sigma^{-1}(X-\mu)\Psi^{-1}) \\
    &= -\half \vecc(X-\mu)^\top (\Psi^{-1} \otimes \Sigma^{-1}) \vecc(X-\mu) \\
    &= -\half (\vecc X- \vecc \mu)^\top (\Psi \otimes \Sigma)^{-1} (\vecc X- \vecc \mu).     
  \end{align*} 
  Also, $|\Sigma|^{-m/2}|\Psi|^{-n/2} = |\Psi \otimes \Sigma|^{-1/2}$.
  This converts the matrix normal pdf to that of a multivariate normal pdf.
\end{proof}

Some useful properties of the matrix normal distribution are listed:
\begin{itemize}
  \item \textbf{Expected values}.
  \begin{align*}
    \E (X-\mu)(X-\mu)^\top &= \tr(\Psi)\Sigma \in \bbR^{n\times n} \\
    \E (X-\mu)^\top(X-\mu) &= \tr(\Sigma)\Psi \in \bbR^{m\times m} \\
    \E [XAX^\top] &= \tr(A^\top\Psi)\Sigma + \mu A\mu^\top \\
    \E [X^\top BX] &= \tr(\Sigma B^\top)\Psi + \mu^\top B\mu \\   
    \E [X CX] &=  \Sigma C^\top\Psi  + \mu C \mu \\    
  \end{align*} 
  \item \textbf{Transpose}. $X^\top \sim \MN_{m,n}(\mu^\top, \Psi, \Sigma)$.
  \item \textbf{Linear transformation}. Let $A \in \bbR^{a \times n}$ be of full-rank $a \leq n$ and $B \in \bbR^{m \times b}$ be of full-rank $b\leq m$. Then $AXB  \sim \MN_{a,b}(\mu^\top, A\Sigma A^\top, B^\top \Psi B)$.
  \item \textbf{Iid}. If $X_i \iid \N_m(\mu,\Psi)$ for $i=1,\dots,n$, and we arranged these vectors row-wise into the matrix $X = (X_1^\top,\dots,X_n^\top)^\top \in \bbR^{n\times m}$, then $X \sim \MN(1_n \mu^\top, I_n, \Psi)$.
\end{itemize}

\section{Truncated univariate normal distribution}
\label{apx:truncuninorm}

Let $X \sim \N(\mu,\sigma^2)$ with the random variable $X$ restricted to the interval $(a,b) \subset \bbR$.
Then we say that $X$ follows a truncated normal distribution, and we denote this by $X\sim\tN(\mu,\sigma^2,a,b)$.
Let $\alpha = (a-\mu)/\sigma$, $\beta = (b-\mu)/\sigma$, and $C = \Phi(\beta) - \Phi(\alpha)$.
Then,
\begin{itemize}
  \item \textbf{Pdf}. $p(X|\mu,\sigma,a,b) = C^{-1} (2\pi\sigma^2)^{-1/2}e^{-\frac{1}{2\sigma^2} (X-\mu)^2} = \sigma C^{-1} \phi(\frac{X-\mu}{\sigma})$.
  \item \textbf{Moments}. 
  \vspace{-1.2em}
  \begin{gather*}
    \E X = \mu + \sigma \frac{\phi(\alpha) - \phi(\beta)}{C} \\
    \E X^2 = \sigma^2 + \mu^2 + \sigma^2  \frac{\alpha\phi(\alpha) - \beta\phi(\beta)}{C}   + 2\mu\sigma \frac{\phi(\alpha) - \phi(\beta)}{C} \\
    \Var X = \sigma^2 \left[ 1 +  \frac{\alpha\phi(\alpha) - \beta\phi(\beta)}{C} - \left(\frac{\phi(\alpha) - \phi(\beta)}{C}\right)^2 \right]
  \end{gather*}
  \item \textbf{Entropy}.
  \begin{align*}
    H(p) 
    &= \half\log 2\pi e\sigma^2 + \log C + \frac{\alpha\phi(\alpha) - \beta\phi(\beta)}{2C} \\
    &= \half\log 2\pi e\sigma^2 + \log C + \frac{1}{2\sigma^2}\cdot \myoverbrace{\sigma^2\frac{\alpha\phi(\alpha) - \beta\phi(\beta)}{C}}{\Var X -\sigma^2 + (\E X - \mu)^2} \\
    &= \half\log 2\pi \sigma^2 + \log C + \frac{1}{2\sigma^2}\E [X - \mu]^2 
  \end{align*}
  because $\Var X + (\E X - \mu)^2 = \E X^2 - \cancel{(\E X)^2} + \cancel{(\E X)^2} + \mu^2 - 2\mu\E X.$
\end{itemize}

For binary probit models, the distributions that come up are one-sided truncations at zero, i.e. $\tN(\mu,\sigma^2,0,+\infty)$ (upper tail/positive part) and $\tN(\mu,\sigma^2,-\infty,0)$ (lower tail/negative part), for which their moments are of interest.
As an aside, if $\mu = 0$ then the truncation $\tN(0,\sigma^2,0,+\infty) \equiv \N_+(0,\sigma^2)$ is called the \emph{folded-normal} distribution.
For the positive one-sided truncation at zero, $C = \Phi(+\infty) - \Phi(-\mu/\sigma) = 1 - \Phi(-\mu/\sigma) = \Phi(\mu/\sigma)$, and for the negative one-sided truncation at zero, $C = \Phi(-\mu/\sigma) - \Phi(-\infty) = 1 - \Phi(\mu/\sigma)$.
Additionally, if $\sigma=1$, then $\tN(0,1,0,+\infty) \equiv \N_+(0,1)$ is called the \emph{half-normal} distribution.

One may simulate random draws from a truncated normal distribution by drawing from $\N(\mu,\sigma^2)$ and discarding samples that fall outside $(a,b)$.
Alternatively, the inverse-transform method using
\[
  X = \mu + \sigma\Phi^{-1}\left( \Phi(\alpha) + UC \right)
\]
with $U\sim\Unif(0,1)$ will work too.
Either of these methods will work reasonably well as long as the truncation region is not too far away from $\mu$, but neither is particularly fast.
Efficient algorithms have been explored which are along the lines of either accept/reject algorithms \citep{robert1995simulation}, Gibbs sampling \citep{damien2001sampling}, or pseudo-random number generation algorithms \citep{chopin2011fast}.
The latter algorithm is inspired by the Ziggurat algorithm \citep{marsaglia2000ziggurat} which is considered to be the fastest Gaussian random number generator.


\section{Truncated multivariate normal distribution}
\label{apx:truncmultinorm}

Consider the restriction of $X\sim \N_d(\mu,\Sigma)$ to a convex subset\footnote{A convex subset is a subset of a space that is closed under convex combinations. In Euclidean space, for every pair of points in a convex set, all the points that lie on the straight line segment which joins the pair of points are also in the set.}~$\cA \subset \bbR^d$.
Call this distribution the truncated multivariate normal distribution, and denote it $X \sim \tN_d(\mu,\Sigma,\cA)$.
The pdf is $p(X|\mu,\Sigma,\cA) = C^{-1}\phi(X|\mu,\Sigma)\ind[X\in\cA]$, where
\[
  C = \int_\cA \phi(x|\mu,\Sigma) \dint x = \Prob(X \in \cA).
\] 

Generally speaking, there are no closed-form expressions for $\E g(X)$ for any well-defined functions $g$ on $X$.
One strategy to obtain values such as $\E X$ (mean), $\E X^2$ (second moment) and $E \log p(X)$ (entropy) would be Monte Carlo integration.
If $X^{(1)},\dots,X^{(T)}$ are samples from $X\sim\tN_d(\mu,\Sigma,\cA)$, then $\widehat{\E g(X)} = \frac{1}{T} \sum_{i=1}^T g(X^{(i)})$.

Sampling from a truncated multivariate normal distribution is described by \citet{robert1995simulation}, who used a Gibbs-based approach, which we now describe.
%\citet{damien2001sampling}.
%In the latter, the authors explore a simple Gibbs-based approach that is easy to implement in practice.
Assume that the one-dimensional slices of $\cA$ 
\[
  \cA_k(X_{-j}) = \{X_j | (X_1,\dots,X_{j-1},X_j, X_{j+1},\dots,X_d) \in \cA \}
\]
are readily available so that the bounds or anti-truncation region of $X_j$ given the rest of the components $X_{-j}$ are known to be $(x_j^-, x_j^+)$.
Using properties of the normal distribution, the full conditionals of $X_j$ given $X_{-j}$ is
\begin{gather*}
  X_j|X_{-j} \sim \tN(\tilde\mu_j,\tilde\sigma_j^2, x_j^-, x_j^+) \\
  \tilde\mu_j = \mu_{j} + \Sigma_{j,-j}^\top\Sigma_{-j,-j}(x_{-j} - \mu_{-j}) \\
  \tilde\sigma_j^2 = \Sigma_{11} - \Sigma_{j,-j}^\top \Sigma_{-j,-j} \Sigma_{j,-j}.
\end{gather*}
According to \citet{robert1995simulation}, if $\Psi = \Sigma^{-1}$, then 
\[
  \Sigma_{-j,-j}^{-1} = \Psi_{-j,-j} - \Psi_{j,-j}\Psi_{-j,-j}^\top / \Psi_{jj}
\]
which means that we need only compute one global inverse $\Sigma^{-1}$.
Therefore, the Gibbs sampler makes draws from truncated normal distributions in the following sequence, given initial values $X^{(0)}$:
\begin{itemize}
  \item Draw $X^{(t)}_1|X^{(t)}_2,\dots,X^{(t)}_d \sim \tN(\tilde\mu_1,\tilde\sigma_1^2, x_1^-, x_1^+)$.
  \item Draw $X^{(t)}_2|X^{(t+1)}_1,X^{(t)}_3,\dots,X^{(t)}_d \sim \tN(\tilde\mu_2,\tilde\sigma_1^2, x_2^-, x_2^+)$.
  \item $\cdots$
  \item Draw $X^{(t)}_d|X^{(t+1)}_1,\dots,X^{(t+1)}_{d-1} \sim \tN(\tilde\mu_d,\tilde\sigma_d^2, x_d^-, x_d^+)$.
\end{itemize}

In a later work, \citet{damien2001sampling} introduce a latent variable $Y \in \bbR$ such that the joint pdf of $X$ and $Y$ is
\[
  p(X_1,\dots,X_d,Y) \propto \exp(-Y/2) \ind\big( Y > (X-\mu)^\top\Sigma^{-1}(X-\mu)\big)\ind(X\in\cA).
\]
Now, the Gibbs conditional densities for the $X_k$'s are given by
\[
  p(X_j|X_{-j},Y) \propto \ind(X_j \in \cB_j)
\]
where
\[
  \cB_j \in (x_j^-, x_j^+) \cap \{X_j \,|\, (X-\mu)^\top\Sigma^{-1}(X-\mu) < Y \}.
\]
Thus, given values for $X_{-j}$ and $Y$, the bounds for $X_j$ involves solving a quadratic equation in $X_j$.
The Gibbs conditional density for $Y|X$ is a shifted exponential distribution, which can be sampled using the inverse-transform method.
Thus, both $X$ and $Y$ can be sampled directly from uniform variates.

For probit models, we are interested in the conical truncations $\cC_j = \{ X_j > X_k | k\neq j, \text{and } k=1,\dots,m  \}$ for which the $j$'th component of $X$ is largest.
These truncations form cones in $d$-dimensional space such that $\cC_1 \cup \cdots \cup \cC_d = \bbR^d$, and hence the name.

In the case where $\Sigma$ is a diagonal matrix, the conically truncated multivariate normal distributions are easier to deal with due to the independence structure in the covariance matrix.
In particular, most calculations of interest involve only a one dimensional integral of products of normal cdfs.
We present some results that we have not previously seen before elsewhere.

\begin{lemma}\label{thm:contruncn}
  Let $X\sim \tN_d(\mu,\Sigma,\cC_j)$, with  $\mu=(\mu_1,\dots,\mu_d)^\top$ and $\Sigma = \diag(\sigma_1^2,\dots,\sigma_d^2)$, and $\cC_j = \{ X_j > X_k | k\neq j, \text{and } k=1,\dots,m  \}$ a conical truncation of $\bbR^d$ such that the $j$'th component is largest.
  Then,
  \begin{enumerate}[label=(\roman*)]
    \item \textbf{Pdf}. The pdf of $X$ has the following functional form:
    \[
    p(X) = \frac{C^{-1}}{\sigma_1 \cdots \sigma_d (2\pi)^{d/2}}\exp\left[- \half \sum_{i=1}^d \left( \frac{x_i - \mu_i}{\sigma_i} \right)^2 \right]
    \]
    where $\phi$ is the pdf of a standard normal distribution and
    \[
      C = \E_Z \bigg[ \mathop{\prod_{i=1}^d}_{i \neq j} \Phi \left(\frac{\sigma_j}{\sigma_i}Z + \frac{\mu_j - \mu_i}{\sigma_i} \right) \bigg]
    \]
    where $Z \sim \N(0,1)$. 
    \item \textbf{Moments}. The expectation $\E X = \big(\E X_1, \dots, \E X_d \big)^\top$ is given by
    \[
      \E X_i =
      \begin{cases}
        \mu_i - \sigma_i C^{-1} \E_Z \left[\phi_i 
%        {\displaystyle \mathop{\prod_{k=1}^d}_{k \neq i,j}}
        \prod_{k\neq i,j}
        \Phi_k \right] 
        &\text{ if } i \neq j \\
        \mu_j - \sigma_j \sum_{i \neq j} \big(\E X_i - \mu_i \big) &\text{ if } i = j \\
      \end{cases}
    \]
    and the second moments $\E [X - \mu]^2$ are given by
    \[
      \E [X_i - \mu_i]^2 =
      \begin{cases}
        \sigma_i^2 + (\mu_j-\mu_i)(\E X_i - \mu_i) 
        + \sigma_i\sigma_j C^{-1} \E_Z \left[Z \phi_i 
        \prod_{k\neq i,j}
        \Phi_k \right] 
        &\text{ if } i \neq j \\
        C^{-1} \sigma_j^2 \E_Z \left[Z^2  
        \prod_{k\neq j}
        \Phi_k \right]  &\text{ if } i = j \\
      \end{cases}        
    \]
    where we had defined
    \begin{align*}
      \phi_i = \phi_i(Z) &= \phi \left( \frac{\sigma_j Z + \mu_j - \mu_i}{\sigma_i} \right), \rlap{\text{and}} \\
%      \hspace{0.5cm}\text{and}\hspace{0.5cm}
      \Phi_i = \Phi_i(Z) &= \Phi \left( \frac{\sigma_j Z + \mu_j - \mu_i}{\sigma_i} \right).
    \end{align*}
    \item \textbf{Entropy}. The entropy is given by
    \[
      H(p) = \log C + \half[d] \log 2\pi + \half \sum_{i=1}^d \log \sigma_i^2 + \half \sum_{i=1}^d \frac{1}{\sigma_i^2} \E [ x_i - \mu_i ]^2.
    \]
  \end{enumerate}
\end{lemma}

\begin{proof}
  See \cref{apx:contrunproof} for the proof.
\end{proof}

%\section{Wishart distribution}
%
%Let $X\in\bbR^{m\times m}$ be a symmetric, positive-definite matrix.
%The Wishart distribution with scale matrix $\Psi$ and $s >  m-1$ degrees of freedom is denoted $X \sim \Wis_m(\Psi,s)$, and its pdf, moments and entropy are
%\begin{itemize}
%  \item \textbf{Pdf}. 
%  \[
%    p(X) = \frac{\abs{X}^{(m-s-1)/2}e^{-\tr(\Psi^{-1}X)/2}}{2^{sm/2}\abs{\Psi}^{s/2}\Gamma_m(s/2)}.
%  \]
%  \item \textbf{Moments}. $\E X=s\Psi$, $\Var X_{ij} = s(\Psi_{ij}^2 + \Psi_{ii}\Psi_{jj})$.
%  \item \textbf{Entropy}. 
%  \[
%    H(p) = \frac{m+1}{2}\log\abs{\Psi} + \half m(m+1)\log 2 + \log \Gamma_p \left(\half[s] \right) - \half[s-m-1] \psi_m \left(\half[s] \right) + \half[sm].
%  \]
%\end{itemize}
%In the above, $\Gamma_m(\cdot)$ and $\psi_m(\cdot)$ are the multivariate gamma and digamma functions, respectively, given by
%\[
%  \Gamma_m(a) = \int_{U>0} \exp(-\tr U) \abs{U}^{a-(m+1)/2} \dint U
%\]
%for positive-definite, real, $m \times m$ matrices $U$, and
%\[
%  \psi_m(a) = \frac{\partial}{\partial a}\log \Gamma_p(a).
%\]
%
\section{Gamma distribution}
\label{apx:gammadist}

\vspace{-0.3em}
For $X\in\bbR_{\geq 0}$, let $X$ be distributed according to the gamma distribution with shape $s$ and rate $r$, denoted $X\sim\Gamma(s,r)$. 
Then,
\begin{itemize}
  \item \textbf{Pdf}. $p(X) = \Gamma(s)^{-1} r^s X^{s-1} e^{-rX}$.
  \item \textbf{Moments}. $\E X=s/r$, $\Var X = s/r^2$.
  \item \textbf{Entropy}. $H(p)=s - \log r + \log \Gamma(s) + (1-s)\psi(s)$.
\end{itemize}
In the above, $\Gamma(\cdot) = \Gamma_1(\cdot)$ and $\psi(\cdot) = \psi_1(\cdot)$ are the gamma and digamma functions, defined by
\[
  \Gamma(a) = 
  \begin{cases}
    (a-1)! &\text{if } a \in \bbZ^+ \\
    \int_0^\infty u^{a-1} e^{-u} \dint u &\text{otherwise} \\
  \end{cases}
\]
and
\[
  \psi(a) = \frac{\partial}{\partial a}\log \Gamma(a) = \frac{\partial\Gamma(a)/\partial a}{\Gamma(a)}.
\]
Often, the gamma distribution is parameterised according to shape $s$ and scale $\sigma = 1/r$ parameters, $X\sim\Gamma(s,\sigma)$.
\vspace{-0.6em}

\section{Inverse gamma distribution}
\label{def:invgam}

\vspace{-0.3em}
For $X\in\bbR_{\geq 0}$, a random variable $X$ distributed according to an inverse gamma distribution with parameters $s$ (shape) and $\sigma$ (scale) is denoted by $X\sim\Gamma^{-1}(s,\sigma)$.
Then,
\begin{itemize}
  \item \textbf{Pdf}. $p(X) = \Gamma(s)^{-1} \sigma^s  X^{-(s+1)} e^{-\sigma/X}$.
  \item \textbf{Moments}. $\E X=\sigma/(s-1)$, $\Var X = \sigma^2\big((s-1)^2(s-2) \big)^{-1}$.
  \item \textbf{Entropy}. $H(p)= s + \log \big(\sigma\Gamma(s)\big) - (1+s)\psi(s)$.
\end{itemize}
with $\Gamma(\cdot)$ and $\psi(\cdot)$ representing the gamma and digamma functions respectively, as defined in \cref{apx:gammadist}.

\begin{lemma}
  If $X$ has a Gamma distribution with shape and rate $s$ and $r$, then $1/X \sim \Gamma^{-1}(s,r)$.
\end{lemma}

\begin{proof}
  Let $Y=1/X$.
  Then the pdf of $Y$ is
  \begin{align*}
    p_Y(Y) 
    &= p_X(1/Y) \left\vert \frac{\partial}{\partial Y} (1/Y) \right\vert \\
    &= \Gamma(s)^{-1} r^s (1/Y)^{s-1} e^{-r/Y} (1/Y^2) \\
    &= \Gamma(s)^{-1} r^s Y^{-(s+1)} e^{-r/Y}
  \end{align*}
  which is the pdf of an inverse gamma with shape $s$ and scale $r$.
\end{proof}



