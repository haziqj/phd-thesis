Define the linear model which relates the real-valued responses $\mathbf y=(y_1,\dots,y_n)$ to the data $\mathbf X = (\mathbf x_1, \dots, \mathbf x_n)^\top$, not in terms of $\mathbf X$ and $\boldsymbol\beta$ as we previously did in \eqref{eq:linmod1}, but instead in terms of a regression function $f$ belonging to a vector space of functions $\mathcal{F}$ with intercept $\boldsymbol\alpha=\alpha\mathbf 1_n$, as given below:
\begin{align}\label{eq:linmod2}
	\begin{gathered}
		\mathbf y = \boldsymbol\alpha + \mathbf f + \boldsymbol\epsilon \\
		\boldsymbol\epsilon \sim \text{N}(\mathbf 0, \boldsymbol\Psi^{-1}).\\
	\end{gathered}
\end{align}
Here, we have opted for the more compact matrix notation for $\mathbf f$, which represents the vector of evaluation functionals $(f(\mathbf x_1), \dots, f(\mathbf x_n)) \in \mathbb{R}^n$, where each $\mathbf x_i = (x_{i1}, \dots, x_{ip})$ ranging over some set $\mathcal X$ represents the $p$-dimensional explanatory variables relating to observation $i=1,\dots, n$. This model is said to be linear by virtue of $\mathcal{F}$ being a vector space, as it enjoys all the axiomatic properties of linearity that a vector space brings.  
%It is probably better to think of the data $\mathbf x_i$ as a set of characteristics for observation $i$ belonging to a set $\mathcal{X}$. This could be things such as real-valued covariates, categorical covariates, or even nominal valued characteristics indicating grouping (as in a multi-level setting). The vector space of functions $\mathcal{F}$ is then defined over this set $\mathcal{X}$, giving rise to various types of RKHS/RKKS. More on this in Section \ref{sec:toolbox}.

The I-prior methodology considers estimating a regression function $f$ with an objective prior based on the Fisher information of the function. The prior, which can be viewed as having maximum entropy (under certain constraints) over the space of functions being considered, is an ideal objective prior to use in situations where any prior knowledge about the function to be estimated is absent \citep{Jaynes1957, Jaynes1957a, jaynes2003}. To implement I-priors, we need a function space which is well-behaved, and we find this in reproducing kernel Hilbert spaces (RKHS). RKHSs give us convenient topologies - in particular, it allows us to evaluate the closeness of functions with respect to the norm of the function space. In general, we also have pointwise closeness as an implication of functions being close in the norm of the space. 

We also concern outselves with finding the Fisher information for our regression function, which requires careful thought on its existence. A RKHS has the property of having a continuous evaluation functional (by definition, in fact), and this is a necessary condition for the existence of the Fisher information for $f$ \citep{Bergsma2014}. As a remark, the Fisher information only exists for the orgthogonal projection of $f$ on an at most $n$-dimensional subspace of $\mathcal F$ - $n$ being the number of data points - thus potentially reducing the dimensionality of the problem when the regression model is of higher dimension compared to the sample size and possibly infinite. The I-prior for $f$ is then shown to be Gaussian with a hyperparameter $f_0$ (the prior mean) and covariance matrix the Fisher information based on the principle of maximum entropy.

The I-prior regression entails constructing or choosing a positive-definite kernel function $h:\mathcal X \times \mathcal X \rightarrow \mathbb R$ over the data set $\mathcal X$ which induces a RKHS $\mathcal F$ in which our regression function lives, and then estimating the regression function by its posterior mean after assuming an I-prior on the function. This, in comparison to maximum likelihood methods, may guard against overfitting when the dimension of the model is larger than the sample size. Simulations and real data examples show that the I-prior methodology performs competitively with existing methods, such are regularization.

\begin{remark}
	The model in \eqref{eq:linmod2} assumes normally distributed errors with mean zero and precision matrix $\boldsymbol\Psi$. It is simpler to further assume independently distributed errors, i.e. $\boldsymbol\Psi=\psi\mathbf I_n$, although this needn't be the case if the model requires some structure to the error covariance matrix (such as correlated errors in time series models).
\end{remark}

For the remainder of this section, the aim is to give a brief run-through of the theory and methodology of I-priors, and showcase an example of I-prior modelling for the multiple regression case which will be relevant to the Bayesian variable selection part of this paper. To achieve this aim, we first present some definitions and concepts in functional analysis. These are very brief notes that should give the reader some basic familiarity when we go on to describe the I-prior in Section \ref{sec:iprior2}. For a further in-depth reading, \cite{steinwart2008}, \cite{berlinet2011}, and \cite{Sejdinovic2012} are suggested.
