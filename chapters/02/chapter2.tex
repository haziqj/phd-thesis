\documentclass[a4paper,showframe,11pt]{report}
\usepackage{standalone}
\standalonetrue
\ifstandalone
  \usepackage{../../haziq_thesis}
  \usepackage{../../haziq_maths}
  \usepackage{../../haziq_glossary}
  \addbibresource{../../bib/haziq.bib}
  \externaldocument{../01/.texpadtmp/introduction}
\fi

\begin{document}
\hChapterStandalone[2]{Vector space of functions}

One of the main assumptions for regression modelling with I-priors is that the regression functions lie in some vector space of functions.
At first glance, this may seem strange, that the notion of functions (as mappings from input to output space) and vector spaces are somehow equatable.
Upon further thought, one realises that firstly, two functions of a similar, particular form may be added together (in some meaningful way) resulting in a function in that same form. 
Secondly, multiplication of a function by a scalar $c$ can be thought of as $c$ times the output of that function.
Indeed, running through the checklist\footnotemark~of what constitutes a vector space, we find that a ``space of functions'' satisfies them all.
\footnotetext{In modern linear algebra texts, this is the eight axioms of vector spaces over a field $\bbF$: The vectors forms an abelian group under addition, and this group has an $\bbF$-module structure.}

The purpose of this chapter is to provide a concise review of functional analysis leading up to the theory of reproducing kernel Hilbert and Kreĭn spaces (RKHS/RKKS).
The interest with these RKHS and RKKS is that these spaces have well-established mathematical structure and offer desirable topologies.
In particular, it allows the possibility of deriving the Fisher information for regression functions---this will be covered in Chapter 3.
As we shall see, RKHS are also extremely convenient in that they may be specified completely via their reproducing kernels.
Several of these function spaces are of interest to us, for example, spaces of linear functions, smoothing functions, and functions whose inputs are nominal values and even functions themselves.
RKHS are widely studied in the literature, but perhaps RKKS are less so.
To provide an early insight, RKKS are simply an extension of RKHS when its kernel is not positive-definite.
The flexibility provided by RKKS will prove both useful and necessary, especially when considering scaled function spaces, as I-prior modelling does.

It is emphasised that a deep knowledge of functional analysis, including RKHS and RKKS theory, is not at all necessary for I-prior modelling, so perhaps the advanced reader may wish to skip Sections 2.1--2.3. 
Section 2.4 describes the fundamental RKHS of interest for I-prior regression, which we refer to as the ``building block'' RKHS/RKKS.
The reason for this is that it is possible to construct new RKKS from existing ones, and this is described in Section 2.5.

A remark on notation: Elements of the vector space $\cF$ of real functions over a set $\cX$ are denoted $f(\cdot)$, or simply $f$.
This distinguishes them from the actual evaluation of the function at an input point $x \in \cX$, denoted $f(x) \in \bbR$.
For a much cleaner read, we dispense with boldface notation for vectors and matrices when talking about them, without ambiguity, in the abstract sense. 

\section{Some functional analysis}
\input{functional-analysis}

\section{Reproducing kernel Hilbert space theory}\label{sec:rkhstheory}
\input{rkhs}

\section{Reproducing kernel Kreĭn space theory}
\input{rkks}

\section{RKHS building blocks}
\input{rkhs-building-blocks}

\section{Constructing RKKS from existing RKHS}\label{sec:constructrkks}
\input{rkks-construction}

\section{Summary}

The brief notes on functional analysis allow us to describe the theory of reproducing kernel Hilbert and Kreĭn spaces.
These are of great interest to us because the topology endowed on such spaces gives great assurances---in particular, all evaluation functionals are continuous in these spaces.
Moreover, RKHS and RKKS can be specified completely through kernel functions, with new and complex function spaces built simply by manipulation of these kernel functions.
Of particular importance is the ANOVA functional decomposition, for which we realise provides an objective way of constructing various statistical models (such models will be described later on in detail in Chapter 4).

An annotated collection of bibliographical references used for this chapter is as follows.
\begin{itemize}
  \item \textbf{Functional analysis}. On the introductory material relating to functional analysis in Section 2.1, the lecture notes by \citet{sejdinovic2012} is recommended, and forms the basis for most of the material described. Additionally, \citet{rudin1987real} provides a complementary reading.
  \item \textbf{RKHS theory}. There are certainly no shortages of texts relating to the theory of RKHS: \citet{steinwart2008support}, \citet{berlinet2011reproducing}, and \citet{gu2013smoothing} to name a few.
  \item \textbf{RKKS theory}. The innovation of indefinite inner product spaces perhaps started in mathematical physics literature, for which the theory of special relativity depends. The four-dimensional Euclidean space-time is an often cited example. In any case, we referred to mainly \citet{ong2004learning}, which gives an overview in the context of learning using indefinite kernels. \citet{alpay1991some} and \citet{zafeiriou2012subspace} were also useful for understanding the fundamental concepts of RKKS.
  \item \textbf{RKHS building blocks}. The main building block RKHS, i.e. the canonical RKHS, the fBm RKHS and the Pearson RKHS are described in the manuscript of \citet{bergsma2017}.
  \item \textbf{ANOVA and functional ANOVA}. Classical ANOVA is pretty much existent in every fundamental statistical textbook. These texts have extremely well written introductions to this very important concept: \citet[Ch. 11]{casella2002statistical}, \citet[Ch. 3]{dean1999design}. On the relation between classical ANOVA and functional ANOVA decomposition, \citet{gu2013smoothing} offers novel insights. There is diverse literature concerning functional ANOVA, namely from the fields of machine learning (e.g. \cite{durrande2013anova}), applied mathematics (e.g. \cite{kuo2010decompositions}), and sensitivity analysis (e.g. \cite{sobol2001global}). What is interesting is that several authors who simply set out to obtain a suitable functional decomposition, all ended up somewhat independently recovering the ANOVA decomposition as being ``optimal'' in some sense. This speaks largely to this classical idea that is ANOVA.
\end{itemize}





\hClosingStuffStandalone
\end{document}