
The previous section outlined all of the basic RKHSs of functions that will form the building blocks when constructing more complex function spaces.
We will see, at the outset, that sums of kernels are kernels and products of kernels are also kernels. 
This provides us a platform for constructing new function spaces from existing ones.
To be more flexible in the specification of these new function spaces, we do not restrict ourselves to positive-definite kernels only, thereby necessitating us to use the theory of RKKSs.

\subsection{Sums, products and scaling of RKHS}

Sums of positive definite kernels are also positive definite kernels, and the product of positive definite kernel is a positive definite kernel.
They each, in turn, are associated with an RKHS that is defined by the sum of kernels and product of kernels, respectively.
The two lemmas below formalise these two facts. 

\begin{lemma}[Sum of kernels]\label{thm:sumkernels}
  If $h_1$ and $h_2$ are positive-definite kernels on $\cX_1$ and $\cX_2$ respectively, then $h = h_1 + h_2$ is a positive-definite kernel on $\cX_1 \times \cX_2$.
  Moreover, denote $\cF_1$ and $\cF_2$ the RKHS defined by $h_1$ and $h_2$ respectively.
  Then $\cF = \cF_1 \oplus \cF_2$ is an RKHS defined by $h = h_1 + h_2$, where
  \[
    \cF_1 \oplus \cF_2 = \{ f:\cX_1\times\cX_2 \to\bbR \,|\, f = f_1 + f_2, f_1\in\cF_1 \text{ and } f_2\in\cF_2 \}.
  \]
  For all $f\in\cF$,
  \[
    \norm{f}_\cF^2 = \min_{f_1+f_2=f} \left\{ \norm{f_1}_{\cF_1}^2 + \norm{f_2}_{\cF_2}^2 \right\}.
  \]
\end{lemma}

\begin{proof}
  That $h_1+h_2$ is a positive-definite kernel should be obvious, as the sum of two positive definite functions is also positive definite.
  For a proof of the remaining statements, see \citet[Theorem 5]{berlinet2011reproducing}.
\end{proof}

\begin{lemma}[Products of kernels]\label{thm:prodkernels}
  Let $\cF_1$ and $\cF_2$ be two RKHS of functions over $\cX_1$ and $\cX_2$, with respective reproducing kernels $h_1$ and $h_2$.
  Then, $h = h_1 h_2$ is a kernel on $\cX_1 \times \cX_2$.
  Moreover, the tensor product space $\cF_1 \otimes \cF_2$ is an RKHS with reproducing kernel $h$.
\end{lemma}

\begin{proof}
  Fix $n\in\bbN$, and let $\bH_1$ and $\bH_2$ be the kernel matrices for $h_1$ and $h_2$ respectively.
  Since these kernel matrices are symmetric and positive definite by virtue of $h_1$ and $h_2$ being symmetric and positive-definite functions, we can write $\bH_1 = \bA^\top\bA$ and $\bH_1 = \bB^\top\bB$ for some matrices $\bA$ and $\bB$: perform an (orthogonal) eigendecomposition of each of the kernel matrices, and take square roots of the eigenvalues.
  Let $\bH$ be the kernel matrix for $h = h_1h_2$.
  With $x_i = (x_{i1}, x_{i2})$, its $(i,j)$ entries are
  \begin{align*}
    h(x_i,x_j)
    &= h_1(x_{i1},x_{i2}) \, h_2(x_{j1},x_{j2}) \\
    &= (\bA^\top\bA)_{ij} \, (\bB^\top\bB)_{ij} \\
    &= \sum_{k=1}^n a_{ik}a_{jk} \sum_{l=1}^n b_{il}b_{jl},
  \end{align*}
  where we have denoted $b_{ij}$ and $c_{ij}$ to be the $(i,j)$'th entries of $\bB$ and $\bC$ respectively 
  Then,
  \begin{align*}
    \sum_{i=1}^n\sum_{j=1}^n h(x_i,x_j)
    &= \sum_{k=1}^n \sum_{l=1}^n \sum_{i=1}^n \sum_{j=1}^n  \lambda_i \lambda_j a_{ik}a_{jk}b_{il}b_{jl} \\
    &= \sum_{k=1}^n \sum_{l=1}^n \left(\sum_{i=1}^n \lambda_i a_{ik} b_{il} \right) \left( \sum_{j=1}^n  \lambda_j a_{jk}b_{jl} \right) \\
    &= \sum_{k=1}^n \sum_{l=1}^n \left(\sum_{i=1}^n \lambda_i a_{ik} b_{il} \right)^2 \\
    &\geq 0
  \end{align*}
  Again, for the remainder of the statement in the lemma, we refer to \citet[Theorem 13]{berlinet2011reproducing}.
\end{proof}

\index{Hadamard product}
A familiar fact from linear algebra is realised here from \cref{thm:sumkernels,thm:prodkernels}: 
%1) Multiplying a positive definite matrix by a positive constant results in a positive definite matrix; 
1) the addition of positive-(semi)definite matrices is a positive-(semi)definite matrix; and 
2) the \emph{Hadamard product}\footnotemark~of two positive (semi-)definite matrices is a positive (semi-)definite matrix.
\footnotetext{The Hadamard product is an element-wise multiplication of two matrices $\bA$ and $\bB$ of identical dimensions, denoted $\bA \circ \bB$. That is, $(\bA \circ \bB)_{ij} = \bA_{ij}\bB_{ij}$.}

The scale of an RKHS of functions $\cF$ over a set $\cX$ with kernel $h$ may be arbitrary.
To resolve this issue, a scale parameter $\lambda\in\bbR$ for the kernel $h$ may be introduced, which will typically need to be estimated from the data. 
If $h$ is a positive definite-kernel on $\cX\times\cX$, and $\lambda \geq 0$ a scalar, then this yields a scaled RKHS $\cF_\lambda = \{\lambda f \,|\, f \in \cF \}$ with reproducing kernel $\lambda h$, where $\cF$ is the RKHS defined by $h$.

Restricting $\lambda$ to the positive reals is arbitrary and unnecessarily restrictive.
Especially when considering sums and products of scaled RKHSs, having negative scale parameters also give additional flexibility.
The resulting kernels from summation and/or multiplication with negative kernels may no longer be positive definite, and in such cases, they give rise to RKKSs instead.

\begin{remark}
  Recall that an RKKS $\cF$ of functions over $\cX$ can be uniquely decomposed as the difference between two RKHSs $\cF_+$ and $\cF_-$, and its associated Hilbert space $\cF_\cH$ is the RKHS $\cF_+ \oplus \cF_-$.
  It is important to note that both $\cF$ and $\cF_\cH$ contain identical functions over $\cX$, but different topologies.
  That is to say, functions that are close with respect to the norm of $\cF$ may not be close to each other in the norm of $\cF_\cH$.
\end{remark}

%Without the positive restriction, the kernel may potentially be negative-definite.
%Therefore, the subsequent sections speak of RKKSs, instead of RKHSs, to account solely for the fact that $\lambda$ may be negative.
%All other properties of RKHSs should carry over to RKKSs, so sometimes we might overlook this distinction, and make references to RKHSs when instead RKKSs would be more suited to the context.
%\begin{remark}
%  As it turns out, for I-prior modelling, in cases where the RKHS is $\cF_\lambda$ with kernel $\lambda h$, then the sign of the single scale parameter $\lambda$ is unidentified.
%  Therefore, in such cases, we may restrict $\lambda\in\bbR^+$.
%  More on this in Chapter 4.
%\end{remark}
%Kreĭn spaces can be seen as a generalisation of Hilbert spaces, which caters for inner products not being positive definite.
%To motivate the need for Kreĭn spaces, we first look at several operations on reproducing kernels and the resulting vector space.
%\begin{lemma}[Scaling of kernels]\label{thm:scalingkernels}
%  If $h$ is a kernel on $\cX$, and $\lambda \geq 0$ a scalar, then $\lambda h$ is a kernel.
%  This yields a scaled RKHS $\cF_\lambda = \{\lambda f \,|\, f \in \cF \}$ with reproducing kernel $\lambda h$, where $\cF$ is the RKHS defined by $h$.
%\end{lemma}
%\begin{proof}
%  Multiplying a positive definite function by a positive constant results in a positive definite function still, and thus defines a unique RKHS.
%  The scaling of functions is seen through the fact that $\cF$ is the completion of the space spanned by the kernels, and hence $\cF_\lambda$ by the scaled kernels.
%\end{proof}
%The difference of kernels is not guaranteed to be a positive definite function.

\subsection{The polynomial RKKS}

A polynomial construction based on a particular RKHS building block is considered here.
For example, using the canonical RKHS in the polynomial construction would allow us to easily add higher order effects of the covariates $x \in \cX$.
In particular, we only require a single scale parameter in polynomial kernel construction.

\begin{definition}[Polynomial RKKS]
  \index{polynomial!kernel/RKKS}
  \index{degree}
  \index{offset}
  \index{degree|seealso{polynomial}}
  \index{offset|seealso{polynomial}}
  Let $\cX$ be a Hilbert space.
  The kernel function $\hXXR$ obtained through the $d$-degree polynomial construction of linear kernels is
  \[
    h_\lambda(x,x') = \big(\lambda\ip{x,x'}_\cX + c\big)^d,
  \]
  where $\lambda \in \bbR$ is a scale parameter for the linear kernel, and $c \in \bbR$ is a real constant called the \emph{offset}.
  This kernel defines the \emph{polynomial RKKS} of degree $d$.
\end{definition}

Write
\begin{align*}
  h_\lambda(x,x')_\cF = \sum_{k=0}^d \frac{d!}{k!(d-k)!} c^{k-d} \lambda^k \ip{x,x'}_\cX^k.
%  &= \sum_{k=0}^d 
%  {\color{grymath}\overbrace{\color{black}\frac{d!}{k!(d-k)!} c^{k-d} \lambda^k}^{\beta_k}}
%  \, \ip{x,x'}_\cX^k.
\end{align*}
Evidently, as the name suggests, this is a polynomial involving the canonical kernel.
In particular, each of the $k$-powered kernels (i.e., $\ip{x,x'}_\cX^k$) defines an RKHS of their own (since these are merely products of kernels), and therefore the sum of these $k$-powered kernels define the polynomial RKHS.

The offset parameter influences trade-off between the higher-order versus lower-order terms in the polynomial.
It is sometimes known as the bias term.

\begin{proposition}
  The polynomial RKKS $\cF$ of real functions over $\cX$ contains polynomial functions of the form $f(x)=\sum_{k=0}^d \beta_k x^k$.
\end{proposition}

\begin{proof}
  By construction, $\cF = \cF_\emptyset \oplus \bigoplus_{i=1}^d\bigotimes_{j=1}^i \cF_j$, where each $\cF_j, j \neq 0$ is the canonical RKHS, and $\cF_\emptyset$ is the RKHS of constant functions.
  Each $f \in \cF$ can therefore be written as $f = \beta_0 + \sum_{i=1}^d\prod_{j=1}^i f_j$, and $f_j(x)= b_j x$ as they are functions from the canonical RKHS, where $b_j$ is a constant.
  Therefore, $f(x) = \sum_{k=0}^d \beta_k x^k$.
\end{proof}

\begin{remark}
  We may opt to use other RKHSs as the building blocks of the polynomial RKHS.
  In particular, using the centred canonical kernel seems natural, so that each of the functions in the constituents of the direct sum of spaces is centred.
  However, the polynomial RKKS itself will not be centred.
\end{remark}

\subsection{The ANOVA RKKS}
\label{sec:anovarkks}

\index{ANOVA}
We find it useful to begin this subsection by spending some time to elaborate on the classical \gls*{anova} decomposition, and the associated notions of main effects and interactions.
This will go a long way in understanding the thinking behind constructing an \gls*{anova}-like RKKS of functions.

\subsubsection{The classical ANOVA decomposition}

The standard one-way \gls*{anova} is essentially a linear regression model which allows comparison of means from two or more samples.
Given sets of observations $y_j = \{y_{1j},\dots,y_{n_jj}\}$, $j=1,\dots,m$, we consider the linear model $y_{ij} = \mu_j + \epsilon_{ij}$, where $\epsilon_{ij}$ are independent, univariate, normal random variables with a common variance.
%One would like to test whether the hypothesis $\text{H}_0:\alpha_1=\cdots=\alpha_m$ stands.
%However, obtaining a suitable and significant test statistic to reject the null hypothesis merely tells us that the means are indeed not the same, but does not tell us \emph{where exactly} the difference lies.
%Enter the ANOVA decomposition.
This covariate-less model is used to make inferences about the  \emph{treatment means} $\mu_j$.
Often, the model is written in the \emph{overparameterised} form by substituting $\mu_j = \mu + \tau_j$.
This gives a different, arguably better, interpretability to the model: the $\tau_j$'s, referred to as the \emph{treatment effects}, now represent the amount of deviation from the grand, \emph{overall mean} $\mu$.
Estimating all $\tau_j$'s and $\mu$ separately is not possible because there is one degree of freedom that needs to be addressed in the model: there are $p+1$ mean parameters to estimate but only information from $p$ means.
%Knowledge of the overall mean $\mu$ would only allow us to freely ``choose'' only $m-1$ of the treatment effects $\mu_j$, since necessarily $\sum_{j=1}^m n_j\mu = \sum_{j=1}^m n_j\mu_j$.
A common fix to this identification issue is to set one of the $\mu_j$'s, say the first one $\mu_1$, to zero, or impose the restriction $\sum_{j=1}^m \mu_j = 0$.
The former treats one of the $m$ levels as the control, while the latter treats all treatment effects symmetrically.

%The imposition of these restriction corresponds to a particular hypothesis test regarding the group means.
%For instance, to test $\text{H}_0:\mu=\alpha_1=\cdots=\alpha_m$.

Now write the \gls*{anova} model slightly differently, as $y_{i} = f(x_i) + \epsilon_{i}$, where $f$ is defined on the discrete domain $\cX = \{1,\dots,m\}$, and $i$ indexes all of the $n := \sum_{j=1}^m n_j$ observations.
Here, $f$ represents the group-level mean, returning $\mu_j$ for some $j\in\cX$.
In a similar manner, we can perform the \gls*{anova} decomposition on $f$ as
\[
%  f = \myoverbrace{Af}{f_0} +  \myoverbrace{(I - A)f}{f_1},
  f = Af + (I-A)f = f_o + f_t,
\]
where $A$ is an averaging operator that ``averages out'' its argument $x$ and returns a constant, and $I$ is the identity operator.
$f_o = Af$ is a constant function representing the \textit{\underline{o}verall mean}, whereas $f_t = (I - A)f$ is a function representing the \textit{\underline{t}reatment effects} $\tau_j$.
Here are two choices of $A$:
\begin{itemize}
  \item {\boldmath$Af(x) = f(1) = \mu_1$}. This implies $f(x) = f(1) + \big(f(x) - f(1)\big)$. The overall mean $\mu$ is the group mean $\mu_1$, which corresponds to setting the restriction $\mu_1=0$.
  \item {\boldmath$Af(x) = \sum_{x=1}^m f(x) / m =: \bar \alpha$}. This implies $f(x) = \bar \alpha + \big( f(x) - \bar \alpha \big)$. The overall mean is $\mu = \sum_{j=1}^m \alpha_j/m$, which corresponds to the restriction $\sum_{j=1}^m \mu_j = 0$.
\end{itemize}
By definition, $AAf = A^2f = Af$, because averaging a constant returns that constant.
%Side note: this idempotent property of the linear operator $A$ on $f$ speaks to the possibility of it being an \emph{orthogonal projection}, and indeed this is so---we shall return to this point later when we describe functional ANOVA decomposition.
We must have that $Af_t = A(I - A)f = Af - A^2f = 0$.
The choice of A is arbitrary, as is the choice of restriction, so long as it satisfies the condition that $Af_t = 0$.

\index{power set}
\index{operator!averaging}
The multiway ANOVA can be motivated in a similar fashion. 
Let $x = (x_1,\dots,x_p) \in \prod_{k=1}^p \cX_k$, and consider functions that map $\prod_{k=1}^p \cX_k$ to $\bbR$.
Let $A_j$ be an averaging operator on $\cX_k$ that averages the $k$'th component of $x$ from the active argument list, i.e. $A_kf$ is constant on the $\cX_k$ axis but not necessarily an overall constant function.
An ANOVA decomposition of $f$ is
\begin{equation}
  f = \left( \prod_{k=1}^p (A_k + I - A_k) \right)f = \sum_{\cK\in\cP_p} \left( \prod_{k\in\cK} (I - A_k) \prod_{k\notin\cK} A_k \right)f = \sum_{\cK\in\cP_p} f_\cK
\end{equation}
where we had denoted $\cP_p = \cP(\{1,\dots,p\})$ to be the power set of $\{1,\dots,p\}$ whose cardinality is $2^p$.
The summands $f_\cK$ will compose of the overall effect, main effects, two-way interaction terms, and so on.
Each of the terms will satisfy the condition $A_kf_\cK = 0, \forall k \in \cK \in \cP_p \backslash \{\}$.

\index{ANOVA!two-way}
\begin{example}[Two-way ANOVA decomposition]
  Let $p=2$, $\cX_1=\{1,\dots,m_1\}$, and $\cX_2=\{1,\dots,m_2\}$.
  The power set $\cP_2$ is $\big\{ \{\}, \{1\}, \{2\}, \{1,2\} \big\}$.
  The \gls*{anova} decomposition of $f$ (with indices derived trivially from the power set) is
  \vspace{-0.3em}
  \[
    f = f_\emptyset + f_1 + f_2 + f_{12}.\vspace{-0.5em}
  \]
  Here are two choices for the averaging operator $A_k$ analogous to the previous illustration in the one-way \gls*{anova}.
    \begin{itemize}[noitemsep]
    \item Let $A_1f(x) = f(1,x_2)$ and $A_2f(x) = f(x_1,1)$. Then,
    \begin{alignat*}{2}
      f_\emptyset(x) &= A_1A_2 f          &&= f(1,1) \\
      f_1(x) &= (I-A_1)A_2f       &&= f(x_1,1) - f(1,1) \\
      f_2(x) &= A_1(I-A2)f        &&= f(1,x_2) - f(1,1) \\
      f_{12}(x) &= (I-A_1)(I-A2)f &&= f(x_1,x_2) - f(x_1,1) - f(1,x_2) + f(1,1). 
    \end{alignat*}
    
    \item Let $A_kf(x) = \sum_{x_k=1}^{m_k} f(x_1,x_2) / m_k, k=1,2$. Then,
    \begin{alignat*}{2}
      f_\emptyset(x) &= A_1A_2 f          &&= f_{\bigcdot\bigcdot} \\
      f_1(x) &= (I-A_1)A_2f       &&= f_{x_1\bigcdot} - f_{\bigcdot\bigcdot} \\
      f_2(x) &= A_1(I-A_2)f        &&= f_{\bigcdot x_2} - f_{\bigcdot\bigcdot} \\
      f_{12}(x) &= (I-A_1)(I-A_2)f &&= f - f_{x_1\bigcdot} - f_{\bigcdot x_2} + f_{\bigcdot\bigcdot},
    \end{alignat*}
    where $f_{\bigcdot\bigcdot} = \sum_{x_1,x_2} f(x_1,x_2) / m_1m_2$, $ f_{x_1\bigcdot} = \sum_{x_2} f(x_1,x_2)/m_2$, and \newline $f_{\bigcdot x_1} = \sum_{x_1} f(x_1,x_2)/m_1$.
  \end{itemize}
  
  It is also easy to convince ourselves that $A_1f_1 = A_2f_2 = A_1f_{12} = A_2f_{12} = 0$ in either choice of the averaging operator $A_k$.
\end{example}

\subsubsection{Functional ANOVA decomposition}

\index{ANOVA!multiway}
\index{ANOVA!functional}
Let us now extend the ANOVA decomposition idea to a general function $f:\cX\to\bbR$ in some vector space $\cF$.
We shall jump straight into the multiway ANOVA analogue for functional decomposition, and to that end, consider $x=(x_1,\dots,x_p) \in \prod_{k=1}^p \cX_k =: \cX$ a measurable space, where each of the spaces $\cX_k$ has measure $\nu_k$, and $\nu=\nu_1\times\cdots\times\nu_p$ is the product measure on $\cX$.
In the following, denote by $\cF_k$ the vector space of functions over the set $\cX_k$, $k=1,\dots,p$, and $\cF_\emptyset$ the vector space of constant functions.

As $\cX$ need not necessarily be a collection of finite sets, we need to figure out a suitable linear operator that performs an ``averaging'' of some sort.
Consider the linear operator $A_k:\cF\to \cF_{-k}$, where $\cF_{-k}$ is a vector space of functions for which the $k$th component is constant over $\cX$, defined by
\begin{align}\label{eq:avgoper}
  A_k f(x) = \int_{\cX_k} f(x_1,\dots,x_p) \dint \nu_k(x_k).
\end{align}
Thus, for the one-way ANOVA ($p=1$), we get
\begin{align}\label{eq:functionalanova1}
  f(x) = 
  \myoverbrace{\int_\cX f(x)\dint\nu(x)}{f_\emptyset(x)} 
  + 
  \myoverbrace{\left( f - \int_\cX f(x)\dint\nu(x) \right)}{f_1(x)}
\end{align}
and for the two-way ANOVA ($p=2$), we have $f = f_\emptyset + f_1 + f_2 + f_{12}$, with
\begin{align*}
  f_\emptyset(x) ={}& \int_{\cX_1}\int_{\cX_2} f(x_1,x_2) \dint\nu_1(x_1) \dint\nu_2(x_2) \\
  f_1(x) ={}& \int_{\cX_2} \left( f(x_1,x_2) - \int_{\cX_1} f(x_1,x_2) \dint\nu_1(x_1) \right) \dint\nu_2(x_2)\\  
  f_2(x) ={}& \int_{\cX_1} \left( f(x_1,x_2) - \int_{\cX_2} f(x_1,x_2) \dint\nu_2(x_2) \right) \dint\nu_1(x_1)\\  
  f_{12}(x) ={}& f(x_1,x_2) - \int_{\cX_1} f(x_1,x_2) \dint\nu_1(x_1) - \int_{\cX_2} f(x_1,x_2) \dint\nu_2(x_2) \\
  & + \int_{\cX_1}\int_{\cX_2} f(x_1,x_2) \dint\nu_1(x_1) \dint\nu_2(x_2).
\end{align*}

The averaging operator $A_k$ defined in \cref{eq:avgoper} generalises the concept of the previous subsection's averaging operator.
We must then also have, as before, that $A_kf_\cK = 0, \forall k \in \cK \in \cP_p\backslash\{\}$.
For the one-way functional ANOVA decomposition in \cref{eq:functionalanova1}, it must be that $f_1$ is a zero-mean function.
As for the two-way ANOVA, it is the case that $\int_{\cX_k} f_\cK(x_1,x_2) \dint\nu_k(x_k) = 0, k=1,2$, and $\cK \in \big\{ \{1\}, \{2\}, \{1,2\} \big\}$ \citep{durrande2013anova}.

\index{tensor!product space}
This is highly suggestive as to what the ANOVA decomposition of the space $\cF$ should look like in general.
Starting with $p=1$, any $f \in \cF$ can be decomposed as a sum of a constant plus a zero mean function, so we have the
%geometric --- initially it was orthogonal decomposition
decomposition of the vector space $\cF = \cF_\emptyset \oplus \bar\cF_1$, where 
%$\cF_\emptyset$ is a vector space of constant functions, and 
a bar over $\cF_k$, $k=1,\dots,p$ will be used to denote the vector space of zero mean functions over $\cX_k$.
For $p\geq 2$ we can argue something similar.
Take the vector space space $\cF$ of functions over $\prod_{k=1}^p \cX_k$ to be the tensor product space $\cF = \cF_1 \otimes \cdots \otimes \cF_p$ whose elements are identified as being tensor product functions $f_1 \otimes\cdots\otimes f_p$, where each $f_k:\cX_k\to\bbR$ belongs to $\cF_k$.
This is constructed by repeatedly taking the completion of linear combinations of the tensor product $f_k \otimes f_j$, $k,j\in\{1,\dots,p\}$ as per \cref{def:tensprodspace}.
Considered individually, each $\cF_k$ can then be decomposed as $\cF_k = \cF_{\emptyset k} \oplus \bar\cF_k$, where $\cF_{\emptyset k}$ is the space of functions constant along the $k$'th axis.
Expanding out under the distributivity rule of tensor products and rearranging slightly, we obtain
\begin{align}
  \cF ={}& \big( \cF_{\emptyset 1} \oplus \bar\cF_1 \big) \otimes \cdots \otimes 
  \big( \cF_{\emptyset p} \oplus \bar\cF_p \big) \nonumber \\
  ={}& \cF_{\emptyset}
  \ \oplus \
  \bigoplus_{j=1}^p 
  \Big( \bigotimes_{i\neq j}\cF_{\emptyset i} \otimes \bar\cF_j \Big) 
  \ \oplus \
  \mathop{\bigoplus_{j,k=1}^p}_{j<k} 
  \Big( \bigotimes_{i\neq j,k}\cF_{\emptyset i} \otimes \bar\cF_j \otimes \bar\cF_k \Big)
  \label{eq:funcanovaspace} \\
  & \oplus \ 
  \cdots 
  \ \oplus \ 
  \Big( \bar\cF_1 \otimes \cdots \otimes \bar\cF_p \Big), \nonumber
%  \\
%  &= \cF_\emptyset 
%  \mathop{\oplus}^\bot
%  \bigoplus_{j=1}^p \bar\cF_j
%  \mathop{\oplus}^\bot
%  \bigg( \mathop{\bigotimes_{j,k=1}^p}_{j<k} \bar\cF_j\bar\cF_k \bigg)
%  \mathop{\oplus}^\bot \ \cdots \ \mathop{\oplus}^\bot
%  \Big( \bar\cF_1 \otimes \cdots \otimes \bar\cF_p \Big).
\end{align}
where `$\bigoplus$' and `$\bigotimes$' represent the summation and product operator for direct/tensor sums and products, respectively. 
To clarify,
\begin{itemize}
  \item $\cF_{\emptyset}$ is the space of constant functions $f_\emptyset:\cX_1\times\cdots\times\cX_p\to\bbR$;
  \item $\Big( \bigotimes_{i\neq j}\cF_{\emptyset i} \otimes \bar\cF_j \Big)$ is the space of functions that are constant on all coordinates except the $j$'th coordinate of $x$, and the functions are centred on the $j$'th coordinate;
  \item $\Big( \bigotimes_{i\neq j,k}\cF_{\emptyset i} \otimes \bar\cF_j \otimes \bar\cF_k \Big)$ is the space of functions that are constant on all coordinates except the $j$th and $k$th coordinate of $x$, and the functions are centred on these two coordinates;
  \item $\bar\cF_1 \otimes \cdots \otimes \bar\cF_p$ is the space of zero-mean functions $f:\cX_1\times\cdots\times\cX_p\to\bbR$;
\end{itemize}
and so on for for the rest of the spaces in the summand, of which there are $2^p$ members all together. 
Therefore, given an arbitrary function $f\in\cF$, the projection of $f$ onto the above respective 
%orthogonal 
spaces in \cref{eq:funcanovaspace} leads to the \emph{functional ANOVA representation}
\begin{align}\label{eq:functionalanova2}
  f(x) = \alpha + \sum_{j=1}^p f_j(x_j) + \mathop{\sum_{j,k=1}^p}_{j<k} f_{jk}(x_j,x_k) + \cdots + f_{1\cdots p}(x),
\end{align}
where $\alpha$ is the grand intercept (a constant).

\begin{definition}[Functional ANOVA representation]
  Let $\cP_p = \cP(\{1,\dots,p\})$, the power set of $\{1,\dots,p\}$.
  For any function $f \in \cF\equiv\cF_1\otimes\cdots\otimes\cF_p$, with each $\cF_k$ a space of functions over $\cX_k$, $k=1,\dots,p$, the formula for $f$ in \cref{eq:functionalanova2} is known as the \emph{functional ANOVA representation} of $f$ if $\forall k \in \cK \in \cP_p \backslash \{ \}$,
  \begin{align}\label{eq:funcanovaorth}
    A_k f_\cK(x) = \int_{\cX_k} f_\cK(x) \dint\nu_k(x_k) = 0.
  \end{align}
  In other words, the integral of $f_\cK$ with respect to any of the variables indexed by the elements in $\cK$, is zero. 
\end{definition}

For the constant term, main effects, and two-way interaction terms, the familiar classical expressions are obtained:
\begin{align*}
  f_\emptyset &= \int f \dint\nu; \\
  f_j &= \int f \, \textstyle\prod_{i\neq j} \dint\nu_i  - f_\emptyset; \\
  f_{jk} &= \int f \, \textstyle\prod_{i\neq j,k} \dint\nu_i  - f_j - f_k - f_\emptyset.
\end{align*}

\subsubsection{The ANOVA kernel}

At last, we come to the section of deriving the ANOVA RKKS, and, rest assured, the preceding long build-up will prove to not be in vain.
The main idea is to construct an RKKS such that the functions that lie in them will have the ANOVA representation in \cref{eq:functionalanova2}.
The bulk of the work has been done, and in fact we know exactly how this ANOVA RKKS should be structured---it is the space as specified in \cref{eq:funcanovaspace}. 
The ANOVA RKKS will be constructed by a similar manipulation of the individual kernels representing the RKHS building blocks.

\begin{definition}[ANOVA RKKS]\label{def:anovarkks}
  \index{ANOVA!kernel/RKKS}
  For $k=1,\dots,p$, let $\cF_k$ be centred RKHSs of functions over the set $\cX_k$ with kernel $h_k:\cX_k\times\cX_k\to\bbR$. 
  Let $\lambda_k, k=1,\dots,p$ be real-valued scale parameters.
  The ANOVA RKKS of functions $f:\cX_1\times\cdots\times\cX_p\to\bbR$ is specified by the ANOVA kernel, defined by
  \begin{align}\label{eq:anovarkks}
    h_\lambda(x,x') = \prod_{k=1}^p \big( 1 + \lambda_k h_k(x_k,x_k') \big).
  \end{align}
\end{definition}

It is interesting to note that an ANOVA RKKS is constructed very simply through multiplication of univariate kernels.
Expanding out equations \cref{eq:anovarkks}, we see that it is in fact a sum of products of kernels with increasing orders of interaction:
\begin{align*}
  h_\lambda(x,x') 
  ={}& 1 + \sum_{j=1}^p \lambda_j h_j(x_j,x_j') + \mathop{\sum_{j,k=1}^p}_{j<k} \lambda_j\lambda_k h_j(x_j,x_j')h_k(x_k,x_k') \\
  &+ \cdots + \prod_{j=1}^p \lambda_j h_j(x_j,x_j').
\end{align*}
It is now clear from this expansion that the ANOVA RKKS yields functions that resemble those with the ANOVA representation in \cref{eq:functionalanova2}:
the mean value of the function stems from the `1', i.e. it lies in an RKHS of constant functions; the main effects are represented by the sum of the individual kernels; the two-way interaction terms are represented by the second-order kernel interactions; and so on.

%One thing to note is that restricting the $\lambda$ parameters to the positive orthant might give unsatisfactory results---what if the effect of two functions are in truth opposing one another?
%These are handled through opposing signs of their respective scale parameters, thus  the need for working in RKKSs.

\begin{example}
  Consider two RKKSs $\cF_k$ with kernel $\lambda_k h_k$, $k=1,2$.
  The ANOVA kernel defining the ANOVA RKKS $\cF$ is
  \[
    h_\lambda\big((x_1,x_2),(x_1',x_2') \big) = 1 + \lambda_1 h_1(x_1,x_1') + \lambda_2 h_2(x_2,x_2') + \lambda_1\lambda_2 h_1(x_1,x_1')h_2(x_2,x_2').
  \]  
  Suppose that $\cF_1$ and $\cF_2$ are the centred canonical RKKS of functions over $\bbR$.
  Then, functions in $\cF = \cF_0 \oplus \cF_1 \oplus \cF_2 \oplus (\cF_1 \otimes \cF_2)$ are of the form
  \[
    f(x_1,x_2) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2.
  \]
\end{example}

As a remark, not all of the components of the ANOVA RKKS need to be included in the construction.
The selective exclusion of certain interactions characterises many interesting statistical models.
Excluding certain terms of the ANOVA RKKS is equivalent to setting the scale parameter for those relevant components to be zero, i.e. they play no role in the decomposition of the function.
With this in mind, the ANOVA RKKS then gives us an objective way of model-building, from linear regression, to multilevel models, longitudinal models, and so on.

Finally, we note that the functional ANOVA decomposition of an RKKS is orthogonal.
Without loss of generality, assume that all scale parameters are positive.
For $p=1$, we have that $\cF = \cF_\emptyset \oplus \cF_1$, where each of $\cF_\emptyset$ and $\cF_1$ is an RKHS.
From \cref{thm:sumkernels}, the squared norm of $\cF$ is given by
\[
  \norm{f}_\cF^2 = \norm{f_\emptyset}^2_{\cF_\emptyset} + \norm{f_1}^2_{\cF_1},
\]
if the decomposition $f=f_\emptyset + f_1$ is minimal. 
Hence, the decomposition is orthogonal.
An inductive argument can be used to extend and generalise to any $p\geq 2$.
