Key differences:
\begin{enumerate}
  \item Typically no scale parameter is estimated for the kernels in GPR. Instead, the $x$ and $y$ variables are centred \emph{and} scaled before estimating. New data points are then centred and scaled on the mean and s.d. of the training points.
  \item GPR not usually interested in estimating the error precision $\psi$.
  \item The ``go-to'' kernel is the squared exponential kernel or Gaussian radial basis function defined as
  \[
    k(x,x') = \exp(-\sigma\Vert x-x' \Vert^2)
  \]
  $\sigma$ usually chosen by cross-validation or grid-search methods.
\end{enumerate}

\hltodo{Why do we need to estimate scale parameters and error precision in I-prior models?}

\subsection{The Bayesian connection}

The I-prior methodology is less of a fully Bayesian approach and more of an empirical-Bayes approach, whereby an objective using the Fisher information as the covariance matrix of the prior is used to estimate the parameters of the model through maximisation of the likelihood, set up in a RKHS paradigm. However, the I-prior methodology is still this notion of priors and posteriors, something which is arguably Bayesian. Recall the standard linear regression model with independent errors:
\begin{align*}
	\begin{gathered}
		\mathbf y = \boldsymbol\alpha + \mathbf X \boldsymbol\beta + \boldsymbol\epsilon \\
		\boldsymbol\epsilon \sim \text{N}(\mathbf 0, \psi^{-1}\mathbf I_n). \\
	\end{gathered}
\end{align*}
The I-prior method transformed this model into the random effect representation with kernels that we saw earlier in Section \ref{sec:iprioreg}. However, by simply taking the fundamental idea of I-priors, which is a prior with the covariance matrix equal to 
the Fisher information, nothing is really stopping us from estimating this model fully Bayes. We simply need to assign further priors on the intercept and precision, such as
\begin{align*}
	\begin{gathered}
		\text{\underline{Priors}} \\
		\boldsymbol\beta \sim \text{N}(\mathbf 0, \psi\boldsymbol\Lambda \mathbf X ^\top \mathbf X \boldsymbol\Lambda) \\
		\alpha \sim \text{N}(0, 1000) \\
		\psi, \lambda_1^{-2}, \dots, \lambda_p^{-2} \sim \Gamma(0.001, 0.001). \\
	\end{gathered}
\end{align*}
Here, an I-prior with mean zero is chosen. The choices of normal for $\alpha$, and gamma for the scale parameters $\psi$ and a reparameterization of the $\lambda$s is chosen for conjugacy convenience. In the absence of any prior knowledge about the parameters, it is reasonable to choose such hyperparameters to make the priors quite flat and uninformative. Another choice of uninformative prior for the scale parameters would be the \citeauthor{Jeffreys1946}' prior, which is in fact the limit of the gamma distribution as both hyperparameters approach zero. An MCMC approach such as Gibbs or Metropolis-Hastings sampling is then able to estimate this model, and software such as WinBUGS or JAGS are then able to be used.

The main motivation behind I-priors was to guard against over-fitting in cases where model dimensionality is very large relative to sample size. A prior is devised based on an objective principle (of maximum entropy) which brings about simpler estimation while requiring minimal assumptions, as well as model parsimony. A maximum likelihood approach is used to fit I-prior models, which give promising results in terms of predictive abilities from the simulations conducted. In the next section, I-priors will be discussed with a more Bayesian connotation, applied to Bayesian variable selection.
