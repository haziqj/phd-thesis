We present an EM algorithm to estimate the I-probit latent variables $\by^*$ and $\bw$, in which the E-step consists of a mean-field variational approximation of the conditional density $p(\by^*,\bw|\by,\theta) = q(\by^*)q(\bw)$.
As per assumptions $\ref{ass:A4}$, $\ref{ass:A5}$ and $\ref{ass:A6}$, the parameters of the I-probit model consists of $\theta =\{ \balpha = (\alpha_1,\dots,\alpha_m)^\top,\eta \}$.

\begin{figure}[t]
  \centering
  \begin{tikzpicture}[scale=1, transform shape]
    \tikzstyle{main}=[circle, minimum size=10mm, thick, draw=black!80, node distance=16mm]
    \tikzstyle{connect}=[-latex, thick]
    \tikzstyle{box}=[rectangle, draw=black!100]
%      \node[main, draw=black!0] (blank) [xshift=-0.55cm] {};  % pushes image to right slightly
      \node[main, fill=black!10] (x) [] {$x_i$};
      \node[main,double,double distance=0.6mm] (f) [right=of x,yshift=-1.7cm] {$f_{ij}$};
      \node[main] (eta) [below=of x,yshift=-0.7cm] {$\eta$};        
      \node[main] (w) [above=of f,yshift=0.3cm] {$w_{ij}$};  
      \node[main] (ystar) [right=of f,yshift=1.7cm] {$y_{ij}^*$};
      \node[main,double,double distance=0.6mm] (pij) [right=of ystar] {$p_{ij}$};      
      \node[main, fill = black!10] (y) [right=of pij] {$y_{i}$};      
      \node[main] (alpha) [below=of ystar,yshift=-0.75cm] {$\alpha_j$};  
      \node[main, fill=black!10] (Psi) [above=of ystar,yshift=0.4cm] {$\bPsi$};
      \path (alpha) edge [connect] (ystar)
            (eta) edge [connect] (f)
            (x) edge [connect] node [above] {$h$} (f)
    		(f) edge [connect] (ystar)
    		(ystar) edge [connect] node [above] {$g^{-1}$}  (pij)
            (pij) edge [connect] (y)
            (Psi) edge [connect] (w)
            (Psi) edge [connect] (ystar)
    		(w) edge [connect] (f);
      \node[rectangle, draw=black!100, fit={($(x.north west) + (-0.3,0.3cm)$) ($(y.north east) + (0.3,0cm)$) ($(f.south west) + (0,-0.3cm)$) ($(w.north west) + (0,0.3cm)$)}] {}; 
      \node[draw=none] () [below=of y,xshift=-0.3cm,yshift=-0.4cm] {$i=1,\dots,n$};
      \node[rectangle, draw=black!100, fit={($(alpha.south east) + (0,-0.25cm)$) ($(pij.north east) + (0.3,0cm)$) ($(w.north west) + (-0.3,0.58cm)$)  }] {}; 
      \node[draw=none] () [right=of alpha,xshift=-0.4cm,yshift=-0.48cm] {$j=1,\dots,m$};      
    \end{tikzpicture}
    \caption{A DAG of the I-probit model. Observed/fixed nodes are shaded, while double-lined nodes represents calculable quantities.}
\end{figure}

\subsection{The variational E-step}

Let $\tilde q(\by^*,\bw)$ be the pdf that minimises the Kullbeck-Leibler divergence $\KL\big[ q(\by^*,\bw) \Vert p(\by^*,\bw|\by) \big]$ subject to the mean-field constraint $q(\by^*,\bw) = q(\by^*)q(\bw)$.
By appealing to \citet[equation 10.9, p. 466]{bishop2006pattern}, the optimal mean-field variational density $\tilde q$ for the latent variables $\by^*$ and $\bw$ satisfy
\begin{align}
  \log \tilde q(\by^*) &= \E_{\bw\sim\tilde q} \big[ \log p(\by,\by^*,\bw) \big] + \const \\
  \log \tilde q(\bw) &= \E_{\by^*\sim\tilde q} \big[ \log p(\by,\by^*,\bw) \big] + \const 
\end{align}
where $\log p(\by,\by^*,\bw)$ is given by \cref{eq:logjointprobit}.
We now present variational densities $\tilde q(\by^*)$ and $\tilde q(\bw)$.
For further details on the derivation of these densities, please refer to the appendix.

\subsubsection{Variational distribution for the latent propensities \texorpdfstring{$\by^*$}{$y^*$}}

The fact that the rows $\by_{i \bigcdot}^* \in \bbR^m$, $i=1,\dots,n$ of $\by^* \in \bbR^{n \times m}$ are independent can be exploited, and this results in a further induced factorisation $q(\by^*) = \prod_{i=1}^n q(\by_i^*)$.
Define the set $\cC_j = \{y_{ij}^* > y_{ik}^* \,|\, \forall k\neq j \}$.
Then $q(\by_{i \bigcdot}^*)$ is the density of a multivariate normal distribution with mean $\tilde \bmu_{i \bigcdot} = \balpha + \tilde\bw^\top  \bh_\eta(x_i)$, where $\tilde \bw = \E_{\bw\sim\tilde q}\bw$, and variance $\bPsi^{-1}$, subject to a truncation of its components to the set $\cC_{y_i}$.
That is, for each $i=1,\dots,n$ and noting the observed categorical response $y_i \in \{1,\dots,m\}$ for the $i$'th observation, the $\by_i^*$'s are distributed according to
\begin{align}\label{eq:ystardist}
  \by_{i \bigcdot}^* \iid
  \begin{cases}
    \N_m(\tilde\bmu_{i \bigcdot},  \bPsi^{-1}) & \text{ if } y_{iy_i}^* > y_{ik}^*, \forall k \neq y_i \\
    0 & \text{ otherwise}. \\
  \end{cases}
\end{align}
We denote this by $\by_{i \bigcdot}^* \iid \tN(\tilde\bmu_{i \bigcdot}, \bPsi^{-1},\cC_{y_i})$, and the important properties of this distribution are explored in the appendix.

The required expectation $\tilde\by^* := \E_{\by^*\sim\tilde q}\by^*_{i\bigcdot} = \E_{\by^*\sim\tilde q} (y_{i1}^*,\dots, y_{im}^*)^\top$ in the M-step can be tricky to obtain.
One strategy that can be considered is Monte Carlo integration: using samples from $\N_m(\tilde\bmu_{i \bigcdot},  \bPsi^{-1})$, disregard those that do not satisfy the condition $y_{iy_i}^* > y_{ik}^*, \forall k \neq j$, and then take the sample average.
This works reasonably well so long as the truncation region does not fall into the extreme tails of the multivariate normal.
Alternatively, a fast, Gibbs-based approach \citep{robert1995simulation} for sampling from a truncated multivariate normal can be implemented, and this is detailed in the appendix.

If the independent I-probit model is under consideration, whereby the covariance matrix has the independent structure $\bPsi = \diag(\sigma_1^{-2},\dots,\sigma_m^{-2})$, then the first moment  can be considered component-wise. 
Each component of this expectation is given by
\begin{align}\label{eq:ystarupdate}
  \tilde y_{ik}^* =
  \begin{cases}
    \tilde\mu_{ik} - \sigma_k C_i^{-1} \displaystyle{  \int \phi_{ik}(z) \prod_{l \neq k,y_i} \Phi_{il}(z) \phi(z) \dint z }
    &\text{ if } k \neq y_i \\[1.5em]
    \tilde\mu_{iy_i} - \sigma_{y_i} \sum_{k \neq y_i} \big(\tilde y_{ik}^* -  \tilde\mu_{ik} \big) 
    &\text{ if } k = y_i \\
  \end{cases}
\end{align}
with 
\begin{align*}
  \phi_{ik}(Z) &= \phi \left(\frac{\sigma_{y_i}}{\sigma_k} Z + \frac{\tilde\mu_{iy_i} - \tilde\mu_{ik}}{\sigma_k} \right) \\
  \Phi_{ik}(Z) &= \Phi \left(\frac{\sigma_{y_i}}{\sigma_k} Z + \frac{\tilde\mu_{iy_i} - \tilde\mu_{ik}}{\sigma_k} \right) \\
  C_i &= \int \prod_{l \neq j} \Phi_{il}(z) \phi(z) \dint z
\end{align*}
and $Z \sim \N(0,1)$ with pdf and cdf $\phi(\cdot)$ and $\Phi(\cdot)$ respectively. 
The integrals that appear above are functions of a unidimensional Gaussian pdf, and these can be computed rather efficiently using quadrature methods.

\subsubsection{Variational distribution for the I-prior random effects \texorpdfstring{$\bw$}{$w$}}

Given that both $\vecc \by^* | \vecc \bw$ and $\vecc\bw$ are normally distributed as per the model \cref{eq:iprobitmod}, we find that the full conditional distribution $p(\bw|\by^*,\by) \propto p(\by^*,\by,\bw) \propto p(\by^*|\bw)p(\bw)$ is also normal. 
The variational density $q$ for $\vecc \bw \in \bbR^{nm}$ is found to be Gaussian with mean and precision given by
\begin{gather}\label{eq:varipostw}
   \vecc \tilde\bw = \tilde\bV_w 
    (\bPsi \otimes \bH_\eta) \vecc (\tilde\by^* - \bone_n\balpha^\top)
  \hspace{0.5cm}\text{and}\hspace{0.5cm} 
  \tilde \bV_w^{-1} = \bPsi \otimes \bH_\eta^2 + \bPsi^{-1} \otimes \bI_n = \bV_{y^*}.
\end{gather}
As a computational remark, computing the inverse $\tilde\bV_w^{-1}$ presents a challenge, as this takes $O(n^3m^3)$ time if computed na√Øvely. 
By exploiting the Kronecker product structure in $\tilde\bV_w$, we are able to efficiently compute the required inverse in roughly $O(n^3m)$ time---see the \hltodo{Section X} for details.

If the independent I-probit model is assumed, i.e. $\bPsi = \diag(\psi_1,\dots,\psi_m)$, then the posterior covariance matrix $\tilde\bV_w$ has a simpler structure which implies column independence in the matrix $\bw$.
By writing $\bw_{\bigcdot j} = (w_{1j},\dots,w_{nj})^\top \in \bbR^n$, $j=1,\dots,m$, to denote the column vectors of $\bw$, and with a slight abuse of notation, we have that
\begin{align*}
  \N_{nm}(\vecc \bw|\vecc\wtilde, \tilde\bV_w) 
  = \prod_{j=1}^m \N_{n}(\bw_{\bigcdot j}|\tilde\bw_{\bigcdot j}, \tilde\bV_{w_j}),
\end{align*}
where 
\[
  \tilde \bw_{\bigcdot j} = \psi_j\tilde \bV_{w_j}\bH_\eta (\tilde\by^*_j - \alpha_j\bone_n) \ \text{ and } \ \tilde \bV_{w_j} = \big(\psi_j\bH_{\eta}^2 + \psi_j^{-1}\bI_n \big)^{-1}.
\]
We note the similarity between \cref{eq:varipostw} above and the posterior distribution for the I-prior random effects in a normal model \cref{eq:posteriorw} seen in the previous chapter, with the difference being \cref{eq:varipostw} uses the continuous latent propensities $\by^*$ instead of the the observations $\by$.
The consequence of this is that the posterior regression functions are class independent, the exact intended effect by specifying a diagonal precision matrix $\bPsi$.

\begin{remark}
  The variational distribution $q(\bw)$ which approximates $p(\bw|\by)$ is in fact exactly $p(\bw|\by^*)$, the conditional density of the I-prior random effects given the latent propensities.
  By the law of total expectations, 
  \begin{gather*}
    \E[r(\bw)|\by] = \E_{\by^*}\big[ \E[r(\bw)|\by^*] \,|\, \by \big],
  \end{gather*}
  where $r(\cdot)$ is some function of $\bw$, and expectations are taken under the posterior distribution of $\by^*$.
  Hypothetically, if the true pdf $p(\by^*|\by)$ were tractable,  then the E-step can be computed using the true conditional distribution.
  Since it is not tractable, we resort to a
\end{remark}



\subsection{The M-step}
%\subsection{Kernel parameters $\eta$}
\label{sec:varupdeta}

The posterior density $q$ involving the kernel parameters is of the form
\begin{align*}
  \log q(\eta) 
  &=  -\half\tr\E_{\cZ\backslash\{\eta\}\sim q} \Big[ 
  (\by^* - \bone_n\balpha^\top - \bH_\eta\bw) \bPsi (\by^* - \bone_n\balpha^\top - \bH_\eta\bw)^\top \Big] + \log p(\eta) \\
  &\phantom{==} + \const
\end{align*}
where $p(\eta)$ is an appropriate prior density for $\eta$.
Generally, samples $\eta^{(1)},\dots,\eta^{(T)}$ from $\tilde q(\eta)$ may be obtained using a Metropolis algorithm, so that quantities such as $\tilde\bH_{\eta} = \E_{\eta \sim q} \bH_{\eta}$ and the like may be approximated using $\frac{1}{T}\sum_{t=1}^T \bH_{\eta^{(t)}}$.
Details of the Metropolis sampler is available in the appendix.

When only RKHS scale parameters are involved, then the distribution $q$ can be found in closed-form, much like in the exponential family EM algorithm described in \cref{sec:expfamEM}.
Under the same setting as in that subsection, assume that only $\eta = \{\lambda_1,\dots,\lambda_p\}$ need be estimated, and for each $k=1,\dots,p$, we can decompose the kernel matrix as $\bH_\eta = \lambda_k \bR_k + \bS_k$ and its square as $\bH_\eta^2 = \lambda_k^2 \bR_k^2 + \lambda_k \bU_k + \bS_k^2$.
Additionally, we impose a further mean-field restriction on $q(\eta)$, i.e., $q(\eta) = \prod_{k=1}^p p(\lambda_k)$.
Then, by using independent and identical normal priors on the $\lambda_k$'s, such as the one listed at the beginning of this section, we find that $q(\lambda_k)$ is the density of a normal distribution with mean $d_kc_k^{-1}$ and variance $c_k^{-1}$, where
\begin{gather*}
  c_k = \tr\big(\bPsi\E[\bw^\top\bR_k^2\bw]\big) + v_\lambda^{-2} \\
  \hspace{0.5cm}\text{and}\hspace{0.5cm} \\
  d_k = \tr\Big(\bPsi(\tilde\by^* - \bone_n\tilde\balpha^\top)^\top\bR_k \tilde\bw 
  - \half\bPsi\E[\bw^\top \bU_k \bw] \Big). 
\end{gather*}
For a method of evaluating quantities such as $\tr(\bC\E[\bw^\top\bD\bw ])$ for suitably sized matrices $\bC$ and $\bD$, refer to the appendix.

%\subsection{Error precision \texorpdfstring{$\bPsi$}{$\Psi$}}
%
%Our derivations indicate that, in both the full and the independent I-probit model, the variational density $q(\bPsi)$ is not of recognisable form:
%\begin{align*}
%  \log q(\bPsi)
%  &= \E_{\cZ\backslash\{\bPsi\}\sim q} \Big[ 
%%  \half[n]\log\abs{\bPsi} 
%  - \half\tr \big((\by^* - \bmu)^\top (\by^* - \bmu)\bPsi  \big)
%%  - \half[n]\log\abs{\bPsi} 
%  - \half\tr \big( \bw^\top\bw\bPsi^{-1} \big)
%  \Big] \\
%  &\phantom{==} + p(\bPsi) + \const 
%\end{align*}
%This stems from the unique parameterisation of the I-prior model, in which both the error precision $\bPsi$ and the variance $\bPsi^{-1}$ come into play.

%A small reparameterisation of the I-prior random effects is necessary to achieve conjugacy for the $\bPsi$ parameter.
%Let $\bu\in\bbR^{n\times m}$ be a matrix defined by $\bPsi^{-1}\bw$.
%Then $\bu \sim \MN_{n,m}(\bzero, \bI_n, \bPsi^{-1})$ a priori.
%From \cref{eq:varipostw}, the posterior for $\vecc \bu$ is normal with mean $\vecc \tilde\bu = \vecc (\tilde\bw\tilde\bPsi^{-1} )$ and variance
%\begin{align*}
%  \tilde \bV_u
%  &= (\tilde\bPsi^{-1} \otimes \bI_n) \tilde \bV_w (\tilde\bPsi^{-1} \otimes \bI_n). 
%\end{align*}
%%Note that the dependence of $q(\bu)$ on the parameter of interest $\bPsi$ is not an issue.
%In essence, this reparameterisation simply introduces an additional step in the CAVI algorithm.
%
%With a Wishart prior on the precision matrix $\bPsi\sim\Wis_m(\bG,g)$, the mean-field variational density for $\bPsi$ is found to satisfy
%\begin{align*}
%  \log \tilde q(\bPsi)
%  &= \const - \half \sum_{i=1}^n \tr \left( 
%  \big(\bG_1 + \bG_2 + \bG \big)\bPsi 
%  \right) + \frac{2n + g - (m+1)}{2} \log \abs{\bPsi}
%\end{align*}
%which is recognised as the log density of a Wishart distribution with scale matrix $\tilde\bG := \bG_1 + \bG_2 + \bG$ and $\tilde g = 2n + g$ degrees of freedom, where
%\begin{align}
%  \begin{gathered}
%  \bG_1 = \E \left[ 
%  (\by^* - \bone_n\balpha^\top - \bH_\eta\bw)
%  (\by^* - \bone_n\balpha^\top - \bH_\eta\bw)^\top 
%  \right]   \\
%  \bG_2 = \E [\bu^\top \bu ].
%  \end{gathered}
%\end{align}
%The challenge here is that this distribution involves the second posterior moment of the conically truncated multivariate normal distribution for $\by^*$, among other things. 
%This is slightly awkward to calculate analytically, although sampling methods provide a reasonable way out.
%
%Consider now the independent I-probit model for which $\bPsi = \diag(\psi_1,\dots,\psi_m)$ with independent gamma priors on the $\psi_j$'s.
%The posterior for $\bPsi$ is of a similar factorised form, namely $ q(\bPsi) = \prod_{j=1}^m  q(\psi_j)$, where each $ q(\psi_j)$ is the pdf of a gamma distribution with shape and rate parameters $\tilde s = 2n+s-1$ and $\tilde r = \half \E\norm{\by_{\bigcdot j}^* - \alpha_j\bone_n - \bH_\eta\bw_{\bigcdot j}}^2 + \half \E\norm{\bu_{\bigcdot j}}^2  + r$ respectively.
%
%As a remark, the fact that both parameterisations of the I-prior random effects $\bw$ and $\bu$ are used seems suspect. 
%Because of the way $\bu$ was defined, there is a linear dependence between the two sets of parameters.

\subsection{Intercepts \texorpdfstring{$\balpha$}{$\alpha$}}

Finally, the posterior distribution for the intercepts follow a normal distribution with the normal priors specified earlier.
The posterior mean and variance for the intercepts are given by $\tilde\balpha = \tilde \bA^{-1}\tilde \ba$ and $\tilde \bA^{-1}$ respectively, where
\[
  \tilde\ba = \sum_{i=1}^n \bPsi\big( \tilde\by^*_{i \bigcdot} -\tilde\bw^\top\tilde\bh_\eta(x_i)\big)
  \hspace{0.5cm}\text{and}\hspace{0.5cm}
  \tilde\bA = n\bPsi + v_\alpha \bI_m.
\]
If $\bPsi$ is diagonal, the components of $\balpha$ would be independent, and each would be distributed according to
\[
  \N\left( 
  \frac{\psi_j\sum_{i=1}^n (\tilde y^*_{ij} -  \tilde f_{ij})^2}{n\psi_j + v_\alpha^{-1}},
  \frac{1}{n\psi_j + v_\alpha^{-1}} 
  \right).
\]
Here, we used the notation $\tilde f_{ij}$ to mean the $(i,j)$'th element of 
$\E[\bH_\eta\bw] = \tilde\bH_\eta \tilde \bw \in \bbR^{n\times m}$.
Note that it is necessary, as discussed earlier, that $\sum_{j=1}^m\alpha_j = 0$ for identifiability.

\subsection{The CAVI algorithm}

One will have noticed that the evaluation of each component of the posterior depends on knowing the posterior distribution of the rest of the components.
This circular dependence is dealt with by way of an iterative updating scheme of the components.
Using an arbitrary starting value, each component is updated in turn according to the above derivations, until a maximum number of iterations is reached, or ideally, until a convergence criterion is met.
In variational inference, the ELBO is used to asses convergence.
The expression for the ELBO for the I-probit model is derived in the appendix.
The CAVI algorithm for the I-probit model is summarised in \cref{alg:caviiprobit}.

Similar to the EM algorithm, each iteration of the algorithm increases the ELBO to a stationary point \citep{blei2017variational}---hence the name coordinate ascent variational inference (CAVI).
Unlike the EM algorithm though, the CAVI algorithm does \emph{not} guarantee an increase in the marginal log-likelihood at each step, nor does it guarantee convergence to the global maxima of the log-likelihood.

Further, the ELBO expression to be maximised is often not convex, which means the CAVI algorithm may terminate at local modes, for which they may be many.
Note that the variational distribution with the higher ELBO value is the distribution that is closer, in terms of the KL divergence, to the true posterior distribution.
In our experience, multiple random starts alleviates this issue for the I-probit model.

\algrenewcommand{\algorithmiccomment}[1]{{\color{gray} \hfill $\triangleright$ #1}}
\begin{algorithm}[hbt]
\caption{CAVI for the I-probit model}\label{alg:caviiprobit}
\begin{algorithmic}[1]
  \Procedure{Initialisation}{}
%    \State Hyperpriors $v_\alpha$
    \State Initialise $\tilde\by^{*(0)},\tilde\bw^{(0)},\tilde\balpha^{(0)},\tilde\bH_{\eta^{(0)}}, \bPsi$
    \State $t \gets 0$
  \EndProcedure 
  \Statex
  \While{not converged}{}
    \For{$i=1,\dots,n$} \Comment{Update $\by^*$}
      \State $q^{(t+1)}(\by^*_{i \bigcdot}) \gets \tN_m\big(\tilde\balpha^{(t)} + \tilde\bw^{(t)\top} \tilde\bh_{\eta^{(t)}}(x_i), \bPsi, \cC_{y_i}\big)$ 
      \State $\tilde \by^{*(t+1)}_{i \bigcdot} \gets \E_{q^{(t+1)}}[\by^*_{i \bigcdot}]$
    \EndFor
    \Statex
    \State $\bV_w^{(t+1)} \gets \big((\bPsi \otimes \tilde\bH_{\eta^{(t)}}^2) + (\bPsi^{-1} \otimes \bI_n)\big)^{-1}$ \Comment{Update $\bw$}
    \State $\tilde\bw^{(t+1)}  \gets \tilde\bV_w^{(t+1)} 
    (\bPsi \otimes \tilde\bH_{\eta^{(t)}}) \vecc (\tilde\by^{*(t+1)} - \bone_n\tilde\balpha^{(t)\top})$
    \State $q^{(t+1)}(\bw) \gets \N_{nm}\big( \tilde\bw^{(t+1)}, \bV_w^{(t+1)} \big)$ 
    \Statex
    \State Update $q^{(t+1)}(\eta)$ as per \cref{sec:varupdeta} \Comment{Update $\eta$}
    \State Sample $\eta^{[1]},\dots,\eta^{[T]} \sim q^{(t+1)}(\eta)$
    \State $\tilde\bH_{\eta^{(t+1)}} \gets \frac{1}{T}\sum_{i=1}^T \bH_{\eta^{[i]}}$ and $\tilde\bH_{\eta^{(t+1)}}^2 \gets \frac{1}{T}\sum_{i=1}^T \bH_{\eta^{[i]}}^2$
    \Statex
    \State $\tilde\balpha^{(t+1)} \gets \frac{1}{n} \sum_{i=1}^n \big( \tilde\by^{*(t+1)}_{i \bigcdot} - \tilde\bw^{(t+1)\top} \tilde\bh_{\eta^{(t+1)}}(x_i) \big)$ \Comment{Update $\balpha$}
    \State $q^{(t+1)}(\balpha) \gets \N_{m}\big( \tilde\balpha^{(t+1)}, \frac{1}{n}\bPsi^{-1} \big)$
    \Statex
    \State Calculate ELBO $\cL^{(t+1)}$
    \State $t \gets t + 1$
  \EndWhile
\end{algorithmic}
\end{algorithm}
