We present a variational inference algorithm to estimate the I-probit latent variables $\by^*$ and $\bw$, together with the parameters $\theta = \{\balpha = (\alpha_1,\dots,\alpha_m)^\top,\eta,\bPsi \}$.
Begin by assuming some prior distribution on the parameters $p(\theta) = p(\balpha)p(\eta)p(\bPsi)$. 
The following flat, uninformative priors are suggested:
\begin{itemize}
  \item \textbf{Kernel parameters $\eta$}. This may include parameters such as the Hurst index, lengthscale and offset parameters, in addition to the RKHS scale parameters $\lambda_1,\dots,\lambda_p$, and each with their own support.
  For the scale parameters, assign each $\lambda_k$ the vague prior
  \[
    \lambda_k \iid \N(0,v_\lambda = 0.001^{-1}), \ k=1,\dots,p.
  \]
  As $v_k^{-1}\to 0$, the prior becomes $p(\lambda_k)\propto \const$, an improper prior.
  The default choice for the rest of the kernel parameters is an improper prior $p(\eta) \propto \const$
  \item \textbf{Error precision $\bPsi$}. For the full I-probit model,
  \[
    \bPsi \sim \Wis(\bG,g)
  \]
  with known scale matrix $\bG = \diag(0.001,\dots,0.001)$ and degrees of freedom $g=m+1.001$. 
  This implies a vague gamma prior for the precisions, and a near uniform prior for the off-diagonal elements \citep{alvarez2014bayesian}.
  For the independent I-probit model such that $\bPsi = \diag(\psi_1,\dots,\psi_m)$,
  \[
    \psi_j \iid \Gamma(s,r), \ j=1,\dots,m,
  \]
  with vague shape $s = 0.001$ and rate $r = 0.001$ parameters. Note that as $s,r \to 0$ then $p(\psi_j) \propto \psi_j^{-1}$, an improper Jeffreys' prior for scale parameters.
  \item \textbf{Intercepts $\alpha_1,\dots,\alpha_m$}. Also assign a vague normal prior for each intercept
  \[
    \alpha_j \iid \N(0,v_\alpha=0.001^{-1}).
  \]
\end{itemize}
Although one may devote more attention to the prior specification of these parameters, for our purposes it suffices that they are independent component-wise, and that they are conjugate priors for the complete conditional density $p(\theta|\by,\by^*,\bw)$.
%The exponential family requirement greatly eases the complexity of deriving the variational algorithm later on\footnote{
%Of interest, one may even opt to assign improper priors on $\theta$ and the algorithm would still work.
%This is akin to obtaining empirical Bayes estimate of the $\theta$ if seen from an EM algorithm standpoint.
%}.

The posterior density of $\cZ = \{\by^*,\bw,\theta \}$ is approximated by a mean-field variational density $q$, i.e.
\[
  p(\by^*,\bw,\theta|\by) = \tilde q(\by^*)q(\bw)q(\theta).
\]
Additionally, we assume independence among the components of $\theta$ so that $q(\theta) = \prod_k q(\theta_k)$.
We now present the mean-field variational distributions for each of unknowns in $\cZ$.
On notation: we will typically refer to posterior means of the parameters $\by^*$, $\bw$, $\theta$ and so on by the use of a tilde.
For instance, we write $\tilde\bw$ to mean $\E_{\bw \sim \tilde q}[\bw]$, the expected value of $\bw$ under the pdf $\tilde q(\bw)$.
The distributions are simply stated, but a full derivation is given in the appendix.

\begin{figure}[t]
  \centering
  \begin{tikzpicture}[scale=1, transform shape]
    \tikzstyle{main}=[circle, minimum size=10mm, thick, draw=black!80, node distance=16mm]
    \tikzstyle{connect}=[-latex, thick]
    \tikzstyle{box}=[rectangle, draw=black!100]
%      \node[main, draw=black!0] (blank) [xshift=-0.55cm] {};  % pushes image to right slightly
      \node[main, fill=black!10] (x) [] {$x_i$};
      \node[main,double,double distance=0.6mm] (f) [right=of x,yshift=-1.7cm] {$f_{ij}$};
      \node[main] (eta) [below=of x,yshift=-0.7cm] {$\eta$};        
      \node[main] (w) [above=of f,yshift=0.3cm] {$w_{ij}$};  
      \node[main] (ystar) [right=of f,yshift=1.7cm] {$y_{ij}^*$};
      \node[main,double,double distance=0.6mm] (pij) [right=of ystar] {$p_{ij}$};      
      \node[main, fill = black!10] (y) [right=of pij] {$y_{i}$};      
      \node[main] (alpha) [below=of ystar,yshift=-0.75cm] {$\alpha_j$};  
      \node[main] (Psi) [above=of ystar,yshift=0.4cm] {$\bPsi$};
      \path (alpha) edge [connect] (ystar)
            (eta) edge [connect] (f)
            (x) edge [connect] node [above] {$h$} (f)
    		(f) edge [connect] (ystar)
    		(ystar) edge [connect] node [above] {$g^{-1}$}  (pij)
            (pij) edge [connect] (y)
            (Psi) edge [connect] (w)
            (Psi) edge [connect] (ystar)
    		(w) edge [connect] (f);
      \node[rectangle, draw=black!100, fit={($(x.north west) + (-0.3,0.3cm)$) ($(y.north east) + (0.3,0cm)$) ($(f.south west) + (0,-0.3cm)$) ($(w.north west) + (0,0.3cm)$)}] {}; 
      \node[draw=none] () [below=of y,xshift=-0.3cm,yshift=-0.4cm] {$i=1,\dots,n$};
      \node[rectangle, draw=black!100, fit={($(alpha.south east) + (0,-0.25cm)$) ($(pij.north east) + (0.3,0cm)$) ($(w.north west) + (-0.3,0.58cm)$)  }] {}; 
      \node[draw=none] () [right=of alpha,xshift=-0.4cm,yshift=-0.48cm] {$j=1,\dots,m$};      
    \end{tikzpicture}
    \caption{A DAG of the I-probit model. Observed nodes are shaded, while double-lined nodes represents calculable quantities.}
\end{figure}

\subsection{Latent propensities \texorpdfstring{$\by^*$}{$y^*$}}

The fact that the rows $\by_{i \bigcdot}^* \in \bbR^m$, $i=1,\dots,n$ of $\by^* \in \bbR^{n \times m}$ are independent can be exploited, which yields an induced factorisation $q(\by^*) = \prod_{i=1}^n q(\by_i^*)$.
Define the set $\cC_j = \{y_{ij}^* > y_{ik}^* \,|\, \forall k\neq j \}$.
Then $q(\by_{i \bigcdot}^*)$ is the density of a multivariate normal distribution with mean $\tilde\bmu_{i \bigcdot} = \tilde\balpha + \tilde\bw^\top \tilde \bh_\eta(x_i)$, and variance $\tilde \bPsi^{-1}$ subject to the truncation of its components to the set $\cC_{y_i}$.
That is, for each $i=1,\dots,n$ and noting the observed value $y_i \in \{1,\dots,m\}$, the $\by_i^*$'s are distributed according to
\begin{align}\label{eq:ystardist}
  \by_{i \bigcdot}^* \iid
  \begin{cases}
    \N_m(\tilde\bmu_{i \bigcdot}, \tilde \bPsi^{-1}) & \text{ if } y_{iy_i}^* > y_{ik}^*, \forall k \neq y_i \\
    0 & \text{ otherwise}. \\
  \end{cases}
\end{align}
We denote this by $\by_{i \bigcdot}^* \iid \tN(\tilde\bmu_{i \bigcdot},\tilde \bPsi^{-1},\cC_{y_i})$, and the important properties of this distribution are explored in the appendix.

The required expectations $\E\by_{i \bigcdot}^* = \E (y_{i1}^*,\dots,y_{im}^*)^\top$ are tricky to compute.
One strategy might be Monte Carlo integration: using samples from $\N_m(\tilde\bmu_{i \bigcdot}, \tilde \bPsi^{-1})$, disregard those that do not satisfy the condition $y_{iy_i}^* > y_{ik}^*, \forall k \neq j$, and then take the sample average.
This works reasonably well so long as the truncation region does not fall into the extreme tails of the multivariate normal.
Alternatively, a fast, Gibbs based approach to estimating the mean or any other quantity $\E r(\by_{i \bigcdot}^*)$ can be implemented, and this is detailed in the appendix.

If the independent I-probit model is considered, where the covariance matrix has the independent structure $\tilde\bPsi = \diag(\tilde\sigma_1^{-2},\dots,\tilde\sigma_m^{-2})$, then the expected value can be considered component-wise, and each component of this expectation is given by
\begin{align}\label{eq:ystarupdate}
  \tilde y_{ik}^* =
  \begin{cases}
    \tilde\mu_{ik} - \tilde\sigma_k C_i^{-1} \displaystyle{  \int \phi_{ik}(z) \prod_{l \neq k,j} \Phi_{il}(z) \phi(z) \dint z }
    &\text{ if } k \neq y_i \\[1.5em]
    \tilde\mu_{iy_i} - \tilde\sigma_{y_i} \sum_{k \neq y_i} \big(\tilde y_{ik}^* - \tilde f_{ik} \big) 
    &\text{ if } k = y_i \\
  \end{cases}
\end{align}
with 
\begin{align*}
  \phi_{ik}(Z) &= \phi \left(\frac{\tilde\sigma_{y_i}}{\tilde\sigma_k} Z + \frac{\tilde\mu_{iy_i} - \tilde\alpha_k - \tilde f_{ik}}{\tilde\sigma_k} \right) \\
  \Phi_{ik}(Z) &= \Phi \left(\frac{\tilde\sigma_{y_i}}{\tilde\sigma_k} Z + \frac{\tilde\mu_{iy_i} - \tilde\alpha_k - \tilde f_{ik}}{\tilde\sigma_k} \right) \\
  C_i &= \int \prod_{l \neq j} \Phi_{il}(z) \phi(z) \dint z
\end{align*}
and $Z \sim \N(0,1)$ with pdf and cdf $\phi(\cdot)$ and $\Phi(\cdot)$ respectively. 
The integrals that appear above are functions of a unidimensional Gaussian pdf, and these can be computed rather efficiently using quadrature methods.

\subsection{I-prior random effects \texorpdfstring{$\bw$}{$w$}}

Given that both $\vecc \by^* | \vecc \bw$ and $\vecc\bw$ are normally distributed, we find that the conditional posterior distribution $p(\bw|\cZ_{-\bw},\by)$ is also normal, and therefore the approximate posterior density $q$ for $\vecc \bw \in \bbR^{nm}$ is also normal with mean and precision given by
\begin{gather}\label{eq:varipostw}
   \vecc \tilde\bw = \tilde\bV_w 
    (\tilde\bPsi \otimes \tilde\bH_\eta) \vecc (\tilde\by^* - \bone_n\tilde\balpha^\top)
  \hspace{0.5cm}\text{and}\hspace{0.5cm} 
  \tilde \bV_w^{-1} = (\tilde\bPsi \otimes \tilde\bH_\eta^2) + (\tilde\bPsi^{-1} \otimes \bI_n).
\end{gather}
We note the similarity between \cref{eq:varipostw} above and the posterior distribution for the I-prior random effects in a normal model \cref{eq:posteriorw} seen in the previous chapter.
Na√Øvely computing the inverse $\tilde\bV_w^{-1}$ presents a computational challenge, as this takes $O(n^3m^3)$ time. 
By exploiting the Kronecker product structure in $\tilde\bV_w$, we are able to efficiently compute the required inverse in roughly $O(n^3m)$ time---see the appendix for details.
%Equivalently, we can express the distribution for $\bw \sim \tilde q$ as a matrix normal distribution
%\begin{align}\label{eq:varipostw2}
%  \MN_{nm}\Big(\,
%  \greyoverbrace{\tilde\bH_\eta^{-1}(\tilde\by^* - \bone_n\tilde\balpha^\top)\tilde\bPsi^2}{\tilde\bw} ,\, 
%  \tilde\bH_\eta^{-2},\,
%  \tilde\bPsi  
%  \Big).
%\end{align}

If the independent I-probit model is assumed, i.e. $\tilde\bPsi = \diag(\tilde\psi_1,\dots,\tilde\psi_m)$, then the posterior covariance matrix $\tilde\bV_w$ has a simpler structure:  random matrix $\bw$ will have columns which are independent of each other.
By writing $\bw_{\bigcdot j} = (w_{1j},\dots,w_{nj})^\top \in \bbR^n$, $j=1,\dots,m$, to denote the column vectors of $\bw$ and with a slight abuse of notation, we have that
\begin{align*}
  \N_{nm}(\vecc \bw|\vecc\wtilde, \tilde\bV_w) 
  = \prod_{j=1}^m \N_{n}(\bw_{\bigcdot j}|\tilde\bw_{\bigcdot j}, \tilde\bV_{w_j}),
\end{align*}
where 
\[
  \tilde \bw_{\bigcdot j} = \tilde\psi_j\tilde \bV_{w_j}\tilde\bH_\eta (\tilde\by^*_j - \tilde\alpha_j\bone_n) \ \text{ and } \ \tilde \bV_{w_j} = \big(\tilde\psi_j\tilde\bH_{\eta}^2 + \tilde\psi_j^{-1}\bI_n \big)^{-1}.
\]
The consequence of this is that the posterior regression functions are class independent, the exact intended effect by specifying a diagonal precision matrix $\bPsi$.
%Computationally and algebraically, this simplifies matters as estimation for each class can be conducted separately.

\subsection{Kernel parameters $\eta$}

The posterior density $q$ involving the kernel parameters is of the form
\begin{align*}
  \log q(\eta) 
  &=  -\half\tr\E_{\cZ\backslash\{\eta\}\sim q} \Big[ 
  (\by^* - \bone_n\balpha^\top - \bH_\eta\bw) \bPsi (\by^* - \bone_n\balpha^\top - \bH_\eta\bw)^\top \Big] + \log p(\eta) \\
  &\phantom{==} + \const
\end{align*}
where $p(\eta)$ is an appropriate prior density for $\eta$.
Generally, samples $\eta^{(1)},\dots,\eta^{(T)}$ from $\tilde q(\eta)$ may be obtained using a Metropolis algorithm, so that quantities such as $\tilde\bH_{\eta} = \E_{\eta \sim q} \bH_{\eta}$ and the like may be approximated using $\frac{1}{T}\sum_{t=1}^T \bH_{\eta^{(t)}}$.
Details of the Metropolis sampler is available in the appendix.

When only RKHS scale parameters are involved, then the distribution $q$ can be found in closed-form, much like in the exponential family EM algorithm described in \hltodo{Section 4.3.3}.
Under the same setting as in that subsection, assume that only $\eta = \{\lambda_1,\dots,\lambda_p\}$ need be estimated, and for each $k=1,\dots,p$, we can decompose the kernel matrix as $\bH_\eta = \lambda_k \bR_k + \bS_k$ and its square as $\bH_\eta^2 = \lambda_k^2 \bR_k^2 + \lambda_k \bU_k + \bS_k^2$.
Additionally, we impose a further mean-field restriction on $q(\eta)$, i.e., $q(\eta) = \prod_{k=1}^p p(\lambda_k)$.
Then, by using independent and identical normal priors on the $\lambda_k$'s, such as the one listed at the beginning of this section, we find that $q(\lambda_k)$ is the density of a normal distribution with mean $d_kc_k^{-1}$ and variance $c_k^{-1}$, where
\begin{gather*}
  c_k = \tr\big(\tilde\bPsi\E[\bw^\top\bR_k^2\bw]\big) + v_\lambda^{-2} \\
  \hspace{0.5cm}\text{and}\hspace{0.5cm} \\
  d_k = \tr\Big( \tilde\bPsi(\tilde\by^* - \bone_n\tilde\balpha^\top)^\top\bR_k \tilde\bw 
  - \half\tilde\bPsi\E[\bw^\top \bU_k \bw] \Big). 
\end{gather*}
For a method of evaluating quantities such as $\tr(\bC\E[\bw^\top\bD\bw ])$ for suitably sized matrices $\bC$ and $\bD$, refer to the appendix.

\subsection{Error precision \texorpdfstring{$\bPsi$}{$\Psi$}}

A small reparameterisation of the I-prior random effects is necessary to achieve conjugacy for the $\bPsi$ parameter.
Let $\bu\in\bbR^{n\times m}$ be a matrix defined by $\bPsi^{-1}\bw$.
Then $\bu \sim \MN_{n,m}(\bzero, \bI_n, \bPsi^{-1})$ a priori.
From \cref{eq:varipostw}, the posterior for $\vecc \bu$ is normal with mean $\vecc \tilde\bu = \vecc (\tilde\bw\tilde\bPsi^{-1} )$ and variance
\begin{align*}
  \tilde \bV_u
  &= (\tilde\bPsi^{-1} \otimes \bI_n) \tilde \bV_w (\tilde\bPsi^{-1} \otimes \bI_n). 
\end{align*}
%Note that the dependence of $q(\bu)$ on the parameter of interest $\bPsi$ is not an issue.
In essence, this reparameterisation simply introduces an additional step in the CAVI algorithm.

With a Wishart prior on the precision matrix $\bPsi\sim\Wis_m(\bG,g)$, the mean-field variational density for $\bPsi$ is found to satisfy
\begin{align*}
  \log \tilde q(\bPsi)
  &= \const - \half \sum_{i=1}^n \tr \left( 
  \big(\bG_1 + \bG_2 + \bG \big)\bPsi 
  \right) + \frac{2n + g - (m+1)}{2} \log \abs{\bPsi}
\end{align*}
which is recognised as the log density of a Wishart distribution with scale matrix $\tilde\bG := \bG_1 + \bG_2 + \bG$ and $\tilde g = 2n + g$ degrees of freedom, where
\begin{align}
  \begin{gathered}
  \bG_1 = \E \left[ 
  (\by^* - \bone_n\balpha^\top - \bH_\eta\bw)
  (\by^* - \bone_n\balpha^\top - \bH_\eta\bw)^\top 
  \right]   \\
  \bG_2 = \E [\bu^\top \bu ].
  \end{gathered}
\end{align}
The challenge here is that this distribution involves the second posterior moment of the conically truncated multivariate normal distribution for $\by^*$, among other things. 
This is slightly awkward to calculate analytically, although sampling methods provide a reasonable way out.

Consider now the independent I-probit model for which $\bPsi = \diag(\psi_1,\dots,\psi_m)$ with independent gamma priors on the $\psi_j$'s.
The posterior for $\bPsi$ is of a similar factorised form, namely $ q(\bPsi) = \prod_{j=1}^m  q(\psi_j)$, where each $ q(\psi_j)$ is the pdf of a gamma distribution with shape and rate parameters $\tilde s = 2n+s-1$ and $\tilde r = \half \E\norm{\by_{\bigcdot j}^* - \alpha_j\bone_n - \bH_\eta\bw_{\bigcdot j}}^2 + \half \E\norm{\bu_{\bigcdot j}}^2  + r$ respectively.

As a remark, the fact that both parameterisations of the I-prior random effects $\bw$ and $\bu$ are used seems suspect. 
Because of the way $\bu$ was defined, there is a linear dependence between the two sets of parameters.

Finally, the posterior distribution for the intercepts follow a normal distribution should the prior specified on the intercepts also be a normal distribution, e.g. $\balpha \sim \N_m(\bzero,\bA)$.
The posterior mean and variance for the intercepts are given by
\[
  \tilde\balpha = \tilde\bV_\alpha \tilde\bSigma^{-1}\big(\tilde\by_i^* - \tilde\bff(x_i)\big) \ \text{ and } \ \tilde\bV_\alpha = \big(n\tilde\bSigma^{-1} + \bA^{-1}\big)^{-1}.
\]

Note that the evaluation of each of the component of the posterior depends on some of the components itself, and so this circular dependence is dealt with by using some arbitrary starting values and after which an iterative updating scheme of the components ensues.
The updating scheme is performed until a maximum number of iterations is reached, or ideally until some of convergence criterion is met.
In variational inference, the \emph{variational lower bound} is typically used to asses convergence.
The lower bound is given by
\begin{align*}
  \cL 
  &= \int q(\by^*,\bw,\theta) \log \left[ \frac{p(\by,\by^*,\bw,\theta)}{q(\by^*,\bw,\theta)} \right] \d\by^* \d\bw \d\theta \\
  &= \E[\log p(\by,\by^*,\bw,\theta)] - \E[\log q(\by^*,\bw,\theta)].
\end{align*}
These are calculable once the posterior distributions $\tilde q$ are known---the first term is the expectation of the logarithm of the joint density, whereas the second term factorises into the entropy of its individual components.
Similar to the EM algorithm, this quantity is\hltodo[Proof?]{expected to increase with every iteration.}


%The following pseudocode summarises the variational algorithm for I-probit models.
%
%\algrenewcommand{\algorithmiccomment}[1]{{\color{gray}\hskip2em$\triangleright$ #1}}
%\begin{algorithm}[H]
%\caption{VB-EM algorithm for the probit I-prior model}\label{alg:VBEM}
%\begin{algorithmic}[1]
%\Procedure{Initialise}{}
%  \State $\bSigma^{(0)} \gets \bI_m$
%  \For{$j=1,\dots,m$}
%    \State Randomise $\alpha_j^{(0)}$, $\eta_j^{(0)}$, $\bw_j^{(0)}$
%%    \State $\bw_j^{(0)} \gets \bzero_{n}$ %\Comment{or draw $w_i^{(0)} \ \sim \N(0,1)$ for $i=1,\dots,n$.}
%    \State Calculate $\bH_{\eta_j}$ as per kernels chosen
%  \EndFor
%%  \State $\bW^{(t)} \gets \big(\bw_1^{(t+1)} \cdots  \bw_m^{(t+1)} \big)$
%  \State $\bG^{(t)} \gets \diag(\bH_{\eta_1}, \dots, \bH_{\eta_m})$
%\EndProcedure
%\Statex
%%\Procedure{Update for $\bff$ } {time $t$}
%%  \For{$j=1,\dots,m$}
%%    \State $\bff_j^{(t+1)} \gets \alpha_j^{(t)}\bone_{n} + \bH_{\eta_j}\bw_j^{(t)}$
%%  \EndFor
%%  \State $\bF^{(t+1)} \gets \big(\bff_1^{(t+1)} \cdots  \bff_m^{(t+1)} \big)$
%%\EndProcedure
%%\Statex
%\Procedure{Update for $\by^*$ }{time $t$}
%  \For{$i=1,\dots,n$}
%    \State $j \gets y_i$
%    \If{Independent I-probit model}
%      \State $(y_{i1}^{*(t+1)},\dots,y_{im}^{*(t+1)}) \gets \E[\by_i^*]$ as per \eqref{eq:ystarupdate}
%    \Else
%      \State Sample from truncated normal as per \eqref{eq:ystardist}
%      \State $(y_{i1}^{*(t+1)},\dots,y_{im}^{*(t+1)}) \gets$ sample mean
%    \EndIf  
%  \EndFor
%\EndProcedure	
%\Statex
%\Procedure{Update for $\bw$ }{time $t$}
%  \State $\bV_w^{(t+1)} \gets \big(\bG(\bSigma^{-1} \otimes \bI_n)\bG + (\bSigma \otimes \bI_n) \big)^{-1}$ 
%  \Comment{Simpler if independent I-probit}
%  \State $\bw^{(t+1)} \gets \bV_w\bG^{(t+1)} (\bSigma^{-1} \otimes \bI_n) (\by^* - \balpha)$ 
%\EndProcedure	
%\Statex
%\Procedure{Update for $\eta$ }{time $t$}
%  \State Metropolis sampling from density\vspace{-0.5em} \Comment{Simpler calculations if only RKHS scales}
%  \[
%    \tilde q(\eta) \propto \exp\left[ (\by^{*(t)} - \balpha^{(t)} - \bG^{(t)}\bw^{(t)})^\top (\bSigma^{-1} \otimes \bI_n) (\by^{*(t)} - \balpha^{(t)} - \bG^{(t)}\bw^{(t)}) \right] \vspace{-0.7em}
%  \]
%  \State $\eta^{(t+1)} \text{ and } \bG^{(t+1)} \gets$ sample mean
%\EndProcedure	
%\algstore{VBEMbreak1}	
%\end{algorithmic}
%\end{algorithm}
%
%
%\begin{algorithm}[H]
%\begin{algorithmic}[1]
%\algrestore{VBEMbreak1}
%\Procedure{Update for $\bSigma$ }{time $t$}
%  \State $\bu^{(t)} \gets \bSigma^{(t)}\bP\bw$ and $\bV_u^{(t)} \gets \bSigma^{(t)}\bP\bV_w^{(t)}\bP^\top\bSigma^{(t)}$
%  \State $\bA_1 \gets \sum_{i=1}^n  \E_{\by^*} \left[ (\by^*_i - \balpha^{(t)} - \bff_i^{(t)})(\by^*_i - \balpha^{(t)} - \bff_i^{(t)})^\top \right]$  
%  \State $\bA_2 \gets \sum_{i=1}^n \left(\bu_i\bu_i^\top + \bV_{u_i} \right)$
%  \State $\bSigma^{(t+1)} \gets (\bA_1 + \bA_2)/(2n)$ \Comment{Simpler if independent I-probit}
%\EndProcedure	
%\Statex
%\Procedure{Update for $\balpha$ }{time $t$}
%  \State $\balpha^{(t+1)} \gets \sum_{i=1}^n\big(\by_i^* - \bff^{(t)}(x_i)\big)/n $
%\EndProcedure	
%\Procedure{The VB-EM algorithm}{}
%  \State $t \gets 0$ and initialise $\cL^{(0)}$
%  \While{$\cL^{(t+1)} - \cL^{(t)} > \delta$ \textbf{or} $t < t_{max}$}{}
%    \State \textbf{call} \Call{Update for $\by^*$}{}
%    \State \textbf{call} \Call{Update for $\bw$}{}
%    \State \textbf{call} \Call{Update for $\eta$}{}
%    \State \textbf{call} \Call{Update for $\bSigma$}{}
%    \State \textbf{call} \Call{Update for $\balpha$}{}
%    \State \textbf{call} Calculate variational lower bound $\cL^{(t+1)}$
%    \State $t \gets t + 1$
%  \EndWhile
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}