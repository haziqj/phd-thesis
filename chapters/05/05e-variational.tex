We present a variational inference algorithm to estimate the I-probit latent variables $\by^*$ and $\bw$, together with the parameters $\theta = \{\balpha:=(\alpha_1,\dots,\alpha_m),\eta,\bPsi \}$.
Begin by assuming some prior distribution on the parameters $p(\theta) = p(\balpha)p(\eta)p(\bPsi)$. 
Although one may devote more attention to the prior specification of these parameters, for our purposes it suffices that they are independent component-wise, and the PDFs belong to the exponential family of distributions with known hyperparameters.
The exponential family requirement greatly eases the complexity of deriving the variational algorithm later on\footnote{
Of interest, one may even opt to assign improper priors on $\theta$ and the algorithm would still work.
This is akin to obtaining empirical Bayes estimate of the $\theta$ if seen from an EM algorithm standpoint.
}.

Recall that $\by^*|\bw \sim \MN_{n,m}(\bone_n\balpha^\top  + \bH_\eta \bw, \bI_n,\bPsi^{-1} )$ and $\bw \sim \MN_{n,m}(\bzero, \bI_n,\bPsi)$.
The required posterior distribution is then $
  p(\by^*,\bw,\theta|\by) \propto p(\by|\by^*)p(\by^*|\bw,\theta)p(\bw|\theta)p(\theta)
$.
This is approximated by a mean-field distribution of the form $q(\by^*,\bw,\theta) \equiv q(\by^*)q(\bw)q(\theta)$, and also $q(\theta) = q(\balpha)q(\eta)q(\bPsi)$.
Denote by $\tilde q$ the distributions which minimise the Kullbeck-Leibler divergence (maximise the variational lower bound).
By appealing to \citet[equation 10.9, p. 466]{bishop2006pattern}, we find that for each $\xi \in \{ \by^*,\bw,\theta \} =: \cZ$, $\tilde q$ satisfies
\begin{align}\label{eq:qtilde}
  \log \tilde q(\xi) = \E_{-\xi} [\log p(\by, \by^*, \bw, \theta)] + \const
\end{align}
where expectation of the log joint density of $(\by, \by^*, \bw, \theta)$ is taken with respect to all of the unknowns $\cZ$ except the one currently in consideration, under their respective $q$ densities. 
Estimates of the latent variables and parameters are then obtained by taking the mean of their respective approximate posterior distribution.

In practice, rather than an explicit calculation of the normalising constant, one simply needs to inspect \cref{eq:qtilde} to recognise it as a known log-density function, which is the case when exponential family distributions are considered.
That is, suppose that each complete conditional $p(\xi|\cZ_{-\xi}, \by)$ follows an exponential family distribution,
\[
  p(\xi|\cZ_{-\xi}, \by) = B(\xi)\exp \big(\ip{\zeta_\xi(\cZ_{-\xi}, \by) , \xi} - A(\zeta_\xi) \big).
\]
Then, from \cref{eq:qtilde},
\begin{align*}
  \tilde q(\xi)
  &\propto \exp\big(\E_{-\xi}[\log p(\xi|\cZ_{-\xi}, \by)] \big) \\
  &= \exp \Big(\log B(\xi) + \E \ip{\zeta_\xi(\cZ_{-\xi}, \by) , \xi} - \E [ A(\zeta_\xi) ] \Big) \\
  &\propto B(\xi)\exp \E\ip{\zeta_\xi(\cZ_{-\xi}, \by) , \xi}
\end{align*}
is also in the same exponential family.
In situations where there is no closed form expression for $\tilde q$, then one resorts to sampling methods such as a Metropolis random walk to estimate quantities of interest.
This stochastic step within a deterministic algorithm has been explored before in the context of a Monte Carlo EM algorithm---see \citet[§4, pp. 537--538]{meng1997algorithm} and references therein.

\begin{figure}[t]
  \centering
  \begin{tikzpicture}[scale=1, transform shape]
    \tikzstyle{main}=[circle, minimum size=10mm, thick, draw=black!80, node distance=16mm]
    \tikzstyle{connect}=[-latex, thick]
    \tikzstyle{box}=[rectangle, draw=black!100]
%      \node[main, draw=black!0] (blank) [xshift=-0.55cm] {};  % pushes image to right slightly
      \node[main, fill=black!10] (x) [] {$x_i$};
      \node[main,double,double distance=0.6mm] (f) [right=of x,yshift=-1.7cm] {$f_{ij}$};
      \node[main] (eta) [below=of x,yshift=-0.7cm] {$\eta$};        
      \node[main] (w) [above=of f,yshift=0.3cm] {$w_{ij}$};  
      \node[main] (ystar) [right=of f,yshift=1.7cm] {$y_{ij}^*$};
      \node[main,double,double distance=0.6mm] (pij) [right=of ystar] {$p_{ij}$};      
      \node[main, fill = black!10] (y) [right=of pij] {$y_{i}$};      
      \node[main] (alpha) [below=of ystar,yshift=-0.75cm] {$\alpha_j$};  
      \node[main] (Psi) [above=of ystar,yshift=0.4cm] {$\bPsi$};
      \path (alpha) edge [connect] (ystar)
            (eta) edge [connect] (f)
            (x) edge [connect] node [above] {$h$} (f)
    		(f) edge [connect] (ystar)
    		(ystar) edge [connect] node [above] {$g^{-1}$}  (pij)
            (pij) edge [connect] (y)
            (Psi) edge [connect] (w)
            (Psi) edge [connect] (ystar)
    		(w) edge [connect] (f);
      \node[rectangle, draw=black!100, fit={($(x.north west) + (-0.3,0.3cm)$) ($(y.north east) + (0.3,0cm)$) ($(f.south west) + (0,-0.3cm)$) ($(w.north west) + (0,0.3cm)$)}] {}; 
      \node[draw=none] () [below=of y,xshift=-0.3cm,yshift=-0.4cm] {$i=1,\dots,n$};
      \node[rectangle, draw=black!100, fit={($(alpha.south east) + (0,-0.25cm)$) ($(pij.north east) + (0.3,0cm)$) ($(w.north west) + (-0.3,0.58cm)$)  }] {}; 
      \node[draw=none] () [right=of alpha,xshift=-0.4cm,yshift=-0.48cm] {$j=1,\dots,m$};      
    \end{tikzpicture}
    \caption{A DAG of the I-probit model. Observed nodes are shaded, while double-lined nodes represents calculable quantities.}
\end{figure}

We now present the mean-field variational distributions $\tilde q$.
On notation: we will typically refer to posterior means of the parameters $\by^*$, $\bw$, $\theta$ and so on by the use of a tilde.
For instance, we write $\tilde\bw$ to mean $\E_{\bw \sim \tilde q}[\bw]$, the expected value of $\bw$ under the pdf $\tilde q(\bw)$.
The distributions are simply stated, but a full derivation is given in the appendix.

\subsection[Latent propensities ystar]{Latent propensities $\by^*$}

The fact that the rows of $\by^*$ are independent can be exploited.
Write $\by_i^* = (y_{i1}^*,\dots,y_{im}^*)^\top$.
Then $\by_{i}^*|\theta,x_i \sim \N_m(\balpha + \bff(x_i),\bPsi^{-1})$, and we have the induced factorisation of the distribution $q(\by^*) = \prod_{i=1}^n q(\by_i^*)$, where each $q(\by_i^*)$ is the density of a \emph{conically truncated multivariate normal disribution}.
That is, for each $i=1,\dots,n$ and noting the observed values $y_i = j \in \{1,\dots,m\}$, the $\by_i^*$'s are distributed according to
\begin{align}\label{eq:ystardist}
  \by_i^* \iid
  \begin{cases}
    \N_m(\tilde \balpha + \tilde \bff(x_i), \tilde\bSigma) & \text{ if } y_{ij}^* > y_{ik}^*, \forall k \neq j \\
    0 & \text{ otherwise}. \\
  \end{cases}
\end{align}
The required expectations $\E\by_i^* = \E (y_{i1}^*,\dots,y_{im}^*)^\top$ are tricky to compute.
One strategy might be Monte Carlo integration: using samples from $\N_m(\tilde \alpha + \tilde \bff(x_i), \tilde\bPsi^{-1})$, zero out those that do not satisfy the condition $y_{ij}^* > y_{ik}^*, \forall k \neq j$, then take the sample average.
If the independent I-probit model is considered, where $\bPsi = \diag(\sigma_1^{-2},\dots,\sigma_m^{-2})$, the expected value can be considered component-wise, and each component of this expectation is given by
\begin{align}\label{eq:ystarupdate}
  \tilde y_{ik}^* =
  \begin{cases}
    \tilde\alpha_k + \tilde f_{ik} - \tilde\sigma_k C_i^{-1} \displaystyle{  \int \phi_{ik}(z) \prod_{l \neq k,j} \Phi_{il}(z) \phi(z) \dint z }
    &\text{ if } k \neq y_i \\[1.5em]
    \tilde\alpha_{y_i} + \tilde f_{iy_i} - \tilde\sigma_{y_i} \sum_{k \neq y_i} \big(\tilde y_{ik}^* - \tilde f_{ik} \big) 
    &\text{ if } k = y_i \\
  \end{cases}
\end{align}
with 
\begin{align*}
  \phi_{ik}(Z) &= \phi \left(\frac{\tilde\sigma_{y_i}}{\tilde\sigma_k} Z + \frac{\tilde\alpha_{y_i} + \tilde f_{iy_i} - \tilde\alpha_k - \tilde f_{ik}}{\tilde\sigma_k} \right) \\
  \Phi_{ik}(Z) &= \Phi \left(\frac{\tilde\sigma_{y_i}}{\tilde\sigma_k} Z + \frac{\tilde\alpha_{y_i} + \tilde f_{iy_i} - \tilde\alpha_k - \tilde f_{ik}}{\tilde\sigma_k} \right) \\
  C_i &= \int \prod_{l \neq j} \Phi_{il}(z) \phi(z) \dint z
\end{align*}
and $Z \sim \N(0,1)$ with pdf and cdf $\phi(\cdot)$ and $\Phi(\cdot)$ respectively. 
The integrals that appear above are functions of a unidimensional Gaussian pdf, and these can be computed rather efficiently using quadrature methods.

\subsection[I-prior random effects w]{I-prior random effects $\bw$}

Given that both $\vecc \by^* | \vecc \bw$ and $\vecc\bw$ are normally distributed, we find that the conditional posterior distribution $p(\bw|\cZ_{-\bw},\by)$ is also normal, and therefore the approximate posterior density $\tilde q$ for $\vecc \bw \in \bbR^{nm}$ is also normal with mean and precision given by
\begin{gather}\label{eq:varipostw}
   \vecc \tilde\bw = \tilde\bV_w 
    (\tilde\bPsi \otimes \tilde\bH_\eta) \vecc (\tilde\by^* - \bone_n\tilde\balpha^\top)
  \hspace{0.5cm}\text{and}\hspace{0.5cm} 
  \tilde \bV_w^{-1} = (\tilde\bPsi \otimes \tilde\bH_\eta^2) + (\tilde\bPsi^{-1} \otimes \bI_n).
\end{gather}
We note the similarity between \cref{eq:varipostw} above and the posterior distribution for the I-prior random effects in a normal model \cref{eq:posteriorw} seen in the previous chapter.
Naïvely computing the inverse $\tilde\bV_w^{-1}$ presents a computational challenge, as this takes $O(n^3m^3)$ time. 
By exploiting the Kronecker product structure in $\tilde\bV_w^{-1}$, we are able to efficiently compute the required inverse in roughly $O(n^3m)$ time---see the appendix for details.
Equivalently, we can express the distribution for $\bw \sim \tilde q$ as a matrix normal distribution
\begin{align}\label{eq:varipostw2}
  \MN_{nm}\Big(\,
  \greyoverbrace{\tilde\bH_\eta^{-1}(\tilde\by^* - \bone_n\tilde\balpha^\top)\tilde\bPsi^2}{\tilde\bw} ,\, 
  \tilde\bH_\eta^{-2},\,
  \tilde\bPsi  
  \Big).
\end{align}

If the independent I-probit model is assumed, i.e. $\tilde\bPsi = \diag(\tilde\sigma_1^{-2},\dots,\tilde\sigma_m^{-2})$, then the posterior covariance matrix $\tilde\bV_w$ has a simpler structure.
This means that the random matrix $\bw$ will have columns which are independent of each other.
By writing $\bw_j = (w_{1j},\dots,w_{nj})^\top \in \bbR^n$, $j=1,\dots,m$, to denote the column vectors of $\bw$ and with a slight abuse of notation, we have that
\begin{align*}
  \N_{nm}(\vecc \bw|\vecc\wtilde, \tilde\bV_w) 
  = \prod_{j=1}^m \N_{n}(\bw_j|\tilde\bw_j, \tilde\bV_{w_j}),
\end{align*}
where 
\[
  \tilde \bw_j = \sigma_j^{-2}\tilde \bV_{w_j}\tilde\bH_\eta (\tilde\by^*_j - \tilde\alpha_j\bone_n) \ \text{ and } \ \tilde \bV_{w_j} = \big(\sigma_j^{-2}\tilde\bH_{\eta}^2 + \sigma_j^{2}\bI_n \big)^{-1}.
\]

\subsection[RKHS parameters eta]{RKHS parameters $\eta$}

The posterior density $\tilde q$ involving the RKHS parameters is of the form
\[
  \log\tilde q(\eta) =  -\half\tr\E_{-\eta} \Big[ 
  (\by^* - \bone_n\balpha^\top - \bH_\eta\bw)^\top \bPsi (\by^* - \bone_n\balpha^\top - \bH_\eta\bw) \Big] + \log p(\eta) + \const,
\]
where $p(\eta)$ is an appropriate prior distribution for $\eta$.
Generally, samples $\eta^{(1)},\dots,\eta^{(T)}$ from $\tilde q(\eta)$ may be obtained using a Metropolis algorithm, and quantities such as $\tilde\bH_{\eta} = \E_q[\bH_{\eta}]$ may be approximated using $\frac{1}{T}\sum_{t=1}^T \bH_{\eta^{(t)}}$.

However, when only RKHS scale parameters are involved, then the distribution $\tilde q$ can be found in closed-form, much like in the exponential family EM algorithm described in \hltodo{Section 4.3.3}.
Under the same setting as in that subsection, assume that only $\eta = \{\lambda_1,\dots,\lambda_p\}$ need be estimated, and for each $k=1,\dots,p$, we can decompose the kernel matrix as $\bH_\eta = \lambda_k \bR_k + \bS_k$ and its square as $\bH_\eta^2 = \lambda_k^2 \bR_k^2 + \lambda_k \bU_k + \bS_k^2$.
Additionally, we impose a further mean-field restriction on $q(\eta)$, and that is $q(\eta) = \prod_{k=1}^p p(\lambda_k)$.
Then, by using independent and identical normal priors for $\lambda_k$, say $\lambda_k \sim \N(0,v_\lambda)$, each $\tilde q(\lambda_k)$ density is normal with mean and variance

\hltodo{Write down the mean and variance for lambda}

\subsection[Error precision Psi]{Error precision $\bPsi$}

A small reparameterisation of the I-prior random effects is necessary to achieve conjugacy for the $\bPsi$ parameter. 
Let $\bu\in\bbR^{n\times m}$ be a matrix defined by $\bPsi^{-1}\bw$.
Then $\bu \sim \MN_{n,m}(\bzero, \bI_n, \bPsi^{-1})$ a priori.
From \cref{eq:varipostw2}, the optimal variational distribution for $\bu$ would be $\MN_{n,m}(\tilde\bw\tilde\bPsi^{-1}, \tilde\bH_\eta^2, \tilde\bPsi^{-1})$.
With a Wishart prior on the precision matrix $\bPsi\sim\Wis_m(\bG,g)$, where $g\geq m$, the optimal variational density for $\bPsi$ is found to satisfy
\begin{align*}
  \log \tilde q(\bPsi)
  &= \const - \half \sum_{i=1}^n \tr \left( 
  \big(\bG_1 + \bG_2 + \bG \big)\bPsi 
  \right) + \frac{g-m-1}{2} \log \abs{\bPsi}
\end{align*}
which is recognised as the log density of a Wishart distribution with scale matrix $\bG_1 + \bG_2 + \bG$ and $g$ degrees of freedom, where
\begin{align}
  \begin{gathered}
  \bG_1 = \E_{\cZ\backslash\{\bPsi\}\sim q} \left[ 
  \sum_{i=1}^n 
  (\by^*_i - \balpha - \bff(x_i))
  (\by^*_i - \balpha - \bff(x_i))^\top 
  \right]   \\
  \bG_2 = \sum_{i=1}^n \E_{\bu\sim q}\left[\bu_i\bu_i^\top \right].
  \end{gathered}
\end{align}
The challenge here is that it involves the second posterior moment of the conically truncated multivariate normal distribution for $\by^*$, which may be obtained by sampling or numerical integration as described earlier.

%As a remark on identifiability, we might require that one of the components of $\bSigma$ be fixed, e.g. $\bSigma_{11} = 1$.
%\cite{mcculloch2000bayesian} gives a Gibbs sampling algorithm which we find useful for our variational algorithm as well.
%Partition $\bSigma$ as follows:
%\[
%  \bSigma = \begin{pmatrix}
%    1       &\bzeta^\top \\
%    \bzeta  &\bZ + \bzeta\bzeta^\top
%  \end{pmatrix}.
%\]
%We can choose the priors $\bzeta \sim \N_{m-1}(\bzero,\bB)$ and $\bZ \sim \invWis(\bA,a)$, independent of each other with appropriately chosen hyperparameters.
%Define $\tilde\bepsilon_i = \by^*_i - \tilde\balpha - \tilde\bff(x_i) = (\tilde\epsilon_{i1},\dots,\tilde\epsilon_{im})^\top$, and let $\upsilon_i = \surd 2 \tilde\epsilon_{i1}$ and $\bnu_i = \surd 2(\tilde\epsilon_{i2},\dots,\tilde\epsilon_{im})^\top$ such that $\surd 2\tilde\epsilon_i^\top = (\upsilon_i, \bnu_i^\top)^\top$.
%The posterior distribution $\tilde q$ for $\bZ$ is inverse Wishart with scale equal to $\bA_3 + \bA$, where\hltodo[These equations can be simplified further.]{}
%\begin{align*}
%  \begin{gathered}
%    \bA_3 = \E_{\bnu,\upsilon_i,\zeta} \left[ \sum_{i=1}^n (\bnu_i - \upsilon_i \bzeta)(\bnu_i - \upsilon_i \bzeta)^\top \right]   \\
%    \end{gathered}
%\end{align*}
%and $2n(m-1)+a$ degrees of freedom.
%The posterior distribution $\tilde q$ for $\bzeta$ is normal with mean and variance $\tilde\bzeta = \frac{1}{n}\sum_{i=1}^n \tilde\upsilon_i \tilde\bV_\zeta \tilde\bZ\tilde\bnu_i$ and $\tilde\bV_\zeta = (\tilde\bupsilon\tilde\bupsilon^\top\tilde\bZ^{-1} +\bB)^{-1}$.

If the independent I-probit model is considered, then $\bSigma = \diag(\sigma_1^2,\dots,\sigma_m^2)$, class independence holds so we can use independent inverse gamma distributions as a prior for $\bSigma$, i.e. $ p(\bSigma) = \prod_{j=1}^m  p(\sigma_j^2)$, where each $p(\sigma_j) \equiv \Gamma^{-1}(r,s)$.
The posterior for $\bSigma$ will also be of a similar factorised form , namely $\tilde q(\bSigma) = \prod_{j=1}^m \tilde q(\sigma_j^2)$, where $\tilde q(\sigma_j^2)$ is the PDF of an inverse gamma distribution with shape and scale parameters $\tilde r = 2n+r-1$ and $\tilde s = \half\Vert \tilde\by^*_j - \tilde\alpha_j - \tilde\bff_j \Vert^2 + \half \Vert \tilde\bu_j \Vert^2 + s$ respectively.

Finally, the posterior distribution for the intercepts follow a normal distribution should the prior specified on the intercepts also be a normal distribution, e.g. $\balpha \sim \N_m(\bzero,\bA)$.
The posterior mean and variance for the intercepts are given by
\[
  \tilde\balpha = \tilde\bV_\alpha \tilde\bSigma^{-1}\big(\tilde\by_i^* - \tilde\bff(x_i)\big) \ \text{ and } \ \tilde\bV_\alpha = \big(n\tilde\bSigma^{-1} + \bA^{-1}\big)^{-1}.
\]

Note that the evaluation of each of the component of the posterior depends on some of the components itself, and so this circular dependence is dealt with by using some arbitrary starting values and after which an iterative updating scheme of the components ensues.
The updating scheme is performed until a maximum number of iterations is reached, or ideally until some of convergence criterion is met.
In variational inference, the \emph{variational lower bound} is typically used to asses convergence.
The lower bound is given by
\begin{align*}
  \cL 
  &= \int q(\by^*,\bw,\theta) \log \left[ \frac{p(\by,\by^*,\bw,\theta)}{q(\by^*,\bw,\theta)} \right] \d\by^* \d\bw \d\theta \\
  &= \E[\log p(\by,\by^*,\bw,\theta)] - \E[\log q(\by^*,\bw,\theta)].
\end{align*}
These are calculable once the posterior distributions $\tilde q$ are known---the first term is the expectation of the logarithm of the joint density, whereas the second term factorises into the entropy of its individual components.
Similar to the EM algorithm, this quantity is\hltodo[Proof?]{expected to increase with every iteration.}


%The following pseudocode summarises the variational algorithm for I-probit models.
%
%\algrenewcommand{\algorithmiccomment}[1]{{\color{gray}\hskip2em$\triangleright$ #1}}
%\begin{algorithm}[H]
%\caption{VB-EM algorithm for the probit I-prior model}\label{alg:VBEM}
%\begin{algorithmic}[1]
%\Procedure{Initialise}{}
%  \State $\bSigma^{(0)} \gets \bI_m$
%  \For{$j=1,\dots,m$}
%    \State Randomise $\alpha_j^{(0)}$, $\eta_j^{(0)}$, $\bw_j^{(0)}$
%%    \State $\bw_j^{(0)} \gets \bzero_{n}$ %\Comment{or draw $w_i^{(0)} \ \sim \N(0,1)$ for $i=1,\dots,n$.}
%    \State Calculate $\bH_{\eta_j}$ as per kernels chosen
%  \EndFor
%%  \State $\bW^{(t)} \gets \big(\bw_1^{(t+1)} \cdots  \bw_m^{(t+1)} \big)$
%  \State $\bG^{(t)} \gets \diag(\bH_{\eta_1}, \dots, \bH_{\eta_m})$
%\EndProcedure
%\Statex
%%\Procedure{Update for $\bff$ } {time $t$}
%%  \For{$j=1,\dots,m$}
%%    \State $\bff_j^{(t+1)} \gets \alpha_j^{(t)}\bone_{n} + \bH_{\eta_j}\bw_j^{(t)}$
%%  \EndFor
%%  \State $\bF^{(t+1)} \gets \big(\bff_1^{(t+1)} \cdots  \bff_m^{(t+1)} \big)$
%%\EndProcedure
%%\Statex
%\Procedure{Update for $\by^*$ }{time $t$}
%  \For{$i=1,\dots,n$}
%    \State $j \gets y_i$
%    \If{Independent I-probit model}
%      \State $(y_{i1}^{*(t+1)},\dots,y_{im}^{*(t+1)}) \gets \E[\by_i^*]$ as per \eqref{eq:ystarupdate}
%    \Else
%      \State Sample from truncated normal as per \eqref{eq:ystardist}
%      \State $(y_{i1}^{*(t+1)},\dots,y_{im}^{*(t+1)}) \gets$ sample mean
%    \EndIf  
%  \EndFor
%\EndProcedure	
%\Statex
%\Procedure{Update for $\bw$ }{time $t$}
%  \State $\bV_w^{(t+1)} \gets \big(\bG(\bSigma^{-1} \otimes \bI_n)\bG + (\bSigma \otimes \bI_n) \big)^{-1}$ 
%  \Comment{Simpler if independent I-probit}
%  \State $\bw^{(t+1)} \gets \bV_w\bG^{(t+1)} (\bSigma^{-1} \otimes \bI_n) (\by^* - \balpha)$ 
%\EndProcedure	
%\Statex
%\Procedure{Update for $\eta$ }{time $t$}
%  \State Metropolis sampling from density\vspace{-0.5em} \Comment{Simpler calculations if only RKHS scales}
%  \[
%    \tilde q(\eta) \propto \exp\left[ (\by^{*(t)} - \balpha^{(t)} - \bG^{(t)}\bw^{(t)})^\top (\bSigma^{-1} \otimes \bI_n) (\by^{*(t)} - \balpha^{(t)} - \bG^{(t)}\bw^{(t)}) \right] \vspace{-0.7em}
%  \]
%  \State $\eta^{(t+1)} \text{ and } \bG^{(t+1)} \gets$ sample mean
%\EndProcedure	
%\algstore{VBEMbreak1}	
%\end{algorithmic}
%\end{algorithm}
%
%
%\begin{algorithm}[H]
%\begin{algorithmic}[1]
%\algrestore{VBEMbreak1}
%\Procedure{Update for $\bSigma$ }{time $t$}
%  \State $\bu^{(t)} \gets \bSigma^{(t)}\bP\bw$ and $\bV_u^{(t)} \gets \bSigma^{(t)}\bP\bV_w^{(t)}\bP^\top\bSigma^{(t)}$
%  \State $\bA_1 \gets \sum_{i=1}^n  \E_{\by^*} \left[ (\by^*_i - \balpha^{(t)} - \bff_i^{(t)})(\by^*_i - \balpha^{(t)} - \bff_i^{(t)})^\top \right]$  
%  \State $\bA_2 \gets \sum_{i=1}^n \left(\bu_i\bu_i^\top + \bV_{u_i} \right)$
%  \State $\bSigma^{(t+1)} \gets (\bA_1 + \bA_2)/(2n)$ \Comment{Simpler if independent I-probit}
%\EndProcedure	
%\Statex
%\Procedure{Update for $\balpha$ }{time $t$}
%  \State $\balpha^{(t+1)} \gets \sum_{i=1}^n\big(\by_i^* - \bff^{(t)}(x_i)\big)/n $
%\EndProcedure	
%\Procedure{The VB-EM algorithm}{}
%  \State $t \gets 0$ and initialise $\cL^{(0)}$
%  \While{$\cL^{(t+1)} - \cL^{(t)} > \delta$ \textbf{or} $t < t_{max}$}{}
%    \State \textbf{call} \Call{Update for $\by^*$}{}
%    \State \textbf{call} \Call{Update for $\bw$}{}
%    \State \textbf{call} \Call{Update for $\eta$}{}
%    \State \textbf{call} \Call{Update for $\bSigma$}{}
%    \State \textbf{call} \Call{Update for $\balpha$}{}
%    \State \textbf{call} Calculate variational lower bound $\cL^{(t+1)}$
%    \State $t \gets t + 1$
%  \EndWhile
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}