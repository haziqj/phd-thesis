\index{variational EM algorithm!I-probit}
We present an EM algorithm to estimate the I-probit latent variables $\by^*$ and $\bw$, in which the E-step consists of a mean-field variational approximation of the conditional density $p(\by^*,\bw|\by,\theta) = q(\by^*)q(\bw)$.
As per assumptions $\ref{ass:A4}$, $\ref{ass:A5}$ and $\ref{ass:A6}$, the parameters of the I-probit model consists of $\theta =\{ \balpha = (\alpha_1,\dots,\alpha_m)^\top,\eta \}$.

The algorithm cycles through a variational inference E-step, in which the variational density $q(\by^*,\bw) = \prod_{i=1}^n q(\by^*_{i\bigcdot}) q(\bw)$ is optimised with respect to the Kullback-Leibler divergence $\KL\big( q(\by^*,\bw) \Vert p(\by^*,\bw|\by) \big)$, and an M-step, in which the approximate expected joint density \cref{eq:iprobitQEstep} is maximised with respect to the parameters $\theta$. 
Convergence is assessed by monitoring the ELBO.
Apart from the fact that the variational EM algorithm uses approximate conditional distributions and involves matrices $\by^*$ and $\bw$, it is very similar to the EM described in \cref{chapter4}, and as such, the efficient computational work derived there is applicable.
\index{kernel!front loading}

\subsection{The variational E-step}

\index{Kullback-Leibler|see{KL divergence}}
\index{KL divergence}
Let $\tilde q(\by^*,\bw)$ be the pdf that minimises the Kullback-Leibler divergence $\KL\big( q \Vert p \big)$ subject to the mean-field constraint $q(\by^*,\bw) = q(\by^*)q(\bw)$.
By appealing to \citet[eq. 10.9, p. 466]{bishop2006pattern}, the optimal mean-field variational density $\tilde q$ for the latent variables $\by^*$ and $\bw$ satisfy
\begin{align}
  \log \tilde q(\by^*) &= \E_{\bw\sim\tilde q} [\log p(\by,\by^*,\bw)] + \const \label{eq:logqystar} \\
  \log \tilde q(\bw) &= \E_{\by^*\sim\tilde q} [\log p(\by,\by^*,\bw)] + \const \label{eq:logqw}
\end{align}
where $p(\by,\by^*,\bw) = p(\by|\by^*) p(\by^*|\bw) p(\bw)$ is as per \cref{eq:iprobitlik}.
We now present the variational densities $\tilde q(\by^*)$ and $\tilde q(\bw)$.
For further details on the derivation of these densities, please refer to \cref{apx:varemiprobit} \colp{\mypageref{apx:varemiprobit}}.

\subsubsection{Variational distribution for the latent propensities \texorpdfstring{$\by^*$}{$y^*$}}

The fact that the rows $\by_{i \bigcdot}^* \in \bbR^m$, $i=1,\dots,n$ of $\by^* \in \bbR^{n \times m}$ are independent can be exploited, and this results in a further induced factorisation $q(\by^*) = \prod_{i=1}^n q(\by_i^*)$.
Define the set $\cC_j = \{y_{ij}^* > y_{ik}^* \,|\, \forall k\neq j \}$.
Then $q(\by_{i \bigcdot}^*)$ is the density of a multivariate normal distribution with mean $\tilde \bmu_{i \bigcdot} = \balpha + \tilde\bw^\top  \bh_\eta(x_i)$, where $\tilde \bw = \E_{\bw\sim\tilde q}[\bw]$, and variance $\bPsi^{-1}$, subject to a truncation of its components to the set $\cC_{y_i}$.
That is, for each $i=1,\dots,n$ and noting the observed categorical response $y_i \in \{1,\dots,m\}$ for the $i$'th observation, the $\by_i^*$'s are distributed according to
\begin{align}\label{eq:ystardist}
  \by_{i \bigcdot}^* \iid
  \begin{cases}
    \N_m(\tilde\bmu_{i \bigcdot},  \bPsi^{-1}) & \text{ if } y_{iy_i}^* > y_{ik}^*, \forall k \neq y_i \\
    0 & \text{ otherwise}. \\
  \end{cases}
\end{align}
We denote this by $\by_{i \bigcdot}^* \iid \tN(\tilde\bmu_{i \bigcdot}, \bPsi^{-1},\cC_{y_i})$, and the important properties of this distribution are explored in the appendix.

The required expectation $\tilde\by^* := \E_{\by^*\sim\tilde q} [\by^*_{i\bigcdot}] = \E_{\by^*\sim\tilde q} [y_{i1}^*,\dots, y_{im}^*]^\top$ in the M-step can be tricky to obtain.
One strategy that can be considered is Monte Carlo integration: using samples from $\N_m(\tilde\bmu_{i \bigcdot},  \bPsi^{-1})$, disregard those that do not satisfy the condition $y_{iy_i}^* > y_{ik}^*, \forall k \neq j$, and then take the sample average.
This works reasonably well so long as the truncation region does not fall into the extreme tails of the multivariate normal.
Alternatively, a Gibbs-based approach \citep{robert1995simulation} for sampling from a truncated multivariate normal can be implemented, and this is detailed in \cref{apx:truncmultinorm}.

If the independent I-probit model is under consideration, whereby the covariance matrix has the independent structure $\bPsi = \diag(\sigma_1^{-2},\dots,\sigma_m^{-2})$, then the first moment  can be considered component-wise. 
Each component of this expectation is given by
\begin{align}\label{eq:ystarupdate}
  \tilde y_{ik}^* =
  \begin{cases}
    \tilde\mu_{ik} - \sigma_k C_i^{-1} \displaystyle{  \int \phi_{ik}(z) \prod_{l \neq k,y_i} \Phi_{il}(z) \phi(z) \dint z }
    &\text{ if } k \neq y_i \\[1.5em]
    \tilde\mu_{iy_i} - \sigma_{y_i} \sum_{k \neq y_i} \big(\tilde y_{ik}^* -  \tilde\mu_{ik} \big) 
    &\text{ if } k = y_i \\
  \end{cases}
\end{align}
with 
\vspace{-1em}
\begin{align*}
  \phi_{ik}(Z) &= \phi \left(\frac{\sigma_{y_i}}{\sigma_k} Z + \frac{\tilde\mu_{iy_i} - \tilde\mu_{ik}}{\sigma_k} \right) \\
  \Phi_{ik}(Z) &= \Phi \left(\frac{\sigma_{y_i}}{\sigma_k} Z + \frac{\tilde\mu_{iy_i} - \tilde\mu_{ik}}{\sigma_k} \right) \\
  C_i &= \int \prod_{l \neq j} \Phi_{il}(z) \phi(z) \dint z
\end{align*}
and $Z \sim \N(0,1)$ with pdf and cdf $\phi(\cdot)$ and $\Phi(\cdot)$ respectively. 
The integrals that appear above are functions of a unidimensional Gaussian pdf, and these can be computed rather efficiently using quadrature methods.
\vspace{-0.3em}

\subsubsection{Variational distribution for the I-prior random effects \texorpdfstring{$\bw$}{$w$}}

Given that both $\vecc \by^* | \vecc \bw$ and $\vecc\bw$ are normally distributed as per the model \cref{eq:iprobitmod}, we find that the full conditional distribution $p(\bw|\by^*,\by) \propto p(\by^*,\by,\bw) \propto p(\by^*|\bw)p(\bw)$ is also normal. 
The variational density $q$ for $\vecc \bw \in \bbR^{nm}$ is found to be Gaussian with mean and precision given by
\begingroup
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{4pt}
\begin{gather}\label{eq:varipostw}
   \vecc \tilde\bw = \tilde\bV_w 
    (\bPsi \otimes \bH_\eta) \vecc (\tilde\by^* - \bone_n\balpha^\top)
  \hspace{0.5cm}\text{and}\hspace{0.5cm} 
  \tilde \bV_w^{-1} = \bPsi \otimes \bH_\eta^2 + \bPsi^{-1} \otimes \bI_n = \bV_{y^*}.
\end{gather}
\endgroup
As a computational remark, computing the inverse $\tilde\bV_w^{-1}$ presents a challenge, as this takes $O(n^3m^3)$ time if computed na√Øvely. 
By exploiting the Kronecker product structure in $\tilde\bV_w$, we are able to efficiently compute the required inverse in roughly $O(n^3m)$ time---see \cref{sec:complxiprobit} for details.
Storage requirement is $O(n^2m^2)$, as a result of the covariance matrix in \cref{eq:varipostw}.

If the independent I-probit model is assumed, i.e. $\bPsi = \diag(\psi_1,\dots,\psi_m)$, then the posterior covariance matrix $\tilde\bV_w$ has a simpler structure which implies column independence in the matrix $\bw$.
By writing $\bw_{\bigcdot j} = (w_{1j},\dots,w_{nj})^\top \in \bbR^n$, $j=1,\dots,m$, to denote the column vectors of $\bw$, and with a slight abuse of notation, we have that
\begingroup
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{6pt}
\[
  \N_{nm}(\vecc \bw|\vecc\wtilde, \tilde\bV_w) 
  = \prod_{j=1}^m \N_{n}(\bw_{\bigcdot j}|\tilde\bw_{\bigcdot j}, \tilde\bV_{w_j}),
\]
\endgroup
where $\N_{d}(\bx|\bmu,\bSigma)$ is the pdf of $\bx \sim \N(\bmu,\bSigma)$, and
\begingroup
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{0pt}
\[
  \tilde \bw_{\bigcdot j} = \psi_j\tilde \bV_{w_j}\bH_\eta (\tilde\by^*_j - \alpha_j\bone_n) \ \text{ and } \ \tilde \bV_{w_j} = \big(\psi_j\bH_{\eta}^2 + \psi_j^{-1}\bI_n \big)^{-1}.
\]
\endgroup
We note the similarity between \cref{eq:varipostw} above and the posterior distribution for the I-prior random effects in a normal model \cref{eq:posteriorw} seen in the previous chapter, with the difference being \cref{eq:varipostw} uses the continuous latent propensities $\by^*$ instead of the the observations $\by$.
The consequence of this is that the posterior regression functions are class independent, the exact intended effect by specifying a diagonal precision matrix $\bPsi$.
Storage requirement is $O(n^2m)$, since we need $\bV_{w_1},\dots,\bV_{w_m}$.

\begin{remark}
  The variational distribution $q(\bw)$ which approximates $p(\bw|\by)$ is in fact exactly $p(\bw|\by^*)$, the conditional density of the I-prior random effects given the latent propensities.
  By the law of total expectations, 
  \begin{gather*}
    \E(r(\bw)|\by) = \E_{\by^*}\big( \E(r(\bw)|\by^*) \,\big|\, \by \big),
  \end{gather*}
  where $r(\cdot)$ is some function of $\bw$, and expectations are taken under the posterior distribution of $\by^*$.
  Hypothetically, if the true pdf $p(\by^*|\by)$ were tractable,  then the E-step can be computed using the true conditional distribution.
  Since it is not tractable, we resort to an approximation, and in the case of a variational approximation, \cref{eq:varipostw} is obtained.
\end{remark}

\subsection{The M-step}
\label{sec:varupdeta}

From \cref{eq:iprobitQEstep}, the function to be maximised in the M-step is
\begin{align*}
  Q(\theta) 
  ={}& \E_{\by^*,\bw\sim q^{(t+1)}}  [ \log p(\by,\by^*,\bw|\theta) ] \nonumber \\
  ={}& \const -\half\tr\Big( \bPsi \E(\bw^\top\bH_\eta^2\bw)  + \bPsi^{-1} \E(\bw^\top\bw) \Big)  \nonumber \\
  &- \half \tr \Big( 
  \bPsi \big\{
  \E(\by^{*\top}\by^*)
  + n \balpha\balpha^\top 
  - 2\balpha \bone_n^\top \E \by^*
  - 2\E(\bw^\top)\bH_\eta \big(\E \by^* - \bone_n\balpha^\top \big) 
  \big\} \Big)
  ,
\end{align*}
where expectations are taken with respect to the variational distributions of $\by^*$ and $\bw$. 
Note that since $\bPsi$ is treated as fixed, the term $\E[\by^{*\top}\by^*]$ is absorbed into the constant.
On closer inspection, the trace involving the second moments of $\bw$ is found to be
\begin{align*}
  \tr \Big( \bPsi\E(\bw^\top\bH_\eta^2\bw)  + \bPsi^{-1} \E(\bw^\top\bw) \Big)
  &= \sum_{i,j=1}^m \left\{ \psi_{ij} \tr(\bH_\eta^2\tilde\bW_{ij}) + \psi_{ij}^- \tr(\tilde\bW_{ij}) \right\}
\end{align*}
by the results of the derivations in \cref{{apx:qw}} \colp{\mypageref{eq:trCEwDw}}.
In the above, we had defined $\psi_{ij}^-$ to be the $(i,j)$'th element of $\bPsi^{-1}$, and
\[
  \tilde\bW_{ij} 
  = \E(\bw_{\bigcdot i}\bw_{\bigcdot j}^\top)
  =  \bV_{w}[i,j] + \tilde\bw_{\bigcdot i} \tilde\bw_{\bigcdot j}^\top,
\]
where $\bV_{w}[i,j] \in \bbR^{n\times n}$ refers to the $(i,j)$'th submatrix block of $\bV_w$, and the $n$-vector $\tilde \bw_{\bigcdot j} = \big( \E w_{ij} \big)_{i=1}^n$ is the expected value of the random effects for class $j$.
Specifically, when the error precision is of the form $\bPsi = \diag(\psi_1,\dots,\psi_m)$, this trace reduces to
\vspace{-0.2em}
\begin{align*}
  \tr \Big( \bPsi\E(\bw^\top\bH_\eta^2\bw)  + \bPsi^{-1} \E(\bw^\top\bw) \Big)
  &= \sum_{j=1}^m \left\{ \psi_j \tr(\bH_\eta^2\tilde\bW_{jj}) + \psi_j^{-1} \tr(\tilde\bW_{jj}) \right\} \\
  &= \sum_{j=1}^m \tr\big( 
  (\myoverbrace{\psi_j\bH_\eta^2 + \psi_j^{-1}\bI_n}{\bSigma_{\theta,j}}) 
  \tilde\bW_{jj} \big)
\end{align*}

\vspace{-0.25em}
The bulk of the computational effort required to evaluate $Q(\theta)$ stems from the trace involving the second moments of $\bw$, and the fact that $\bH_\eta^2$ needs to be reevaluated each time $\theta = \{\balpha,\eta\}$ changes.
As discussed previously, each E-step takes $O(n^3m)$ time to compute the required first and second (approximate) posterior moments of $\bw$.
Once this is done, we can use the `front-loading of the kernel matrices' trick described in \cref{sec:efficientEM1}, which effectively renders the evaluation of $Q$ to be linear in $\theta$ (after an initial $O(n^2)$ procedure at the beginning).

As in the normal linear model, we employ a sequential update of the parameters (√† la expectation conditional maximisation algorithm) by solving the first order conditions \index{ECM algorithm}
\begingroup
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{8pt}
\begin{align}
  \frac{\partial}{\partial\eta}Q(\eta|\balpha)
  &= -\half \sum_{i,j=1}^m \psi_{ij} \tr \left( \frac{\partial \bH_\eta^2}{\partial\eta} \tilde\bW_{ij} \right) 
  + \tr\left( \bPsi \tilde\bw^\top  \frac{\partial \bH_\eta}{\partial\eta} (\tilde\by^* - \bone_n\balpha^\top)  \right) \label{eq:vemeta} \\
  \frac{\partial}{\partial\balpha}Q(\balpha|\eta)
  &= 2n\bPsi\balpha - 2 \sum_{i=1}^n \bPsi \big(\by_{i\bigcdot}^* - \tilde\bw^\top\bh_\eta(x_i) \big) \label{eq:vemalpha}
\end{align}
\endgroup
equated to zero, where $\bh_\eta(x_i) \in \bbR^n$ is the $i$'th row of the kernel matrix $\bH_\eta$.
We now present the update equations for the parameters.

\subsubsection{Update for kernel parameters $\eta$}

When only ANOVA RKHS scale parameters are involved, then the conditional solution of $\eta$ to \cref{eq:vemeta} can be found in closed-form, much like in the exponential family EM algorithm described in \cref{sec:expfamEM} \colp{\mypageref{sec:expfamEM}}.
Under the same setting as in that subsection, assume that only $\eta = \{\lambda_1,\dots,\lambda_p\}$ need be estimated, and for each $k=1,\dots,p$, we can decompose the kernel matrix as $\bH_\eta = \lambda_k \bR_k + \bS_k$ and its square as $\bH_\eta^2 = \lambda_k^2 \bR_k^2 + \lambda_k \bU_k + \bS_k^2$.
As a follow-on from \cref{eq:vemeta}, the conditional solution for $\lambda_k$ given the rest of the parameters is obtained by solving
%\vspace{-0.5em}
\begin{align*}
  \frac{\partial}{\partial\lambda_k}Q(\lambda_k|\balpha,\blambda_{-k})
  ={}& -\half \sum_{i,j=1}^m \psi_{ij} \tr \left( (2\lambda_k\bR_k^2 + \bU_k) \tilde\bW_{ij} \right) 
  + \tr\left( \bPsi \tilde\bw^\top \bR_k (\tilde\by^* - \bone_n\balpha^\top)  \right) \\
  ={}& - \lambda_k \sum_{i,j=1}^m \psi_{ij} \tr(\bR_k^2\tilde\bW_{ij})
  - \half \sum_{i,j=1}^m \psi_{ij} \tr(\bU_k\tilde\bW_{ij}) \\
  &+ \tr\left( \bPsi \tilde\bw^\top \bR_k (\tilde\by^* - \bone_n\balpha^\top)  \right) 
\end{align*} 
equals zero.
This yields the solution
\[
  \hat\lambda_k 
  = \frac{
  \tr\left( \bPsi \tilde\bw^\top \bR_k (\tilde\by^* - \bone_n\balpha^\top) \right)
  - \half \sum_{i,j=1}^m \psi_{ij} \tr(\bU_k\tilde\bW_{ij})
  }{
  \sum_{i,j=1}^m \psi_{ij} \tr(\bR_k^2\tilde\bW_{ij})
  }
\]
In the case of the independent I-probit model, where $\bPsi = \diag(\psi_1,\dots,\psi_m)$, $\hat\lambda_k$ has the form
\[
  \hat\lambda_k
  = \frac{
  \sum_{j=1}^m \psi_j \left( \tilde\bw^\top_{\bigcdot j} \bR_k (\tilde\by^*_{\bigcdot j} - \alpha_j\bone_n) - \half \tr(\bU_k\tilde\bW_{jj})\right)
  }{
  \sum_{j=1}^m \psi_j \tr(\bR_k^2\tilde\bW_{jj})
  }.
\]

\begin{remark}
  There is no closed-form solution for $\eta$ when the polynomial kernel is used, or when there are kernel parameters to optimise (e.g. Hurst coefficient or SE kernel lengthscale).
  In these situations, solutions for $\eta$ are obtained using numerical methods (i.e. employ quasi-Newton methods such as an L-BFGS algorithm \index{L-BFGS algorithm} for optimising $Q(\eta)$).
\end{remark}

\subsubsection{Update for intercepts \texorpdfstring{$\balpha$}{$\alpha$}}

It is easy to see that the unique solution to \cref{eq:vemalpha} is
\begin{align*}
  \hat\balpha
  &= \frac{1}{n} \bPsi^{-1} \left( \sum_{i=1}^n \bPsi \big(\by_{i\bigcdot}^* - \tilde\bw^\top\bh_\eta(x_i) \big) \right) 
  =\frac{1}{n} \sum_{i=1}^n \big(\by_{i\bigcdot}^* - \tilde\bw^\top\bh_\eta(x_i) \big) \in \bbR^m.
\end{align*}
Being free of $\bPsi$, the solution is the same whether the full or independent I-probit model is assumed.
Furthermore, we must have that $\sum_{j=1}^m \alpha_j = 0$ for identifiability, so as an additional step to satisfy this condition, the solution $\hat\balpha$ is centred.

\subsection{Summary}

%A summary of the variational EM algorithm is presented.
Notice that the evaluation of each component of the posterior depends on knowing the posterior distribution of the other, i.e. $q(\by^*)$ depends on $q(\bw)$ and vice-versa.
Similarly, each parameter update is obtained conditional upon the value of the rest of the parameters.
These circular dependencies are dealt with by way of an iterative updating scheme: with arbitrary starting values for the distributions $q^{(0)}(\by^*)$ and $q^{(0)}(\bw)$, and for the parameters $\theta^{(0)}$, each are updated in turn according to the above derivations.

\index{I-probit!ELBO}
The updating sequence is repeated until no significant increase in the convergence criterion, the ELBO, is observed.
The ELBO for the I-probit model is given by the quantity
\begin{align}
  \cL_q(\theta) = \half[nm] + \sum_{i=1}^n \log C_i(\theta) + \half\log \abs{\tilde\bV_w} - \half[n]\log\abs{\bPsi} - \half\sum_{i,j=1}^m \psi_{ij}^- \tr(\tilde\bW_{ij}),
\end{align}
where $\psi_{ij}^{-}$ is the $(i,j)$'th entry of $\bPsi^{-1}$, and $C_i(\theta)$ is the normalising constant of the density of $\tN_m(\balpha + \tilde\bw^\top\bh_\eta(x_i),\bPsi^{-1},\cC_{y_i})$, with $\cC_{y_i} = \{y_{iy_i}^* > y_{ik}^* | \forall k \neq y_i \}$.
That is,
\[
  C_i(\theta) = \idotsint\displaylimits_{\{y_{iy_i}^* > y_{ik}^* \,|\, \forall k \neq y_i \}} \phi(y_{i1}^*, \dots, y_{im}^*|\balpha + \tilde\bw^\top\bh_\eta(x_i), \bPsi^{-1}) \dint y_{i1}^* \cdots \dint y_{im}^*.
\]
Similar to the EM algorithm, each iteration of the algorithm increases the ELBO to a stationary point \citep{blei2017variational}.
Unlike the EM algorithm though, the variational EM algorithm does \emph{not} guarantee an increase in the marginal log-likelihood at each step, nor does it guarantee convergence to the global maxima of the log-likelihood.

Further, the ELBO expression to be maximised is often not convex, which means the algorithm may terminate at local modes, for which there may be many.
Note that the variational distribution with the higher ELBO value is the distribution that is closer, in terms of the KL divergence, to the true posterior distribution.
In our experience, multiple random starts alleviates this issue for the I-probit model.

\algrenewcommand{\algorithmiccomment}[1]{{\color{grymath} \hfill $\triangleright$ #1}}
\begin{algorithm}[p]
\caption{Variational EM for the I-probit model (fixed $\bPsi$)}\label{alg:varemiprobit}
\begin{algorithmic}[1]
  \Procedure{Initialisation}{}
    \State Initialise $\theta^{(0)} \gets \{\balpha^{(0)},\eta^{(0)}\}$
    \State $\tilde q^{(0)}(\bw) \gets \MN(\bzero,\bI_n,\bPsi)$
    \State $\tilde q^{(0)}(\by^*_{i\bigcdot}) \gets \tN_m(\tilde\balpha^{(0)},\bPsi^{-1},\cC_{y_i})$
    \State $t \gets 0$
  \EndProcedure 
  \Statex
  \While{not converged}{}
    \Procedure{Variational E-step}{}
      \For{$i=1,\dots,n$} \Comment{Update $\by^*$}
        \State $\tilde q^{(t+1)}(\by^*_{i \bigcdot}) \gets \tN_m\big(\tilde\balpha^{(t)} + \tilde\bw^{(t)\top} \bh_{\eta^{(t)}}(x_i), \bPsi, \cC_{y_i}\big)$ 
        \State $\tilde \by^{*(t+1)}_{i \bigcdot} \gets \E_{q^{(t+1)}} (\by^*_{i \bigcdot})$
      \EndFor
      \Statex
      \State $\tilde\bV_w^{(t+1)} \gets \big(\bPsi \otimes \bH_{\eta^{(t)}}^2 + \bPsi^{-1} \otimes \bI_n\big)^{-1}$ \Comment{Update $\bw$}
      \State $\vecc\tilde\bw^{(t+1)}  \gets \tilde\bV_w^{(t+1)} 
      (\bPsi \otimes \bH_{\eta^{(t)}}) \vecc (\tilde\by^{*(t+1)} - \bone_n\balpha^{(t)\top})$
      \State $\tilde q^{(t+1)}(\bw) \gets \N_{nm}\big( \vecc \tilde\bw^{(t+1)}, \tilde\bV_w^{(t+1)} \big)$ 
    \EndProcedure
    \Statex
    \Procedure{M-step}{}
      \If{ANOVA kernel (closed-form updates)} \Comment{Update $\eta$}
        \For{$k=1,\dots,p$} 
          \State $T_{1k} \gets \sum_{i,j=1}^m \psi_{ij} \tr(\bR_k^2\tilde\bW_{ij})$ 
          \State $T_{2k} \gets \tr\left( \bPsi \tilde\bw^\top \bR_k (\tilde\by^* - \bone_n\balpha^\top) \right) - \half \sum_{i,j=1}^m \psi_{ij} \tr(\bU_k\tilde\bW_{ij})$ 
          \State $\lambda_k^{(t+1)} \gets T_{2k}/T_{1k}$           
        \EndFor
      \Else
        \State $\eta^{(t+1)} \gets \argmax_\eta Q(\eta|\balpha^{(t)})$ by L-BFGS algorithm
      \EndIf
      \Statex
      \State $\ba \gets \frac{1}{n} \sum_{i=1}^n \big( \tilde\by^{*(t+1)}_{i \bigcdot} - \tilde\bw^{(t+1)\top} \tilde\bh_{\eta^{(t+1)}}(x_i) \big)$ \Comment{Update $\balpha$}
      \State $\balpha^{(t+1)} \gets \ba - \frac{1}{m}\sum_{j=1}^m a_j$
    \EndProcedure
    \Statex
    \State Calculate ELBO $\cL^{(t+1)}$
    \State $t \gets t + 1$
    \Statex
    \State $\big\{ \tilde q(\by^*), \tilde q(\bw), \hat\theta \big\} \gets \big\{ \tilde q^{(t)}(\by^*), \tilde q^{(t)}(\bw), \theta^{(t)} \big\}$ 
    \State \textbf{return} Variational densities $\{ \tilde q(\by^*), \tilde q(\bw) \}$
    \State \textbf{return} Estimates $\{ \hat\balpha, \hat\eta \}$
    \State \textbf{return} ELBO $\cL_q(\theta) = \cL^{(t)}$    
  \EndWhile
\end{algorithmic}
\end{algorithm}
