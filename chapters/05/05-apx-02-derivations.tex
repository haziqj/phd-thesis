\section{Derivation of the CAVI algorithm}

Let $\cZ = \{\by^*,\bw,\balpha,\eta,\bPsi \}$.
Approximate the posterior for $\cZ$ by a mean-field variational distribution
\begin{align*}
  p(\by^*, \bw, \alpha, \eta, \bPsi | \by) 
  &\approx q(\by^*)q(\bw)q(\balpha)q(\eta)q(\bPsi) \\
  &= \prod_{i=1}^n q(\by_{i}^*)q(\bw)q(\balpha)q(\eta)q(\bPsi).
\end{align*}
The first line is by assumption, while the second line follows from an induced factorisation on the latent propensities, as we will see later. 
If needed, we also assume that $q(\eta)$ factorises into its constituents components.
Recall that, for each $\xi\in\cZ$, the optimal mean-field variational density $\tilde q$ for $\xi$ satisfies
\[
  \log \tilde q(\xi) = \E_{-\xi} [\log p(\by, \cZ)] + \const \tag{\ref{eq:qtilde}}
\]
Write $\bff = \bH_\eta \bw \in \bbR^{n\times m}$.
The joint likelihood $p(\by, \cZ)$ is given by
\begin{align*}
  p(\by, \cZ) 
  &= p(\by|\cZ)p(\cZ) \\
  &= p(\by|\by^*) p(\by^* | \balpha,\bw,\eta,\bPsi) p(\bw|\bPsi) p(\eta) p(\bPsi)p(\balpha).
\end{align*}
For reference, the relevant distributions are listed below.

\begin{itemize}
  \item {\boldmath$p(\by|\by^*)$}. For each observation $i\in\{1,\dots,n\}$, given the corresponding latent propensities $\by^*_i = (y_{i1}^*,\dots,y_{im}^*)$, the distribution for $y_i$ is a degenerate distribution which depends on the $j$'th component of $\by^*_i$ being largest, where the value observed for $y_i$ was $j$. Since each of the $y_i$'s are independent, everything is multiplicative.
  \begin{align*}
    p(\by|\by^*) &= \prod_{i=1}^n \prod_{j=1}^m p_{ij} 
    = \prod_{i=1}^n \prod_{j=1}^m \ind[y_{ij}^* 
    = \max_k y_{ik}^*]^{\ind[y_i = j]}.
  \end{align*}
  
  \item {\boldmath$p(\by^*|\balpha,\bw,\eta,\bPsi)$}. Given values for the parameters and I-prior random effects, the distribution of the latent propensities is matrix normal
  \[
    \by^*|\balpha,\bw,\eta,\bPsi \sim \MN_{n,m}(\bone_n\balpha^\top + \bH_\eta\bw, \bI_n, \bPsi^{-1}).
  \]
%  Equivalently, 
%  \[
%    \vecc \by^* | \balpha,\bw,\eta,\bPsi \sim \N_{nm}\big(\vecc(\bone_n\balpha^\top + \bH_\eta\bw), \bPsi^{-1} \otimes \bI_n \big).
%  \]
%  The $n$ rows of $\by^* = (\by_1^{*\top},\dots,\by_n^{*\top})^\top$ are independent of each other, with each row following a $m$-variate normal distribution $\by_i^* \sim \N_{m}(\balpha + \bff(x_i), \bPsi^{-1})$.
  Write $\bmu = \bone_n\balpha^\top + \bH_\eta\bw$.
  Its pdf is
  \begin{align*}
    p(\by^*|\balpha,\bw,\eta,\bPsi)
%    &= \prod_{i=1}^n \phi(\by_i^*|\balpha + \bff(x_i), \bPsi^{-1}) \\
    &= \exp \left[-\half[nm]\log 2\pi + \half[n]\log\abs{\bPsi} - \half\tr \big((\by^* - \bmu) \bPsi (\by^* - \bmu)^\top  \big)  \right] \\
    &= \exp \left[-\half[nm]\log 2\pi + \half[n]\log\abs{\bPsi} - \half\sum_{i=1}^n (\by^*_i - \bmu_i)^\top \bPsi (\by^*_i - \bmu_i)   \right],
  \end{align*}
  where $\by^*_i \in\bbR^m$ and $\bmu_i \in\bbR^m$ are the rows of $\by^*$ and $\bmu$ respectively.
  The second line follows directly from the definition of the trace, but  also emanates from the fact that $\by_i^*$ are independent multivariate normal with mean $\bmu_i$ and variance $\bPsi^{-1}$.
  
  \item {\boldmath$p(\bw|\bPsi)$}. The $\bw$'s are normal random matrices $\bw \sim \N_{n,m}(\bzero, \bI_n,\bPsi)$ with pdf
  \begin{align*}
    p(\bw|\bPsi) 
    &= \exp \left[-\half[nm]\log 2\pi - \half[n]\log\abs{\bPsi} - \half\tr \big( \bw \bPsi^{-1} \bw^\top \big)  \right] \\
    &= \exp \left[-\half[nm]\log 2\pi - \half[n]\log\abs{\bPsi} - \half\sum_{i=1}^n  \bw_i^\top \bPsi^{-1} \bw_i   \right].
  \end{align*}
  
  \item {\boldmath$p(\eta)$}. The most common scenario would be $\eta = \{\lambda_1,\dots,\lambda_p\}$ only. In this case, choose independent normal priors for each $\lambda_k \sim \N(m_k,v_k)$, $k=1,\dots,p$, whose pdf is
  \begin{align*}
    p(\eta) = \prod_{k=1}^p \exp\left[-\half\log 2\pi - \half\log v_k - \frac{1}{2v_k^2} (\lambda_k - m_k)^2 \right].
  \end{align*} 
  An improper prior $p(\eta) \propto \const$ can be used as well, and this is the same as letting $m_k \to 0$ and $v_k\to 0$.
  The resulting posterior will be proper.
  If $\eta$ contains other parameters as well, such as the Hurst coefficient $\gamma \in (0,1)$, SE lengthscale $l >0$ or polynomial offset $c>0$, then appropriate priors should be used to match the support of the parameter.
  Choices include $p(\gamma) = \ind\big(\gamma \in (0,1)\big)$ and $l,c \sim \Gamma(a,b)$.
  
  \item {\boldmath$p(\bPsi)$}. For the precision matrix, a Wishart prior with scale matrix $\bG$ and $g$ degrees of freedom, denoted $\bPsi \sim \Wis_m(\bG,g)$, is convenient. It has pdf
  \[
    p(\bPsi) = \exp\left[\const + \half[g-m-1]\log \abs{\bPsi} - \half\tr(\bG^{-1}\bPsi)  \right].
  \]
  For the independent I-probit model, $\bPsi = \diag(\sigma_1^{-2},\dots,\sigma_m^{-2})$, and we choose independent Gamma distributions for each precision $\sigma_j^{-2} \sim \Gamma(r_j,s_j)$, where $r_j$ and $s_j$ are the shape and scale parameters.
  Then,
  \[
    p(\bPsi) = \prod_{j=1}^m \exp\left[\const + (r_j - 1)\log \sigma_j^{-2} - \frac{\sigma_j^{-2}}{s_j}\right].
  \] 
  
  \item {\boldmath$p(\balpha)$}. Choose independent normal priors for the intercept, $\alpha_j \sim \N(a_j,A_j)$ for $j=1,\dots,m$. The pdf is
  \[
    p(\balpha) = \prod_{j=1}^m \exp\left[\log 2\pi - \log A_j - \frac{1}{2A_j} (\alpha_j - a_j)^2  \right].
  \]
\end{itemize}

\begin{remark}
  The priors on the parameters $\{ \balpha,\eta \}$ can be set to very vague or even improper priors, and the resulting posterior will still yield a proper distribution.
  Using improper priors eases the algebra slightly.
  For the precision matrix $\bPsi$, it is best to stick with the Wishart prior to avoid positive-definite issues, unless the independent I-probit model is used, in which case Jeffreys' prior for the precisions $p(\sigma_j^{-2})\propto \sigma_j^2$ is a convenient choice.
\end{remark}

\subsubsection[Derivation of q ystar]{Derivation of $\tilde q(\by^*)$}

The rows of $\by^*$ are independent, and thus we can consider the variational density for each $\by_i^*$ separately.
Consider the case where $y_i$ takes one particular value $j \in \{1,\dots,m\}$. The mean-field density $q(\by_{i}^*)$ for each $i=1,\dots,n$ is found to be
\begin{align*}
  \log \tilde q(\by_{i}^*) 
  &=  \ind[y_{ij}^* = \max_k y_{ik}^*] \, \E_{\cZ\backslash\{\by^*\}\sim q} \left[ - \half (\by^*_i - \bmu_i)^\top \bPsi (\by^*_i - \bmu_i)  \right] + \const \\
  &= \ind[y_{ij}^* = \max_k y_{ik}^*] \, \left[ - \half (\by^*_i - \tilde\bmu_i)^\top \tilde\bPsi (\by^*_i - \tilde\bmu_i)  \right] + \const \\
%  &\equiv
%  \begin{cases}
%    \prod_{k=1}^m \N(\tilde f_{ik}, 1) & \text{ if } y_{ij}^* > y_{ik}^*, \forall k \neq j \\
%    0 & \text{ otherwise} \\
%  \end{cases}
  &\equiv
  \begin{cases}
    \phi(\by_i^*|\tilde\bmu_i,\tilde\bPsi) & \text{ if } y_{ij}^* > y_{ik}^*, \forall k \neq j \\
    0 & \text{ otherwise} \\
  \end{cases}
\end{align*}
where $\tilde\bmu_i = \E\balpha + (\E\bH_\eta \E\bw)_i$, and expectations are taken under the optimal mean-field distribution $\tilde q$. 
The distribution $q(\by_i^*)$ is a truncated $m$-variate normal distribution such that the $j$'th component is always largest. 
Unfortunately, the expectation of this distribution cannot be found in closed-form, and must be approximated by techniques such as Monte Carlo integration.
If, however, the independent I-probit model is used and $\tilde\bPsi$ is diagonal, then \hltodo{Lemma X} provides a simplification.

\subsubsection{Derivation of $\tilde q(\bw)$}

The terms involving $\bw$ in \cref{eq:qtilde} are the $p(\by^*|\balpha,\bw,\eta,\bPsi)$ and $p(\bw|\bPsi)$ terms, and the rest are absorbed into the constant.
The easiest way to derive $\tilde q(\bw)$ is to vectorise $\by^*$ and $\bw$.
We know that
\begin{gather*}
  \vecc \by^* |  \balpha,\bw,\eta,\bPsi \sim \N_{nm}\big(\vecc (\bone_n\balpha^\top + \bH_\eta\bw), \bPsi^{-1}\otimes\bI_n\big) \\
  \text{and}\\
  \vecc \bw | \bPsi \sim \N_{nm} (\bzero, \bPsi\otimes\bI_n)
\end{gather*}
using properties of matrix normal distributions.
We also use the fact that $\vecc (\bH_\eta\bw) = (\bI_m \otimes \bH_\eta)\vecc\bw$.  %\diag(\bH_\eta,\dots,\bH_\eta)\vecc\bw = 
For simplicity, write $\bar\by^* = \vecc(\by^* - \bone_n\balpha^\top)$, and $\bM = (\bI_m \otimes \bH_\eta)$.
Thus,
\begin{align*}
  \log \tilde q(\bw) 
  &= \E_{\cZ\backslash\{\bw \}\sim q} \left[ 
  -\half (\bar\by^* - \bM\vecc\bw )^\top(\bPsi^{-1} \otimes \bI_n)^{-1} (\bar\by^* - \bM\vecc\bw )
  \right] \\
  &\phantom{==} + \E_{\cZ\backslash\{\bw \}\sim q} \left[ 
  -\half (\vecc \bw )^\top(\bPsi \otimes \bI_n)^{-1} \vecc (\bw ) \right] + \const \\
  &= -\half\E_{\cZ\backslash\{\bw \}\sim q} \left[ 
  (\vecc\bw)^\top \Big( \,
  \greyoverbrace{\bM^\top(\bPsi \otimes \bI_n)\bM + (\bPsi^{-1} \otimes \bI_n)}{\bA} 
  \, \Big) \vecc (\bw )
  \right] \\
  &\phantom{==} + \E_{\cZ\backslash\{\bw \}\sim q} \Big[ 
  \greyoverbrace{\bar\by^{*\top} (\bPsi \otimes \bI_n) \bM}{\ba^\top} \vecc (\bw )
  \Big] + \const \\
  &= -\half\E_{\cZ\backslash\{\bw \}\sim q} \left[
  (\vecc\bw - \bA^{-1}\ba)^\top \bA (\vecc\bw - \bA^{-1}\ba)
  \right] + \const
\end{align*}
This is recognised as a multivariate normal of dimension $nm$ with mean and precision given by $\vecc \tilde\bw = \E[\bA^{-1}\ba]$ and $\tilde\bV_w ^{-1}= \E[\bA]$ respectively.
With a little algebra, we find that
\begin{align*}
  \bV_w^{-1} 
  &= \E_{\cZ\backslash\{\bw \}\sim q} [\bA] \\
  &= \E_{\cZ\backslash\{\bw \}\sim q} \left[(\bI_m \otimes \bH_\eta)^\top(\bPsi \otimes \bI_n)(\bI_m \otimes \bH_\eta) + (\bPsi^{-1} \otimes \bI_n) \right] \\
  &= \E_{\cZ\backslash\{\bw \}\sim q} \left[(\bPsi \otimes \bH_\eta^2) + (\bPsi^{-1} \otimes \bI_n) \right] \\
  &= (\tilde\bPsi \otimes \tilde\bH_\eta^2) + (\tilde\bPsi^{-1} \otimes \bI_n) 
\end{align*}
and making a first-order approximation $(\E \bA )^{-1} \approx \E [\bA^{-1}]$,
\begin{align*}
  \vecc \wtilde 
  &= \E_{\cZ\backslash\{\bw \}\sim q} [\bA^{-1}\ba] \\
  &= \tilde\bV_w \E_{\cZ\backslash\{\bw \}\sim q} \big[(\bI_m \otimes \bH_\eta) (\bPsi \otimes \bI_n) \vecc(\by^* - \bone_n\balpha^\top)  \big] \\
  &= \tilde\bV_w \E_{\cZ\backslash\{\bw \}\sim q} \big[(\bPsi \otimes \bH_\eta) \vecc(\by^* - \bone_n\balpha^\top)  \big] \\
  &= \tilde\bV_w (\tilde\bPsi \otimes \tilde\bH_\eta) \vecc(\tilde\by^* - \bone_n\tilde\balpha^\top).
\end{align*}

Ideally, we do not want to work with the $nm \times nm$ matrix $\bV_w$, since its inverse is expensive to compute.
We can exploit the Kronekcer product structure to compute the inverse efficiently.
Perform an orthogonal eigendecomposition of $\bH_\eta$ to obtain $\bH_\eta = \bV\bU\bV^\top$ and of $\bPsi$ to obtain $\bPsi = \bQ\bP\bQ^\top$.
This process takes $O(n^3 + m^3) \approx O(n^3)$ time if $m\ll n$.
Then, manipulate $\bV_w^{-1}$ as follows (for clarity, we drop the tildes from the notations):
\begin{align*}
  \bV_w^{-1} 
  &= (\bPsi \otimes \bH_\eta^2) + (\bPsi^{-1} \otimes \bI_n) \\
  &= (\bQ\bP\bQ^\top \otimes \bV\bU^2\bV^\top) + (\bQ\bP^{-1}\bQ^\top \otimes \bV\bV^\top) \\
  &= (\bQ \otimes \bV)(\bP \otimes \bU^2)(\bQ^\top \otimes \bV^\top) + 
  (\bQ \otimes \bV)(\bP^{-1} \otimes \bI_n)(\bQ^\top \otimes \bV^\top) \\
  &= (\bQ \otimes \bV)(\bP \otimes \bU^2 + \bP^{-1} \otimes \bI_n)(\bQ^\top \otimes \bV^\top) 
\end{align*}
Its inverse is 
\begin{align*}
  \bV_w 
  &=  (\bQ^\top \otimes \bV^\top)^{-1}(\bP \otimes \bU^2 + \bP^{-1} \otimes \bI_n)^{-1} (\bQ \otimes \bV)^{-1} \\
  &= (\bQ \otimes \bV)(\bP \otimes \bU^2 + \bP^{-1} \otimes \bI_n)^{-1}(\bQ^\top \otimes \bV^\top)
\end{align*}
which is easy to compute since the middle term is an inverse of diagonal matrices.






%Let $\bA_j = \E[\mathbf H_{\lambda_j}^2] + \bI_{n}$ and $\ba_j = \E[\mathbf H_{\lambda_j}](\E[\by_j^*] - \E[\alpha_j]\bone_{n})$. Then, using the fact that
%\[
%  \bw_j^\top \bA_j \bw_j - 2 \ba_j^\top\bw_j = (\bw_j - \bA_j^{-1}\ba_j)^\top\bA_j(\bw_j - \bA_j^{-1}\ba_j),
%\]
%we see the $\log \tilde q(\bw)$ is a sum of quadratic terms in $\bw_j$, and we recognise this as the kernel of the product of indepdendent multivariate normal densities. Therefore, for each $j=1,\dots,m$,
%\[
%  \tilde q(\bw_j) \equiv \N(\bA_j^{-1}\ba_j, \bA_j^{-1}),
%\]
%and $\tilde q(\bw) = \prod_{j=1}^m \tilde q(\bw_j)$. Because of this induced factorisation, we can obtain mean-field densities for each $\bw_j$ separately. For convenience later in deriving the lower bound, we note that the second moment of $\tilde q(\bw_j)$ is equal to $\E[\bw_j\bw_j^\top] = \bA_j^{-1}(\bI_{n} + \ba_j\ba_j^\top\bA_j^{-1}) =: \btW_j$.

\subsubsection{$\tilde q(\lambda)$}

For $j = 1,\dots,m$,
\begin{align*}
  \log \tilde q(\lambda_j) 
  &= \E_{\by^*, \bw, \alpha} \left[ 
  - \half \sum_{j=1}^m \, \norm{\by_j^* - \alpha_j\bone_{n} - \lambda_j\bH\bw_j}^2  
  \right] + \const \\
  &= - \half \sum_{j=1}^m \E_{\by^*, \bw, \alpha} \left[ 
  \lambda_j^2 \, \bw_j^\top \bH^2 \bw_j 
  - 2\lambda_j (\by_j^* - \alpha_j\bone_{n})^\top \bH \bw_j \right] 
  + \const \\  
  &= - \half \sum_{j=1}^m \Big( \lambda_j^2 \tr \left(  \bH^2 \E[\bw_j \bw_j^\top] \right) - 2 \lambda_j (\E[\by_j^*] - \E[\alpha_j]\bone_{n})^\top \bH \E[\bw_j] \Big) + \const
\end{align*}
By completing the squares, we recognise this is as the kernel of the product of independent univariate normal densities. Thus, each $\lambda_j \sim \N(d_j/c_j, 1/c_j)$, where
\begin{gather*}
  c_j = \tr \left(  \bH^2 \E[\bw_j \bw_j^\top] \right) 
  \ \text{ and } \
  d_j = (\E[\by_j^*] - \E[\alpha_j]\bone_{n})^\top \bH \E[\bw_j].
\end{gather*}

Supposing we use the same covariance kernel (and therefore scale parameter) for each regression class, the distribution for $\lambda$ is easily seen as
\[
  \lambda \sim \N \left( \frac{\sum_{j=1}^m d_j}{\sum_{j=1}^m c_j}, \frac{1}{\sum_{j=1}^m c_j} \right).
\]

\subsubsection{$\tilde q(\alpha)$}

For $j = 1,\dots,m$, denote $\bH_i$ as the row vector of the kernel matrix $\bH$. Then,
\begin{align*}
  \log \tilde q(\alpha) 
  &= \E_{\by^*, \bw, \lambda} \left[ 
  - \half \sum_{j=1}^m \sum_{i=1}^n \left( y_{ij}^* - \alpha_j 
  - \lambda_j \textstyle\sum_{k=1}^n h(x_i, x_k)w_{kj} \right)^2  
  \right] + \const \\  
  &= - \half \sum_{j=1}^m \E_{\by^*, \bw, \lambda} \left[ 
  n \alpha_j^2 - 2\alpha_j \sum_{i=1}^n(y_{ij}^* - \lambda_j \bH_i \bw_j) 
  \right] + \const \\  
  &= - \half[n] \sum_{j=1}^m \left[ \left( \alpha_j - \frac{1}{n} \sum_{i=1}^n(\E[y_{ij}^*] - \E[\lambda_j] \bH_i \bw_j) \right)^2 \right] + \const \\  
\end{align*}
which is of course the kernel of the product of $m$ univariate normal densities, each with mean and variance 
\[
   \tilde \alpha_j = \frac{1}{n} \sum_{i=1}^n \big(\E [y_{ij}^*] - \E[\lambda_j]\bH_i \E[\bw_j]  \big)
   \ \text{ and } \ 
   v_{\alpha_j} = \frac{1}{n}.
\]

Suppose that we use a single intercept parameter $\alpha$. In this case, $\alpha$ is is also normally distributed with mean and variance
\[
   \tilde \alpha = \frac{1}{nm} \sum_{j=1}^m \sum_{i=1}^n \big(\E [y_{ij}^*] - \E[\lambda_j]\bH_i \E[\bw_j]  \big)
   \ \text{ and } \ 
   v_{\alpha} = \frac{1}{nm}.
\]

\subsection{Monitoring the lower bound}

A convergence criterion would be when there is no more significant increase in the lower bound $\cL$, as defined by
\begin{align*}
  \cL &= \int q(\by^*,\bw,\lambda,\alpha) \log \left[ \frac{p(\by,\by^*,\bw,\lambda,\alpha)}{q(\by^*,\bw,\lambda,\alpha)} \right] \d\by^* \d\bw \d\lambda \d\alpha \\
  &= \E[\log p(\by,\by^*,\bw,\lambda,\alpha)] - \E[\log q(\by^*,\bw,\lambda,\alpha)] \\[8pt]
  &= \cancel{\E\left[\log \prod_{i=1}^n \prod_{j=1}^m p(y_{i}|y_{ij}^*)\right]}
  + \E\left[ \log p(\by^* | \bff) \right]
  + \E\left[ \log p(\bw) \right] 
  + \cancel{\E\left[ \log p(\lambda) \right]}
  + \cancel{\E\left[ \log p(\alpha) \right]}  \\
  &\phantom{==} - \E\left[ \log q(\by^*) \right]
  - \E\left[ \log q(\bw) \right]
  - \E\left[ \log q(\lambda) \right]
  - \E\left[ \log q(\alpha) \right]
\end{align*}

Note that the categorical pmf $p(y_i|y_{ij}^*)$ becomes degenerate once the latent variables are known, so this term is cancelled out. With the exception of $q(\by^*)$, all of the distributions are Gaussian. The following results will be helpful. 

\begin{definition}[Differential entropy]
  The differential entropy $\cH$ of a pdf $p(x)$ is given by
  \[
    \cH(p) = -\int p(x) \log p(x) \d x = -\E_p[\log p(x)].
  \]
\end{definition}

\begin{lemma}\label{thm:normentropy}
  Let $p(x)$ be the pdf of a random variable $x$. Then if
  \begin{enumerate}[label=(\roman*)]
    \item $p$ is a univariate normal distribution with mean $\mu$ and variance $\sigma^2$,
    \[
      \cH(p) = \half (1 + \log 2\pi) + \half \log \sigma^2
    \]
    \item $p$ is a $d$-dimensional normal distribution with mean $\mu$ and variance $\Sigma$,
    \[
      \cH(p) = \half[d] (1 + \log 2\pi) + \half \log \vert \Sigma \vert 
    \]
  \end{enumerate}
\end{lemma}

\subsubsection{Terms involving distributions of $\by^*$}

\begin{align*}
  \E\left[ \log p(\by^* | \bff) \right] - \E\left[ \log q(\by^*) \right]
  &= \sum_{i=1}^n \sum_{j=1}^m \E \left[ \log p(y_{ij}^* | f_{ij}) \right] + \sum_{i=1}^n \cH\big(q(y_i^*)\big) \\
  &= \cancel{\sum_{i=1}^n \sum_{j=1}^m \left( -\half\log 2\pi -\half \E[y_{ij}^* - f_{ij}]^2 \right)} \\
  &\phantom{==} \cancel{+ \sum_{i=1}^n \sum_{j=1}^m \left( \half\log 2\pi + \half \E[y_{ij}^* - f_{ij}]^2 \right)} + \sum_{i=1}^n \log C_i \\
\end{align*}

\subsubsection{Terms involving distributions of $\bw$}

\begin{align*}
  \E\left[ \log p(\bw) \right] - \E\left[ \log q(\bw) \right]
  &= \sum_{j=1}^m \Big( \E\left[ \log p(\bw_j) \right] - \E\left[ \log q(\bw_j) \right] \Big)  \\
  &= \sum_{j=1}^m \left( -\half[n] \log 2\pi - \half \E[\bw_j^\top\bw_j] + \cH\big(q(\bw_j)\big) \right) \\
  &= \sum_{j=1}^m \left( \cancel{-\half[n] \log 2\pi} - \half\tr\left( \E[\bw_j\bw_j^\top]\right) + \half[n] (1 + \cancel{\log 2\pi}) - \half \log \abs{\bA_j} \right) \\
  &= \half[nm] - \half \sum_{j=1}^m \left( \tr \btW_j + \log \abs{\bA_j} \right)
\end{align*}

\subsubsection{Terms involving distribution of $q(\lambda)$}

\begin{align*}
  -\E\left[ \log q(\lambda) \right] 
  &= \sum_{j=1}^m \cH\big(q(\lambda_j)\big)  \\
  &= \sum_{j=1}^m \left( \half (1 + \log 2\pi) - \half \log c_j \right) \\
  &= \half[m](1 + \log 2\pi) - \half \sum_{j=1}^m \log c_j
\end{align*}
or if using single $\lambda$
\begin{align*}
  -\E\left[ \log q(\lambda) \right] 
  &= \half(1 + \log 2\pi) - \half  \log  \sum_{j=1}^m c_j .
\end{align*}

\subsubsection{Terms involving distribution of $q(\alpha)$}

\begin{align*}
  -\E\left[ \log q(\alpha) \right] 
  &= \sum_{j=1}^m \cH\big(q(\alpha_j)\big) \\
  &= \half[m] (1 + \log 2\pi - \log n)
\end{align*}
or if using single $\alpha$
\begin{align*}
  -\E\left[ \log q(\alpha) \right] 
  &= \half (1 + \log 2\pi - \log nm).
\end{align*}

\subsubsection{The lower bound}

\begin{align*}
  \cL 
  &= \sum_{i=1}^n \log C_i + \half[nm] - \half \sum_{j=1}^m \left( \tr \btW_j + \log \abs{\bA_j} \right) \\
  &\phantom{==} + \half[m](1 + \log 2\pi) - \half \sum_{j=1}^m \log c_j + \half[m] (1 + \log 2\pi - \log n) \\
  &= \half[m]\big(n + 2(1 + \log 2\pi) - \log n \big)
  - \half \sum_{j=1}^m \left( \tr \btW_j + \log \abs{\bA_j} + \log c_j \right) + \sum_{i=1}^n \log C_i
\end{align*}
Of course, if using either single $\alpha$ or single $\lambda$, then the formula needs to be adjusted accordingly.
