The parameters in the standard linear multinomial probit model is well known to be unidentified \citep{Keane1992,train2009discrete}, and we find this to be the case in the I-probit model as well.
Unrestricted probit models are not identified for two reasons.
Firstly, an addition of a non-zero constant $a\in\bbR$ to the latent variables $y_{ij}^*$'s in \cref{eq:latentmodel} will not change which latent variable is maximal, and therefore leaves the model unchanged.
It is for this reason that assumptions \ref{ass:A4} and \ref{ass:A5} are imposed.
Secondly, all latent variables can be scaled by some positive constant $c\in\bbR_{>0}$ without changing which latent variable is largest.
This means that $m$-variate normal distribution $\N_m\big(\bmu(x_i), \bPsi^{-1}\big)$ of the underlying latent variables $\by_{i\bigcdot}^*$ would yield the same class probabilities as the multivariate normal distribution $\N_m\big( a\bone_m + c\bmu(x_i), c^2\bPsi^{-1} \big)$, according to \cref{eq:pij}.
Therefore, the multinomial probit model is not identified as there exists more than one set of parameters for which the categorical likelihood $\prod_{i,j} p_{ij}$ is the same.

Identification issues in the probit model is resolved by setting one restriction on the intercepts $\alpha_1,\dots,\alpha_m$ (location) and $m+1$ restrictions on the precision matrix $\bPsi$ (scale).
Restrictions on the intercepts include $\sum_{j=1}^m \alpha_j = 0$ or setting one of the intercepts to zero.
In this work, we apply the former restriction to the I-probit model, as this is analogous to the requirement of zero-mean functions in the functional ANOVA decomposition.
If \ref{ass:A6} holds, then location identification is all that is needed to achieve identification.
However, if $\bPsi$ is a free parameter to be estimated, only $m(m-1)/2-1$ parameters are identified.
Many possible specifications of the restriction on $\bPsi$ is possible, depending on the number of alternatives $m$ and the intended effect of $\bPsi$ (to be explained shortly):
\begin{itemize}
  \item \textbf{Case {\boldmath $m=2$}} (minimum number of restrictions = 3).
  \[
    \bPsi = 
    \begin{pmatrix}[0.9]
    1 & \\
    0 &0 \\  
    \end{pmatrix},
    \text{ or }
    \bPsi = 
    \begin{pmatrix}[0.9]
    1 & \\
    0 &1 \\  
    \end{pmatrix} \vspace{-0.45em}
  \]
  \item \textbf{Case {\boldmath $m=3$}} (minimum number of restrictions = 4).
  \[
    \bPsi = 
    \begin{pmatrix}[0.9]
    1 & \\
    \psi_{12} &\psi_{22} \\  
    0 &0 &0
    \end{pmatrix},
    \text{ or }
    \bPsi = 
    \begin{pmatrix}[0.9]
    1 & \\
    0 &\psi_{22}  \\  
    0 &0 &\psi_{33}  
    \end{pmatrix} \vspace{-0.55em}
  \]
    \item \textbf{Case {\boldmath $m\geq 4$}} (minimum number of restrictions = $m+1$).
  \[
    \bPsi = 
    \begin{pmatrix}[0.9]
    1                    \\
    \psi_{12} &\psi_{22}  \\  
    \vdots    &\vdots    &\ddots \\
    \psi_{1,m-1}         &\psi_{2,m-1} &\cdots &\psi_{m-1,m-1} \\
    0         &0         &\cdots &0 &0
    \end{pmatrix},
    \text{ or }
    \bPsi = 
    \begin{pmatrix}
    \psi_{11} & \\
    &\psi_{22}  \\  
    &&\ddots \\
    &&&\psi_{mm} \\
    \end{pmatrix}
  \]
\end{itemize}

\begin{remark}
  Identification is most commonly achieved by fixing the latent propensities of one of the classes to zero and fixing one element the covariance matrix \citep{dansie1985parameter,bunch1991estimability}.
  Fixing the last class, say, to zero, i.e. $y_{im}^* = 0,\forall i=1,\dots,n$ has the effect of shrinking $\bPsi$ to an $(m-1)$ matrix, and thus one more restriction needs to be made (typically, $\bPsi_{11}$ is set to one).
  This speaks to the fact that the absolute values of the latent propensities themselves do not matter, and only their relative differences do.   
  We also remark that for the binary case ($m=2$), setting the latent propensities for the second class to zero and fixing the remaining variance parameter to unity yields
  \begin{align}
    p_{i1} 
    &= \Prob(y_{i1}^* > y_{i2}^* = 0) \nonumber \\
    &= \Prob\big(\alpha_1 + f_1(x_i) + \epsilon_{i1} > 0 \,|\, \epsilon_{i1} \iid \N(0,1) \big) \nonumber \\
    &= \Phi\big( \alpha_1 + f_1(x_i) \big) \label{eq:iprobitbin} 
  \end{align}
  and $p_{i2} = 1 - \Phi\big( \alpha_1 + f_1(x_i) \big)$,  $i=1,\dots,n$---the familiar binary probit model.
  Note that in the binary case only one set of latent propensities need to be estimated, so we can drop the subscript `1' in the above equations.
  In fact, for $m$ classes, only $m-1$ sets of regression functions need to be estimated (since one of them needs to be fixed), but in the multinomial presentation of this thesis we define regression functions for each class.
\end{remark}

\vspace{-0.1em}
Now, we turn to a discussion of the role of $\bPsi$ in the model.
In decision theory, the independence axiom states that an agent's choice between a set of alternatives should not be affected by the introduction or elimination of a choice option.
The probit model is suitable for modelling multinomial data where the independence axiom, which is also known as the \emph{independence of irrelevant alternatives} (IIA) assumption, is not desired. 
Such cases arise frequently in economics and social science, and the famous Red-Bus-Blue-Bus example is often used to illustrate IIA:
suppose commuters face the decision between taking cars and red busses. 
The addition of blue busses to commuters' choices should, in theory, be more likely chosen by those who prefer taking the bus over cars.
That is, assuming commuters are indifferent about the colour of the bus, commuters who are predisposed to taking the red bus would see the blue bus as an identical alternative.
 Yet, if IIA is imposed, then the three choices are distinct, and the fact that red and blue busses are substitutable is ignored.

To put it simply, the model is IIA if choice probabilities depend only on the choice in consideration, and not on any other alternatives.
In the I-probit model, or rather, in probit models in general, choice dependency is controlled by the error precision matrix $\bPsi$.
Specifically, the off-diagonal elements $\bPsi_{jk}$ capture the correlations between alternatives $j$ and $k$.
Allowing all $m(m+1)/2$ covariance elements of $\bPsi$ to be non-zero leads to the \emph{full I-probit model}, and would not assume an IIA position.
\cref{fig:iprobcovstr} illustrates the covariance structure for the marginal distribution of the latent propensities, $\bV_{y^*} = \bPsi \otimes \bH_\eta^2 + \bPsi^{-1} \otimes \bI_n$, and of the I-prior $\bV_f = \bPsi \otimes \bH_\eta^2$.

\newcommand{\matcol}{lsedpr}
\begin{figure}[hbt]
\vspace{-1em}
\centering\hspace{-13pt}
\begin{blockmatrixtabular}
\valignbox{
\begin{blockmatrixtabular}
&
\mblockmatrix{0.55in}{0in}{\footnotesize $j=1$}&
\mblockmatrix{0.55in}{0in}{\footnotesize $j=2$}&
\mblockmatrix{0.55in}{0in}{$\cdots$}&
\mblockmatrix{0.55in}{0in}{\footnotesize $j=m$}& \\
\mblockmatrix{0in}{0.55in}{\footnotesize $j=1$}&
\fblockmatrix[\matcol!39]{0.55in}{0.55in}{\footnotesize $\bV[1,1]$}& 
\fblockmatrix[\matcol!22]{0.55in}{0.55in}{\footnotesize $\bV[1,2]$}&
\fblockmatrix[\matcol!24]{0.55in}{0.55in}{\footnotesize $\cdots$}& 
\fblockmatrix[\matcol!46]{0.55in}{0.55in}{\footnotesize $\bV[1,m]$}\\
\mblockmatrix{0in}{0.55in}{\footnotesize $j=2$}&
\fblockmatrix[\matcol!22]{0.55in}{0.55in}{\footnotesize $\bV[2,1]$}& 
\fblockmatrix[\matcol!20]{0.55in}{0.55in}{\footnotesize $\bV[2,2]$}&
\fblockmatrix[\matcol!42]{0.55in}{0.55in}{\footnotesize $\cdots$}& 
\fblockmatrix[\matcol!40]{0.55in}{0.55in}{\footnotesize $\bV[2,m]$}\\
\mblockmatrix{0in}{0.55in}{\hspace{10pt}$\vdots$}&
\fblockmatrix[\matcol!24]{0.55in}{0.55in}{\footnotesize $\vdots$}& 
\fblockmatrix[\matcol!42]{0.55in}{0.55in}{\footnotesize $\vdots$}&
\fblockmatrix[\matcol!33]{0.55in}{0.55in}{\footnotesize $\ddots$}& 
\fblockmatrix[\matcol!30]{0.55in}{0.55in}{\footnotesize $\vdots$}\\
\mblockmatrix{0in}{0.55in}{\footnotesize $j=m$}&
\fblockmatrix[\matcol!46]{0.55in}{0.55in}{\footnotesize $\bV[m,1]$}& 
\fblockmatrix[\matcol!40]{0.55in}{0.55in}{\footnotesize $\bV[m,2]$}&
\fblockmatrix[\matcol!30]{0.55in}{0.55in}{\footnotesize $\cdots$}& 
\fblockmatrix[\matcol!20]{0.55in}{0.55in}{\footnotesize $\bV[m,m]$}\\
\end{blockmatrixtabular}
}&
\valignbox{\mblockmatrix{0.31in}{2.8in}{}}&
\valignbox{
\begin{blockmatrixtabular}
%&
\mblockmatrix{0.55in}{0in}{\footnotesize $j=1$}&
\mblockmatrix{0.55in}{0in}{\footnotesize $j=2$}&
\mblockmatrix{0.55in}{0in}{$\cdots$}&
\mblockmatrix{0.55in}{0in}{\footnotesize $j=m$}& \\
%\mblockmatrix{0in}{0.55in}{\footnotesize $j=1$}&
\fblockmatrix[\matcol!39]{0.55in}{0.55in}{\footnotesize $\bV[1,1]$}& 
\fblockmatrix[none]{0.55in}{0.55in}{}&
\fblockmatrix[none]{0.55in}{0.55in}{}& 
\fblockmatrix[none]{0.55in}{0.55in}{}\\
%\mblockmatrix{0in}{0.55in}{\footnotesize $j=2$}&
\fblockmatrix[none]{0.55in}{0.55in}{}& 
\fblockmatrix[\matcol!20]{0.55in}{0.55in}{\footnotesize $\bV[2,2]$}&
\fblockmatrix[none]{0.55in}{0.55in}{}& 
\fblockmatrix[none]{0.55in}{0.55in}{}\\
%\mblockmatrix{0in}{0.55in}{\hspace{10pt}$\vdots$}&
\fblockmatrix[none]{0.55in}{0.55in}{}& 
\fblockmatrix[none]{0.55in}{0.55in}{}&
\fblockmatrix[\matcol!33]{0.55in}{0.55in}{\footnotesize $\ddots$}& 
\fblockmatrix[none]{0.55in}{0.55in}{}\\
%\mblockmatrix{0in}{0.55in}{\footnotesize $j=m$}&
\fblockmatrix[none]{0.55in}{0.55in}{}& 
\fblockmatrix[none]{0.55in}{0.55in}{}&
\fblockmatrix[none]{0.55in}{0.55in}{}& 
\fblockmatrix[\matcol!20]{0.55in}{0.55in}{\footnotesize $\bV[m,m]$}\\
\end{blockmatrixtabular}
}&
\end{blockmatrixtabular}\\ 
\caption[Illustration of the covariance structure of the full I-probit model and the independent I-probit model.]{Illustration of the covariance structure of the full I-probit model (left) and the independent I-probit model (right). The full model has  $m^2$ blocks of $n \times n$ symmetric matrices, and the blocks themselves are arranged symmetrically about the diagonal. The independent model, on the other hand, has a block diagonal structure, and its sparsity induces simpler computational methods for estimation.}
\label{fig:iprobcovstr}
\vspace{-0.5em}
\end{figure}
%most economics articles prefer to estimate scaled probit models. in fact, it is an advantage of it! but do we care about the scale? maybe care more about IIA, which can't do without scales i suppose.

While it is an advantage to be able to model the correlations across choices (unlike in logistic models), there are applications where the IIA assumption would not adversely affect the analysis, such as classification tasks.
Some analyses might also be indifferent as to whether or not choice dependency exists.
In these situations, it would be beneficial, algorithmically speaking, to reduce the I-probit model to a simpler version by assuming $\bPsi = \diag(\psi_1,\dots,\psi_m)$, which would trigger an IIA assumption in the I-probit model.
We refer to this model as the \emph{independent I-probit model}.
The independence structure causes the distribution of the latent variables to be $y_{ij}^* \sim \N(\mu_k(x_i), \sigma_j^2)$ independently for $j=1,\dots,m$, where $\sigma_j^2 = \psi_j^{-1}$.
As a continuation of line \cref{eq:pij}, we can show the class probabilities $p_{ij}$ to be
\begingroup
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{10pt}
\begin{align}
  p_{ij} 
  &= \idotsint\displaylimits_{\{y_{ij}^* > y_{ik}^* | \forall k \neq j\}} 
  \prod_{k=1}^m \Big\{ \phi(y_{ik}^*|\mu_k(x_i), \sigma_k^2) \dint y_{ik}^* \Big\} \nonumber \\
  &= \int \mathop{\prod_{k=1}^m}_{k\neq j} 
  \Phi \left( \frac{y_{ij}^* - \mu_k(x_i)}{\sigma_k} \right) \,
   \phi(y_{ij}^*|\mu_j(x_i), \sigma_j^2)  \dint y_{ij}^* \nonumber \\
  &= \E_Z \Bigg[ \mathop{\prod_{k=1}^m}_{k\neq j} 
  \Phi \left(\frac{\sigma_j}{\sigma_k} Z + \frac{\mu_j(x_i) - \mu_k(x_i)}{\sigma_k} \right) \Bigg] \label{eq:pij2}
\end{align}
\endgroup
where $Z\sim\N(0,1)$, $\Phi(\cdot)$ its cdf, and $\phi(\cdot|\mu,\sigma^2)$ is the pdf of $X\sim\N(\mu,\sigma^2)$.
The equation \cref{eq:pij} is thus simplified to a unidimensional integral involving the Gaussian pdf and cdf, which can be computed fairly efficiently using quadrature methods.
