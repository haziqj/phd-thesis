The I-prior methodology can be used naïvely to fit a categorical regression model.
Suppose, as before, we observe data $\{(y_1,x_1),\dots,(y_n,x_n) \}$ where each $x_i \in \cX$, for $i=1,\dots,n$.
Here, the responses are categorical $y_i \in \{ 1,\dots,m \} =: 
\cM $, and additionally, write $y_i = (y_{i1},\dots,y_{im}) $ where the class responses $y_{ij}$ equal one if individual $i$'s response category is $y_i = j$, and zero otherwise.
In other words, there is exactly a single `1' at the $j$'th position in the vector $y_i = (y_{i1},\dots,y_{im})$, and zeroes everywhere else.
For $j=1,\dots,m$, we model 
\begin{equation}\label{eq:naiveclassmod}
  \begin{gathered}
    y_{ij} = \alpha + \alpha_j + f_j(x_i) + \epsilon_{ij}  \\
    (\epsilon_{i1},\dots,\epsilon_{im})^\top \iid \N_m(\bzero,\bPsi^{-1}).
  \end{gathered}
\end{equation}
The idea here being that we attempt to model the class responses $y_{ij}$ using class-specific regression functions $f_j$, and the class responses are assumed to be independent among individuals, but may or may not be correlated among classes for each individual.
The class correlations are manifest themselves in the variance of the errors $\bPsi^{-1}$, which is an $m\times m$ matrix.

Denote the regression function $f$ in \cref{eq:naiveclassmod} on the set $\cX\times\cM$ as $f(x_i,j) = \alpha_j + f_j(x_i)$.
This regression function can be seen as an ANOVA decomposition of the spaces $\cF_\cM$ and $\cF_\cX$ of functions over $\cM$ and $\cX$ respectively. 
That is, $\cF = \cF_\cM \oplus (\cF_\cM \otimes \cF_\cX)$ is a decomposition into the main effects of `class', and an interaction effect of the covariates for each class.
Let $\cF_\cM$ and $\cF_\cX$ be RKHSs respectively with kernels $a:\cM\times\cM\to\bbR$ and $b:\cX\times\cX\to\bbR$.
Then, the ANOVA RKKS $\cF$ possesses the reproducing kernel $h:(\cX \times \cM)^2 \to \bbR$ as defined by
\begin{align}\label{eq:anovaclass}
  b_\eta\big( (x,j), (x',j') \big) = a(j,j') + a(j,j')h_\eta(x,x').  
\end{align}
The kernel $h_\eta$ may be any of the kernels described in this thesis, ranging from the linear kernel, to the fBm kernel, or even an ANOVA kernel.
Choices for $a:\cM \times \cM \to \bbR$ include 
\begin{enumerate}
  \item \textbf{The Pearson kernel} (as defined in \cref{def:pearson}). With $J\sim\Prob$, a probability measure over $\cM$,
  \[
    a(j,j') = \frac{\delta_{jj'}}{\Prob(J=j)} - 1.
  \]
  \item \textbf{The identity kernel}. With $\delta$ denoting the Kronecker delta function,
  \[
    a(j,j') = \delta_{jj'}.
  \]
\end{enumerate}
The purpose of either of these kernels is to contribute to the class intercepts $\alpha_j$, and to associate a regression function in each class.
We have a slight preference for the identity kernel, which lends itself as being easy to handle computationally.
The only difference between the two is the inverse probability weighting per class that is applied in the Pearson kernel, but not in the identity kernel.

As a remark, the functions in $\cF_\cM$ and $\cF_\cX$ need necessarily be zero-mean functions (as per the functional ANOVA definition in \cref{def:anovarkks}).
What this means is that $\sum_{j=1}^m \alpha_j = 0$, $\sum_{j=1}^m f_j(x_i) = 0$, and $\sum_{i=1}^n f_j(x_i) = 0$.
In particular,
\begin{align*}
  \sum_{j=1}^m y_{ij} 
  &= \sum_{j=1}^m \left(\alpha + \alpha_j + f_j(x_i) \right) \\
  &= m\alpha + \cancelto{0}{\sum_{j=1}^m \alpha_j} + \cancelto{0}{\sum_{j=1}^m f_j(x_i)}
\end{align*}
and since $\sum_{j=1}^m y_{ij} = 1$, we have that $\alpha = 1/m$ and can thus be fixed to resolve identification.
The Pearson RKHS will contain zero mean functions, but the RKHS of constant functions induced by the identity kernel may not.
If this is the case, then it should be ensured that $\sum_{j=1}^m \alpha_j =0$ in other ways; perhaps during the estimation process.

With $f \in \cF$ the RKKS with kernel $h_\eta$, it is straightforward to assign an I-prior on $f$. 
It is in fact
\begin{align}\label{eq:naiveclassiprior}
  \begin{gathered}
    f(x_i,j) = \sum_{j'=1}^m\sum_{i'=1}^n a(j,j')\big(1 + h_\eta(x_i,x_{i'})\big) w_{i'j'} \\
    (w_{i'1},\dots,w_{i'm})^\top \sim \N_m(\bzero,\bPsi)
  \end{gathered}
\end{align}
assuming a zero prior mean $f_0(x,j) = 0$.
It is much convenient to work in vector and matrix form, so let us introduce some notation.
%Consider the naïve I-prior classification model, this time written in vector form
%\begin{gather*}
%  \overbrace{
%  \begin{pmatrix}
%    y_{i1} \\ \vdots \\ y_{im}
%  \end{pmatrix} 
%  }^{\by_i}
%  = 
%  \overbrace{  
%  \begin{pmatrix}
%    \alpha_{1} \\ \vdots \\ \alpha_{m}
%  \end{pmatrix} 
%  }^\balpha
%  +
%  \overbrace{    
%  \begin{pmatrix}
%    f(x_i,1) \\ \vdots \\ f(x_i,m)
%  \end{pmatrix} 
%  }^{\bff(x_i)}
%  +
%  \overbrace{      
%  \begin{pmatrix}
%    \epsilon_{i1} \\ \vdots \\ \epsilon_{im}
%  \end{pmatrix} 
%  }^{\bepsilon_i}
%  \\
%  \bepsilon_i \iid \N_m(\bzero,\bPsi^{-1}).
%\end{gather*}
Let $\bw$ (c.f. $\by$, $\bff$ and $\bepsilon$) be an $n \times m$ matrix whose $(i,j)$  entries contain $w_{ij}$ (c.f. $y_{ij}$, $f(x_i,j)$, and $\epsilon_{ij}$).
The row-wise entries of $\bw$ are independent of each other (independence assumption of the $n$ observations), while any two of their columns have covariance as specified in $\bPsi$.
This means that $\bw \sim \MN_{n,m}(\bzero, \bI_n,\bPsi)$ which implies $\vecc \bw \sim \N_{nm}(\bzero, \bPsi \otimes \bI_n)$, and similarly, $\bepsilon \sim \N_{nm}(\bzero, \bPsi^{-1} \otimes \bI_n)$.
Denote by $\bH_\eta$ the $n\times n$ kernel matrix with entries supplied by $1 + h_\eta$, and $\bA$ the $m\times m$ matrix with entries supplied by $a$.
From \cref{eq:naiveclassiprior}, we have that 
\[
  \bff = \bH_\eta \bw \bA \in \bbR^{n \times m},
\]
and thus $\vecc \bff \sim \N_{nm}(\bzero, \bA\bPsi\bA \otimes \bH_\eta^2)$.
As $\by = \balpha + \bff + \bepsilon$, where $\balpha \in \bbR^{n\times m}$ with $(i,j)$ entries given by $\alpha + \alpha_j = \alpha_j + 1/m$, by linearity we have that 
\begin{equation}
  \vecc \by \sim \N_{nm}\big(\vecc \balpha, (\bA\bPsi\bA \otimes \bH_\eta^2) + (\bPsi^{-1} \otimes \bI_n) \big)
\end{equation}
and
\begin{equation}
  \vecc \by|\vecc \bw \sim \N_{nm}\big(\vecc (\balpha  + \bH_\eta \bw \bA), (\bPsi^{-1} \otimes \bI_n) \big).
\end{equation}
which can then be estimated using the methods described in Chapter 4.

When using the identity kernel in conjunction with an assumption of iid errors ($\bPsi = \psi\bI_n$), the above distributions simplify further.
Specifically, the variance in the marginal distribution becomes 
\begin{align*}
  \Var(\vecc \by) 
  &= (\psi\bI_m \otimes \bH_\eta^2) + (\psi^{-1}\bI_m \otimes \bI_n) \\
  &= (\bI_m \otimes \psi\bH_\eta^2) + (\bI_m \otimes \psi^{-1}\bI_n) \\
  &= \bI_m \otimes (\myoverbrace{\psi\bH_\eta^2 + \psi^{-1}\bI_n}{\bV_y}).
\end{align*}
which implies independence and identical variances $\bV_y$ for the vectors $(y_{1j},\dots,y_{nj})^\top$ for each class $j=1,\dots,m$.
%, i.e. a block diagonal matrix structure $\Var(\vecc \by)  = \diag(\bV_y,\dots,\bV_y)$.
Evidently, this stems from the implied independence structure of the prior on $f$ too, since now $\Var (\vecc \bff) = \diag(\psi\bH_\eta^2,\dots, \psi\bH_\eta^2)$, which could be interpreted as having independent and identical I-priors on the regression functions for each class $\bff_j = \big(f(x_1,j),\dots,f(x_n,j)\big)^\top$.

%\begin{proof}
%  \[
%    f(x_i,j) = \sum_{j'=1}^m\sum_{i'=1}^n a(j,j')\big(1 + b_\eta(x_i,x_{i'})\big) w_{i'j'}
%  \]
%  
%  \[
%    \alpha_j = \sum_{j'=1}^m\sum_{i'=1}^n a(j,j') w_{i'j'}
%  \]
%  
%  \begin{align*}
%    \sum_{j=1}^m \alpha_j
%    &= \sum_{j=1}^m \sum_{j'=1}^m\sum_{i'=1}^n a(j,j') w_{i'j'} \\
%    &= \sum_{j=1}^m \sum_{j'=1}^m\sum_{i'=1}^n \delta_{jj'} w_{i'j'} \\
%    &= \sum_{j=1}^m\sum_{i'=1}^n  w_{i'j}
%  \end{align*}
%  
%  \begin{align*}
%    \sum_{j=1}^m f_j(x_i)
%    &= \sum_{j=1}^m \sum_{j'=1}^m\sum_{i'=1}^n a(j,j')b_\eta(x_i,x_{i'}) w_{i'j'} \\
%    &= \sum_{j=1}^m \sum_{j'=1}^m\sum_{i'=1}^n \delta_{jj'} b_\eta(x_i,x_{i'}) w_{i'j'} \\
%    &= \sum_{j=1}^m\sum_{i'=1}^n b_\eta(x_i,x_{i'}) w_{i'j}
%  \end{align*}
%  
%  \begin{align*}
%    \sum_{i=1}^n f_j(x_i)
%    &= \sum_{i=1}^n \sum_{j'=1}^m\sum_{i'=1}^n a(j,j')b_\eta(x_i,x_{i'}) w_{i'j'} \\
%    &= \sum_{i=1}^n \sum_{j'=1}^m\sum_{i'=1}^n \delta_{jj'} b_\eta(x_i,x_{i'}) w_{i'j'} 
%  \end{align*}
%\end{proof}


%Rearrange the $n$ observations per class.
%Let $\bff_j = \big(f_j(x_1),\dots,f_j(x_n)\big)^\top \in \bbR^n$.
%We can write the I-prior as $\bff_j = \bA_{jj} \cdot \bH\bw_j$
%Therefore, $\bff_j \sim \N_n(\bzero, \bPsi_{jj}\bA_{jj} \bH^2)$, and
%\begin{align*}
%  \Cov(\bff_j,\bff_k) &= \Cov(\bA_{jj} \cdot \bH\bw_j, \bA_{kk} \cdot \bH\bw_k) \\
%  &= \bA_{jj}\bA_{kk} \cdot \bH \Cov(\bw_j,\bw_k) \bH \\
%  &= \bA_{jj}\bA_{kk}\bPsi_{jk} \bH^2.
%\end{align*}

There are several downfalls to using the model described above.
Unlike in the case of continuous response variables, the normal I-prior model is highly inappropriate for categorical responses.
For one, it violates the normality and homoscedasticity assumptions of the errors.
For another, predicted values may be out of the range $[0,m]$ and thus poorly calibrated.
Furthermore, it would be more suitable if the class probabilities---the probability of an observation belonging to a particular class---were also part of the model.
In the next section, we propose an improvement to this naïve I-prior classification model by considering a probit-like transformation of the regression functions.





























