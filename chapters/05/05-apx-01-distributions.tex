\section{Some distributions and their properties}

This is a reference relating to the multivariate normal, conically truncated multivariate normal, matrix normal, Wishart, and gamma distributions for that we collate for convenience.
Of interest are probability density functions, first and second moments, and entropy (as defined in \hltodo{Chapter 3}).

\subsection{Multivariate normal distribution}

Let $X\in\bbR^d$ be distributed according to a multivariate normal (Gaussian) distribution with mean $\mu \in \bbR^d$ and covariance matrix $\Sigma \in\bbR^d$ (a square, symmetric, positive-definite matrix).
We say that $X\sim\N_d(\mu,\Sigma)$.
Then,
\begin{itemize}
  \item \textbf{Pdf}. $p(X|\mu,\Sigma) = (2\pi)^{-d/2}|\Sigma|^{-1/2}\exp\big(-\half (X-\mu)^\top\Sigma^{-1}(X-\mu)\big)$.
  \item \textbf{Moments}. $\E X=\mu$, $\E [XX^\top] = \Sigma + \mu \mu^\top$.
  \item \textbf{Entropy}. $H(p) = \half \log \abs{2\pi e \Sigma} = \half[d](1 + \log 2\pi) + \half\log\abs{\Sigma}$.
\end{itemize}

\begin{lemma}[Properties of multivariate normal]
  Assume that $X \sim \N_d(\mu,\Sigma)$ and $Y \sim \N_{d}(\nu,\Psi)$, where
  \[
    X = 
    \begin{pmatrix}
      X_a \\ X_b
    \end{pmatrix},
    \hspace{0.5cm}
    \mu = 
    \begin{pmatrix}
      \mu_a \\ \mu_b
    \end{pmatrix},
    \hspace{0.25cm}\text{and}\hspace{0.25cm}
    \Sigma = 
    \begin{pmatrix}
      \Sigma_a    &\Sigma_{ab} \\
      \Sigma_{ab}^\top &\Sigma_b \\
    \end{pmatrix}.
  \]
  Then,
  \begin{itemize}
    \item \textbf{Marginal distributions}.
    \[
      X_a \sim \N_{\dim X_a}(\mu_a,\Sigma_a)
      \hspace{0.5cm}\text{and}\hspace{0.5cm}
      X_b \sim \N_{\dim X_b}(\mu_b,\Sigma_b).
    \]
    \item \textbf{Conditional distributions}.
    \[
      X_a|X_b \sim \N_{\dim X_a}(\tilde\mu_a,\tilde\Sigma_a)
      \hspace{0.5cm}\text{and}\hspace{0.5cm}
      X_b \sim \N_{\dim X_b}(\tilde\mu_b,\tilde\Sigma_b),
    \]
    where
    \begin{alignat*}{5}
      & \tilde\mu_a 
      &&= \mu_a + \Sigma_{ab}\Sigma_b^{-1}(X_b-\mu_b)
      &&\hspace{1cm}
      &&\tilde\mu_b 
      &&= \mu_b + \Sigma_{ab}^\top\Sigma_a^{-1}(X_a-\mu_a) \\
      &\tilde\Sigma_a 
      &&= \Sigma_a -  \Sigma_{ab}\Sigma_b^{-1}\Sigma_{ab}^\top
      &&\hspace{1cm}
      &&\tilde\Sigma_b 
      &&= \Sigma_b -  \Sigma_{ab}^\top\Sigma_a^{-1}\Sigma_{ab} 
    \end{alignat*}
    \item \textbf{Linear combinations}. 
    \[
      AX + BY + C \sim \N_{d}(A\mu + B\nu + C, A\Sigma A^\top + B\Psi B^\top)
    \]
    where $A$ and $B$ are appropriately sized matrices, and $C\in\bbR^d$.
    \item \textbf{Product of Gaussian densities}. 
    \[
      p(X|\mu,\Sigma)p(Y|\nu,\Psi) \propto p(Z|m,S)
    \]
    where $p(Z)$ is a Gaussian density, $m = S(\Sigma^{-1}\mu + \Psi^{-1}\nu)$ and $S= (\Sigma^{-1} + \Psi^{-1})^{-1}$.
    The normalising constant is equal to the density of $\mu\sim\N(\nu,\Sigma + \Psi)$.
  \end{itemize}
\end{lemma}

\begin{proof}
  Omitted---see \citet[ยง8]{petersen2008matrix}.
\end{proof}

Frequently, in Bayesian statistics especially, the following identities will be useful in deriving posterior distributions involving multivariate normals.

\begin{lemma}
  Let $x,b\in\bbR^d$ be a vector, $X,B\in\bbR^{n\times d}$ a matrix, and $A \in \bbR^{d \times d}$ a symmetric, invertible matrix.
  Then,
  \begin{align*}
    -\half x^\top A x + b^\top x 
    &= -\half (x - A^{-1}b)^\top A (x - A^{-1}b) + \half b^\top A^{-1} b \\
    -\half \tr (X^\top A X) + \tr(B^\top X)
    &= -\half \tr\big((X - A^{-1}B)^\top A(X - A^{-1}B) \big) + \half\tr(B^\top A^{-1} B).
  \end{align*}
\end{lemma}

\begin{proof}
  Omitted---see \citet[ยง8.1.6]{petersen2008matrix}.
\end{proof}

\subsection{Matrix normal distribution}

The matrix normal distribution is an extension of the Gaussian distribution to matrices.
Let $X\in\bbR^{n \times m}$ matrix, and let $X$ follow a matrix normal distribution with mean $\mu\in\bbR^{n \times m}$ and row and column variances $\Sigma \in \bbR^{n \times n}$ and $\Psi \in \bbR^{m \times m}$ respectively, which we denote by $X\sim\MN_{n,m}(\mu,\Sigma,\Psi)$.
Then,
\begin{itemize}
  \item \textbf{Pdf}. $p(X|\mu,\Sigma,\Psi) = (2\pi)^{-nm/2}|\Sigma|^{-m/2}|\Psi|^{-n/2} e^{-\half \tr \big(\Psi^{-1}(X-\mu)^\top\Sigma^{-1}(X-\mu)\big)}$.
  \item \textbf{Moments}. $\E X=\mu$, $\Var(X_{i \bigcdot }) = \Psi$ for $i=1,\dots,n$, and $\Var(X_{\bigcdot j}) = \Sigma$ for $j=1,\dots,m$. 
  \item \textbf{Entropy}. $H(p) = \half \log \abs{2\pi e (\Psi \otimes \Sigma)} = \half[nm](1 + \log 2\pi) + \half\log\abs{\Sigma}^m\abs{\Psi}^n$.
\end{itemize}

In the above, `$\otimes$' denotes the Kronecker matrix product defined by
\[
  \Psi \otimes \Sigma = 
  \begin{pmatrix}
    \Psi_{11}\Sigma &\Psi_{12}\Sigma &\cdots &\Psi_{1m}\Sigma \\
    \Psi_{21}\Sigma &\Psi_{22}\Sigma &\cdots &\Psi_{2m}\Sigma \\    
    \vdots & \vdots &\ddots  &\vdots \\
    \Psi_{m1}\Sigma &\Psi_{m2}\Sigma &\cdots &\Psi_{mm}\Sigma \\
  \end{pmatrix} \in \bbR^{nm\times nm}.
\]
Of use will be these properties of the Kronecker product \citep{zhang2013kronecker}.
\begin{itemize}
  \item \textbf{Bilinearity and associativity}. For appropriately sized matrices $A$, $B$ and $C$, and a scalar $\lambda$,
  \begin{align*}
    A \otimes (B + C) &= A \otimes B + A \otimes C \\
    (A + B) \otimes C &= A \otimes C + B \otimes C \\
    \lambda A \otimes B &= A \otimes \lambda B = \lambda(A \otimes B) \\
    (A \otimes B) \otimes C &= A \otimes (B \otimes C)
  \end{align*}
  \item \textbf{Non-commutative}. In general, $A \otimes B \neq B \otimes A$, but they are \emph{permutation equivalent}, i.e. $A \otimes B \neq P(B \otimes A)Q$ for some permutation matrices $P$ and $Q$.
  \item \textbf{The mixed product property}. $(A \otimes B)(C \otimes D) = AC \otimes BD$.
  \item \textbf{Inverse}. $A \otimes B$ is invertible if and only if $A$ and $B$ are both invertible, and $(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}$.
  \item \textbf{Transpose}. $(A \otimes B)^\top = A^\top \otimes B^\top$.
  \item \textbf{Determinant}. If $A$ is $n\times n$ and $B$ is $m \times m$, then $\abs{A \otimes B} = \abs{A}^m \abs{B}^n$. Note that the exponent of $\abs{A}$ is the order of $B$ and vice versa.
  \item \textbf{Trace}. Suppose $A$ and $B$ are square matrices. Then $\tr (A \otimes B) = \tr A \tr B$.
  \item \textbf{Rank}. $\rank (A \otimes B) = \rank A \rank B$.
  \item \textbf{Matrix equations}. $AXB = C \Leftrightarrow (B^\top \otimes A) \vecc X = \vecc (AXB) = \vecc C$.
\end{itemize}
The vectorisation operation `$\vecc$' stacks the columns of the matrices into one long vector, for instance,
\[
  \vecc \Psi = (\Psi_{11},\dots,\Psi_{m1},\Psi_{12},\dots,\Psi_{m2},\dots,\Psi_{1m},\dots,\Psi_{mm})^\top \in \bbR^{m \times m}.
\]

\begin{lemma}[Equivalence between matrix and multivariate normal]
  $X\sim\MN_{n,m}(\mu,\Sigma,\Psi)$ if and only if $\vecc X\sim \N_{nm}(\vecc\mu,\Psi \otimes \Sigma)$.
\end{lemma}

\begin{proof}
  In the exponent of the matrix normal pdf, we have
  \begin{align*}
    -\half \tr \big(\Psi^{-1}(X-\mu)^\top &\Sigma^{-1}(X-\mu)\big) \\
    &= -\half \vecc(X-\mu)^\top \vecc(\Sigma^{-1}(X-\mu)\Psi^{-1}) \\
    &= -\half \vecc(X-\mu)^\top (\Psi^{-1} \otimes \Sigma^{-1}) \vecc(X-\mu) \\
    &= -\half (\vecc X- \vecc \mu)^\top (\Psi \otimes \Sigma)^{-1} (\vecc X- \vecc \mu).     
  \end{align*} 
  Also, $|\Sigma|^{-m/2}|\Psi|^{-n/2} = |\Psi \otimes \Sigma|^{-1/2}$.
  This converts the matrix normal pdf to that of a multivariate normal pdf.
\end{proof}

Some useful properties of the matrix normal distribution are listed:
\begin{itemize}
  \item \textbf{Expected values}.
  \begin{align*}
    \E (X-\mu)(X-\mu)^\top &= \tr(\Psi)\Sigma \in \bbR^{n\times n} \\
    \E (X-\mu)^\top(X-\mu) &= \tr(\Sigma)\Psi \in \bbR^{m\times m} \\
    \E XAX^\top &= \tr(A^\top\Psi)\Sigma + \mu A\mu^\top \\
    \E X^\top BX &= \tr(\Sigma B^\top)\Psi + \mu^\top B\mu \\   
    \E X CX &=  \Sigma C^\top\Psi  + \mu C \mu \\    
  \end{align*} 
  \item \textbf{Transpose}. $X^\top \sim \MN_{m,n}(\mu^\top, \Psi, \Sigma)$.
  \item \textbf{Linear transformation}. Let $A \in \bbR^{a \times n}$ be of full-rank $a \leq n$ and $B \in \bbR^{m \times b}$ be of full-rank $b\leq m$. Then $AXB  \sim \MN_{a,b}(\mu^\top, A\Sigma A^\top, B^\top \Psi B)$.
  \item \textbf{Iid}. If $X_i \iid \N_m(\mu,\Psi)$ for $i=1,\dots,n$, and we arranged these vectors row-wise into the matrix $X = (X_1^\top,\dots,X_n^\top)^\top \in \bbR^{n\times m}$, then $X \sim \MN(1_n \mu^\top, I_n, \Psi)$.
\end{itemize}

\subsection{Truncated univariate normal distribution}

Let $X \sim \N(\mu,\sigma^2)$ with $X$ lying in the interval $(a,b)$.
Then we say that $X$ follows a truncated normal distribution, and we denote this by $X\sim\tN(\mu,\sigma^2,a,b)$.
Let $\alpha = (a-\mu)/\sigma$, $\beta = (b-\mu)/\sigma$, and $C = \Phi(\beta) - \Phi(\alpha)$.
Then,
\begin{itemize}
  \item \textbf{Pdf}. $p(X|\mu,\sigma,a,b) = C^{-1} (2\pi\sigma^2)^{-1/2}e^{-\frac{1}{2\sigma^2} (X-\mu)^2} = \sigma C^{-1} \phi(\frac{x-\mu}{\sigma})$.
  \item \textbf{Moments}. 
  \vspace{-1.2em}
  \begin{gather*}
    \E X = \mu + \sigma \frac{\phi(\alpha) - \phi(\beta)}{C} \\
    \E X^2 = \sigma^2 + \mu^2 + \sigma^2  \frac{\alpha\phi(\alpha) - \beta\phi(\beta)}{C}   + 2\mu\sigma \frac{\phi(\alpha) - \phi(\beta)}{C} \\
    \Var X = \sigma^2 \left[ 1 +  \frac{\alpha\phi(\alpha) - \beta\phi(\beta)}{C} - \left(\frac{\phi(\alpha) - \phi(\beta)}{C}\right)^2 \right]
  \end{gather*}
  \item \textbf{Entropy}.
  \begin{align*}
    H(p) 
    &= \half\log 2\pi e\sigma^2 + \log C + \frac{\alpha\phi(\alpha) + \beta\phi(\beta)}{2C} \\
    &= \half\log 2\pi e\sigma^2 + \log C + \frac{1}{2\sigma^2}\cdot \greyoverbrace{\sigma^2\frac{\alpha\phi(\alpha) + \beta\phi(\beta)}{C}}{\Var X -\sigma^2 + (\E X - \mu)^2} \\
    &= \half\log 2\pi \sigma^2 + \log C + \frac{1}{2\sigma^2}\E [X - \mu]^2 
  \end{align*}
  because $\Var X + (\E X - \mu)^2 = \E X^2 - \cancel{(\E X)^2} + \cancel{(\E X)^2} + \mu^2 - 2\mu\E X.$
\end{itemize}

For binary probit models, the distributions that come up are one-sided truncations at zero, i.e. $\tN(\mu,\sigma^2,0,+\infty)$ (upper tail/positive part) and $\tN(\mu,\sigma^2,-\infty,0)$ (lower tail/negative part), for which their moments are of interest.
As an aside, if $\mu = 0$ then the truncation $\tN(0,\sigma^2,0,+\infty)$ is called the \emph{half-normal} distribution.
For the positive one-sided truncation at zero, $C = \Phi(+\infty) - \Phi(-\mu/\sigma) = 1 - \Phi(-\mu/\sigma) = \Phi(\mu/\sigma)$, and for the negative one-sided truncation at zero, $C = \Phi(-\mu/\sigma) - \Phi(-\infty) = 1 - \Phi(\mu/\sigma)$.

One may simulate random draws from a truncated normal distribution by drawing from $\N(\mu,\sigma^2)$ and discarding samples that fall outside $(a,b)$.
Alternatively, the inverse-transform method using
\[
  X = \mu + \sigma\Phi^{-1}\left( \Phi(\alpha) + UC \right)
\]
with $U\sim\Unif(0,1)$ will work too.
Either of these methods will work reasonably well as long as the truncation region is not too far away from $\mu$, but neither is particularly fast.
Efficient algorithms have been explored which are along the lines of either accept/reject algorithms \citep{robert1995simulation}, Gibbs sampling \citep{damien2001sampling}, or pseudo-random number generation algorithms \citep{chopin2011fast}.
The latter algorithm is inspired by the Ziggurat algorithm \citep{marsaglia2000ziggurat} which is considered to be the fastest Gaussian random number generator.


\subsection{Truncated multivariate normal distribution}

Consider the restriction of $X\sim \N_d(\mu,\Sigma)$ to a convex subset\footnote{A convex subset is a subset of a space that is closed under convex combinations. In Euclidean space, for every pair of points in a convex set, all the points that lie on the straight line segment which joins the pair of points are also in the set.}~$\cA \subset \bbR^d$.
Call this distribution the truncated multivariate normal distribution, and denote it $X \sim \tN_d(\mu,\Sigma,\cA)$.
The pdf is $p(X|\mu,\Sigma,\cA) = C^{-1}\phi(X|\mu,\Sigma)\ind[X\in\cA]$, where
\[
  C = \int_\cA \phi(x|\mu,\Sigma) \dint x = \Prob(X \in \cA).
\] 

Generally speaking, there are no closed-form expressions for $\E g(X)$ for any well-defined functions $g$ on $X$.
One strategy to obtain values such as $\E X$ (mean), $\E X^2$ (second moment) and $E \log p(X)$ (entropy) would be Monte Carlo integration.
If $X^{(1)},\dots,X^{(T)}$ are samples from $X\sim\tN_d(\mu,\Sigma,\cA)$, then $\widehat{\E g(X)} = \frac{1}{T} \sum_{i=1}^T g(X^{(i)})$.

Sampling from a truncated multivariate normal distribution is described by \citet{robert1995simulation} and \citet{damien2001sampling}.
In the latter, the authors explore a simple Gibbs-based approach that is easy to implement in practice.
Assume that the one-dimensional slices of $\cA$ 
\[
  \cA_k(X_{-j}) = \{X_j | (X_1,\dots,X_{j-1},X_j, X_{j+1},\dots,X_d) \in \cA \}
\]
are readily available so that the bounds or anti-truncation region of $X_j$ given the rest of the components $X_{-j}$ are known to be $(x_j^-, x_j^+)$.
Using properties of the normal distribution, the full conditionals of $X_j$ given $X_{-j}$ is
\begin{gather*}
  X_j \sim \tN(\tilde\mu_j,\tilde\sigma_j^2, x_j^-, x_j^+) \\
  \tilde\mu_j = \mu_{j} + \Sigma_{j,-j}^\top\Sigma_{-j,-j}(x_{-j} - \mu_{-j}) \\
  \tilde\sigma_j^2 = \Sigma_{11} - \Sigma_{j,-j}^\top \Sigma_{-j,-j} \Sigma_{j,-j}.
\end{gather*}
According to \citet{robert1995simulation}, if $\Psi = \Sigma^{-1}$, then 
\[
  \Sigma_{-j,-j}^{-1} = \Psi_{-j,-j} - \Psi_{j,-j}\Psi_{-j,-j}^\top / \Psi_{jj}
\]
which means that we need only compute one global inverse $\Sigma^{-1}$.
Introduce a latent variable $Y \in \bbR$ such that the joint pdf of $X$ and $Y$ is
\[
  p(X_1,\dots,X_d,Y) \propto \exp(-Y/2) \ind[y > (x-\mu)^\top\Sigma^{-1}(x-\mu)]\ind[X\in\cA].
\]
Now, the Gibbs conditional densities for the $X_k$'s are given by
\[
  p(X_j|X_{-j},Y) \propto \ind[X_j \in \cB_j]
\]
where
\[
  \cB_j \in (x_j^-, x_j^+) \cap \{X_j | (X-\mu)^\top\Sigma^{-1}(X-\mu) < Y \}.
\]
The Gibbs conditional density for $Y|X$ is a shifted exponential distribution, which can be sampled using the inverse-transform method.
Thus, both $X$ and $Y$ can be sampled directly from uniform variates.

For probit models, we are interested in the conical truncations $\cC_j = \{ X_j > X_k | k\neq j, \text{and } k=1,\dots,m  \}$ for which the $j$'th component of $X$ is largest.
These truncations form cones in $d$-dimensional space such that $\cC_1 \cup \cdots \cup \cC_d = \bbR^d$, and hence the name.



\subsection{Wishart distribution}

\subsection{Gamma distribution}













