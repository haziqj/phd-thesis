\section{Proof of Lemma \ref{lem:expectation-entropy-truncated-mvn}}

\begin{proof}
\begin{enumerate}[label=(\roman*)]
  \item Due to the independence structure in the pdf of $\bX$, it is easy to consider the expectations of each of the components separately and marginalising out the rest of the components. For $i \neq j$, we have
  \begin{align*}
    \E[x_i] 
    &= C^{-1} \idotsint \ind[x_k < x_j, \forall k \neq j] \cdot x_i  \prod_{k=1}^d \frac{1}{\sigma_k}\phi \left( \frac{x_k - \mu_k}{\sigma_k} \right) \d x_1 \cdots \d x_d \\
    &= C^{-1} \iint \ind[x_i < x_j] \frac{x_i}{\sigma_i} \, \phi \left( \frac{x_i - \mu_i}{\sigma_i} \right)  \prod_{k \neq i,j} \Phi \left( \frac{x_j - \mu_k}{\sigma_k} \right) \frac{1}{\sigma_j}\phi \left( \frac{x_j - \mu_j}{\sigma_j} \right) \d x_i \d x_j \\
    &= C^{-1} \iint \ind[\sigma_i z_i + \mu_i < \sigma_j z_j + \mu_j] (\sigma_i z_i + \mu_i) \phi (z_i)  \prod_{k \neq i,j} \Phi \left( \frac{\sigma_j z_j + \mu_j - \mu_k}{\sigma_k} \right) \phi (z_j) \d z_i \d z_j \\
    &= \mu_i C^{-1} \iint \ind[ z_i < (\sigma_j z_j + \mu_j - \mu_i) / \sigma_i] \phi (z_i)  \prod_{k \neq i,j} \Phi \left( \frac{\sigma_j z_j + \mu_j - \mu_k}{\sigma_k} \right) \phi (z_j) \d z_i \d z_j \\*    
    &\phantom{==} + \sigma_i C^{-1} \iint \ind[z_i < (\sigma_j z_j + \mu_j - \mu_i) / \sigma_i] z_i \phi (z_i)  \prod_{k \neq i,j} \Phi \left( \frac{\sigma_j z_j + \mu_j - \mu_k}{\sigma_k} \right) \phi (z_j) \d z_i \d z_j \\
    &= \mu_i C^{-1} 
    \overbrace{
    \int  \prod_{k \neq j} \Phi \left( \frac{\sigma_j z_j + \mu_j - \mu_k}{\sigma_k} \right) \phi (z_j) \d z_j
    }^{C} \\  
    &\phantom{==} + \sigma_i C^{-1} \int \ind[ z_i < (\sigma_j z_j + \mu_j - \mu_i) / \sigma_i] z_i \phi (z_i) \prod_{k \neq i,j} \Phi \left( \frac{\sigma_j z_j + \mu_j - \mu_k}{\sigma_k} \right) \phi (z_j) \d z_i \d z_j \\
  \end{align*}
  The integral involving $z_i$ in the second part of the sum is recognised as the (unnormalised) expectation of the lower-tail of a univariate standard normal distribution truncated at $\tau_{ij} = (\sigma_j z_j + \mu_j - \mu_i) / \sigma_i$. That is,
  \[
    \E[Z_i | Z_i < \tau_{ij}] 
    = \big[\Phi(\tau_{ij})\big]^{-1} \int \ind [z_i < \tau_{ij}] z_i \phi(z_i) \d z_i 
    = - \frac{\phi(\tau_{ij})}{\Phi(\tau_{ij})}
  \] Plugging this expression back into the derivation of this expectation, we get
  \begin{align*}
  \E[X_i] 
  &= \mu_i -  \sigma_i C^{-1} \int 
  \phi \left( \frac{\sigma_j z_j + \mu_j - \mu_i}{\sigma_i} \right)
  \prod_{k \neq i,j} \Phi \left( \frac{\sigma_j z_j + \mu_j - \mu_k}{\sigma_k} \right) \phi (z_j) \d z_j \\
  &= \mu_i - \sigma_i C^{-1} \E \left[ \phi \left( \frac{\sigma_j Z_j + \mu_j - \mu_i}{\sigma_i} \right)
  \prod_{k \neq i,j} \Phi \left( \frac{\sigma_j Z_j + \mu_j - \mu_k}{\sigma_k} \right) \right].
  \end{align*}
  
  The expectation for the $j$th component is
  \begin{align*}
    \E[X_j] 
    &= C^{-1} \idotsint \ind[x_k < x_j, \forall k \neq j] \cdot x_j  \prod_{k=1}^d \frac{1}{\sigma_k}\phi \left( \frac{x_k - \mu_k}{\sigma_k} \right) \d x_1 \cdots \d x_d \\
    &= C^{-1} \int x_j  \prod_{k \neq j} \Phi \left( \frac{x_j - \mu_k}{\sigma_k} \right) 
    \cdot \frac{1}{\sigma_j} \phi \left( \frac{x_j - \mu_j}{\sigma_j} \right) \d x_j  \\    
    &= C^{-1} \int (\sigma_j z_j + \mu_j)  \prod_{k \neq j} \Phi \left( \frac{\sigma_j z_j + \mu_j - \mu_k}{\sigma_k} \right) \cdot \phi (z_j) \d z_j  \\   
    &= \mu_j C^{-1} 
    \overbrace{
    \int  \prod_{k \neq j} \Phi \left( \frac{\sigma_j z_j + \mu_j - \mu_k}{\sigma_k} \right) \cdot \phi (z_j) \d z_j
    }^{C}  \\   
    &\phantom{==} + \sigma_j C^{-1} \int  \prod_{k \neq j} \Phi \left( \frac{\sigma_j z_j + \mu_j - \mu_k}{\sigma_k} \right) \cdot z_j \phi (z_j) \d z_j \\
    &= \mu_j + \sigma_j C^{-1} \E \left[ Z_j \prod_{k \neq j} \Phi \left( \frac{\sigma_j Z_j + \mu_j - \mu_k}{\sigma_k} \right) \right] \\
    &= \mu_j + \sigma_j  \mathop{\sum_{i=1}^d}_{i \neq j} \sigma_i C^{-1} \E \Bigg[ \phi \left( \frac{\sigma_j Z_j + \mu_j - \mu_i}{\sigma_i} \right) \mathop{\prod_{k=1}^d}_{k \neq i,j} \Phi \left( \frac{\sigma_j Z_j + \mu_j - \mu_k}{\sigma_k} \right) \Bigg] \\
    &= \mu_j - \sigma_j \sum_{i \neq j} \big(\E[X_i] - \mu_i \big)
  \end{align*}
  where we have made use of Lemma \ref{lem:EZgZ} in the second last step of the above.

  \item The differential entropy is given by
  \begin{align*}
    \cH(p) &= -\int p(\bx) \log p(\bx) \d \bx = -\E \left[ \log p(\bx) \right] \\
    &= -\E \left[-\log C - \half[d] \log 2\pi - \half \sum_{i=1}^d \log \sigma_i^2 - \half \sum_{i=1}^d \left( \frac{x_i - \mu_i}{\sigma_i} \right)^2 \right] \\
    &= \log C + \half[d] \log 2\pi + \half \sum_{i=1}^d \log \sigma_i^2 + \half \sum_{i=1}^d \frac{1}{\sigma_i^2} \E [ x_i - \mu_i ]^2.
  \end{align*}
\end{enumerate}  
\end{proof}

\begin{lemma}\label{lem:EZgZ}
  Let $Z \sim \N(0,1)$. Then for all $m \in \{\bbN \, | \, m > 1\}$ and $(\mu, \sigma) \in \bbR \times \bbR^+$, 
  \[
    \E \Bigg[ Z \mathop{\prod_{k=1}^m}_{k \neq j} \Phi(\sigma_k Z + \mu_k) \Bigg]
    = \mathop{\sum_{i=1}^m}_{i \neq j} \E \Bigg[ \sigma_i \phi(\sigma_i Z + \mu_i) \mathop{\prod_{k=1}^m}_{k \neq i,j} \Phi (\sigma_k Z + \mu_k) \Bigg]
  \]
  for some $j \in \{1, \dots, m\}$.
\end{lemma}

\begin{proof}
  Use the fact that for any differentiable function $g$, $\E[Zg(Z)] = \E[g'(Z)]$, and apply the result with the function $g_m:z \mapsto \prod_{k \neq j} \Phi(\sigma_k z + \mu_k)$. All that is left is to derive the derivative of $g$, and we use an inductive proof to do this. 
  
  We adopt the following notation for convenience:
  \begin{align*}
    \phi_i = \phi(\sigma_i z + \mu_i) \\
    \Phi_i = \Phi(\sigma_i z + \mu_i) 
  \end{align*}
  
  The simplest case is when $m=2$, which can be trivially shown to be true. Without loss of generality, let $j=1$. Then
  \begin{align*}
    g_2(z) &= \Phi_2 \\
    \Rightarrow g_2'(z) &= \sigma_2 \phi_2 = \mathop{\sum_{i=1}^2}_{i \neq 1} \Bigg[ \sigma_i \phi_i \mathop{\sum_{k=1}^2}_{k \neq 1,2} \Phi_k \Bigg].
  \end{align*}
  
  Now assume that the inductive hypothesis holds for some $m \in \{\bbN \, | \, m > 1\}$. That is, the derivative of
  \[
    g_m(z) = \mathop{\prod_{k=1}^m}_{k \neq j} \Phi_k
  \]
  which is
  \[
    g_m'(z) = \mathop{\sum_{i=1}^m}_{i \neq j} \bigg[  \sigma_i \phi_i \mathop{\prod_{k=1}^m}_{k \neq i,j} \Phi_k \bigg],
  \]
  is assumed to be true. Assume that without loss of generality, $j \neq m+1$. Then the derivative of
  \[
    g_{m+1}(z) = \mathop{\prod_{k=1}^{m+1}}_{k \neq j} \Phi_k = g_m(z) \Phi_{m+1}
  \]
  is found to be
  \begin{align*}
    g_{m+1}'(z) &= \sigma_{m+1} \phi_{m+1} g_m(z) + g_m'(z) \Phi_{m+1} \\
    &= \sigma_{m+1} \phi_{m+1} \mathop{\prod_{k=1}^m}_{k \neq j} \Phi_k + \mathop{\sum_{i=1}^m}_{i \neq j} \bigg[  \sigma_i \phi_i \mathop{\prod_{k=1}^m}_{k \neq i,j} \Phi_k \bigg] \Phi_{m+1} \\
    &= \sigma_{m+1} \phi_{m+1} \mathop{\prod_{k=1}^{m+1}}_{k \neq j, m+1} \Phi_k + \mathop{\sum_{i=1}^m}_{i \neq j} \bigg[  \sigma_i \phi_i \mathop{\prod_{k=1}^{m+1}}_{k \neq i,j} \Phi_k \bigg] \\
    &= \mathop{\sum_{i=1}^{m+1}}_{i \neq j} \bigg[  \sigma_i \phi_i \mathop{\prod_{k=1}^{m+1}}_{k \neq i,j} \Phi_k \bigg] \\
    &= g_{m+1}'(z).
  \end{align*}
  Thus, by induction and linearity of expectations, the proof is complete.
\end{proof}

%\begin{lemma}
%  Let $Z \sim \N(0,1)$. Then
%  \[
%    \E[Z\Phi(\mu + \sigma Z)] = \E[\sigma \phi(\mu + \sigma z)].
%  \]
%\end{lemma}
%
%\begin{proof}
%  Use the fact that for any differentiable function $g$, $\E[Zg(Z)] = \E[g'(Z)]$, and apply the function $g:z \mapsto \Phi(\mu + \sigma z)$ with derivative $g'(z) = \sigma \phi(\mu + \sigma z)$.
%\end{proof}

\section{Proof for ...}

\begin{lemma}\label{thm:normentropy}
  Let $p(x)$ be the pdf of a random variable $x$. Then if
  \begin{enumerate}[label=(\roman*)]
    \item $p$ is a univariate normal distribution with mean $\mu$ and variance $\sigma^2$,
    \[
      \cH(p) = \half (1 + \log 2\pi) + \half \log \sigma^2
    \]
    \item $p$ is a $d$-dimensional normal distribution with mean $\mu$ and variance $\Sigma$,
    \[
      \cH(p) = \half[d] (1 + \log 2\pi) + \half \log \vert \Sigma \vert 
    \]
    \item $p$ is distribution of the \textbf{upper-tail} of a univariate, one-sided normal distribution truncated at zero with mean $\mu$ and variance 1,
    \[
      \cH(p) = \half \log 2\pi + \half \left( \E [x^2] + \mu^2 - 2\mu\E [x] \right) + \log \Phi(\mu)
    \]
    \item $p$ is distribution of the \textbf{lower-tail} of a univariate, one-sided normal distribution truncated at zero with mean $\mu$ and variance 1,
    \[
      \cH(p) = \half \log 2\pi + \half \left( \E x^2 + \mu^2 - 2\mu\E x \right) + \log \big(1 - \Phi(\mu)\big)
    \]
  \end{enumerate}
\end{lemma}


\begin{proof}
\hspace{1cm}

Case (i): $-\log p(x) = \half\log 2\pi + \half\log \sigma^2 + \half(x - \mu)^2$. Then 

\begin{align*}
  \cH(p) &= \E_x\left[\half\log 2\pi + \half\log \sigma^2 + \frac{1}{2\sigma^2}(x - \mu)^2\right] \\
  &= \half\log 2\pi + \half\log \sigma^2 + \frac{1}{2\sigma^2} \cancelto{\sigma^2}{\E (x - \mu)^2} \\
  &= \half (1 + \log 2\pi) + \half \log \sigma^2
\end{align*}

Case (ii): $-\log p(x) = \half[d]\log 2\pi + \half\log \vert \Sigma \vert+ \half(x - \mu)^\top\Sigma^{-1}(x - \mu)$. Then

\begin{align*}
  \cH(p) &= \half[d]\log 2\pi + \half\log \vert \Sigma \vert + \half \E_x \left[ (x - \mu)^\top\Sigma^{-1}(x - \mu) \right] \\
  &= \half[d]\log 2\pi + \half\log \vert \Sigma \vert + \half \tr \Big(\Sigma^{-1} \cancelto{\Sigma}{\E_x \left[ (x - \mu)(x - \mu)^\top \right]} \Big) \\
  &= \half[d] (1 + \log 2\pi) + \half\log \vert \Sigma \vert 
\end{align*}

For the next two cases, we state the following properties of a truncated normal distribution without proof.
\begin{lemma}\label{thm:entropytruncatednormal}
    Let $x \sim \N(\mu,\sigma^2)$ with $x$ lying in the interval $(a,b)$. Then we say that $x$ follows a truncated normal distribution, and
    \begin{enumerate}[label=(\roman*)]
      \item the mean of $x$ (conditional on $a<x<b$) is
      \[
        \E[x] = \mu + \sigma\frac{\phi(\alpha) - \phi(\beta)}{Z},
      \]
      \item the variance of $x$ (conditional on $a<x<b$) is
      \[
        \Var[x] = \sigma^2\left[1 + \frac{\alpha\phi(\alpha) - \beta\phi(\beta)}{Z} - \left( \frac{\phi(\alpha) - \phi(\beta)}{Z}\right)^2 \right], \text{and}
      \]
      \item the entropy of the pdf of $x$ (conditional on $a<x<b$) is
      \[
        \cH = \half \log 2\pi + \half \log \sigma^2 + \log Z  + \half + 
        \frac{\alpha\phi(\alpha) - \beta\phi(\beta)}{2Z},
      \]
    \end{enumerate}
    where $\alpha = (a - \mu)/\sigma$, $\beta = (b - \mu)/\sigma$, and $Z = \Phi(\beta) - \Phi(\alpha)$, and $\phi$ and $\Phi$ are the pdf and cdf of a standard normal distribution respectively.
\end{lemma}

In the special case when $\sigma = 1$ (the case we are interested in), then with some manipulation, one arrives at the following expression for the entropy of the pdf $p$ of a truncated normal distribution:
\begin{align*}
  \cH(p) &= \half \log 2\pi + \cancel{\half \log \sigma^2} + \log Z  + \half \left( 1 + \frac{\alpha\phi(\alpha) - \beta\phi(\beta)}{Z} \right) \\
  &=\half \log 2\pi + \log Z  + \half \left( \Var[x] + \left( \frac{\phi(\alpha) - \phi(\beta)}{Z}\right)^2 \right) \\
  &=\half \log 2\pi + \log Z  + \half \left( \E[x^2] - \E^2[x] + \left( \E[x] - \mu \right)^2 \right) \\  
  &= \half \log 2\pi + \log Z  + \half \left( \E[x^2] + \mu^2 - 2\mu\E[x] \right)
\end{align*}

We now continue with the proof.

Case (iii): Using Lemma \ref{thm:entropytruncatednormal} with $a = 0$, $b = +\infty$, and $\sigma = 1$, we get that $Z = 1 - \Phi(-\mu) = \Phi(\mu)$. Therefore, the entropy of $p$ is given by
\begin{align*}
  \cH(p) = \half \log 2\pi + \half \left( \E[x^2] + \mu^2 - 2\mu\E[x] \right) + \log \Phi(\mu) 
\end{align*}

Case (iv): Again, using Lemma \ref{thm:entropytruncatednormal} with $a = -\infty$, $b = 0$, and $\sigma = 1$, we get that $Z = \Phi(-\mu) = 1 -\Phi(\mu)$. Therefore, the entropy of $p$ is given by
\begin{align*}
  \cH(p) = \half \log 2\pi + \half \left( \E[x^2] + \mu^2 - 2\mu\E[x] \right) +  \log \big(1 - \Phi(\mu)\big)
\end{align*}

\end{proof}

\section{Distribution of $\tilde q(\by^*)$ for binary case}

\underline{Case: $y_i = 1$}

\begin{align*}
  \log \tilde q(y_i^*) 
  &=  \ind[y_i^* \geq 0] \cdot \E_{\bw, \alpha, \lambda} \left[ -\half (y_i^* - \alpha - \lambda \bH_i \bw)^2  \right] + \const \\
  &= \ind[y_i^* \geq 0] \cdot \left[ -\half \Big( y_i^{*2} - 2\E_{\bw, \alpha, \lambda} [\alpha + \lambda \bH_i \bw] y_i \Big)  \right] + \const \\
  &= \ind[y_i^* \geq 0]  \left[ -\half (y_i^* - \tilde\eta_i)^2  \right] + \const \\
  &\equiv 
  \begin{cases}
    \N(\tilde\eta_i, 1) & \text{if } y_i^* \geq 0 \\
    0             & \text{if } y_i^* < 0
  \end{cases}
\end{align*}
where 
\[
  \tilde\eta_i = \E\alpha + \E\lambda \bH_i \E\bw
\]
by independence of $q(\bw)$, $q(\alpha)$ and $q(\lambda)$. $\tilde q(y_i^*)$ is recognised as being the upper-tail of a one-sided normal distribution truncated at zero. The mean is 
\[
  \E[y_i^* | y_i^* \geq 0] = \tilde\eta_i + \frac{\phi(\tilde\eta_i)}{\Phi(\tilde\eta_i)}
\]
where $\phi$ and $\Phi$ are, respectively, the pdf and cdf of a standard normal distribution.

\underline{Case: $y_i = 0$}

Following the same argument, we can deduce that $q(y_i^*)$ in this case would be the lower-tail of a one-sided normal distribution truncated at zero. The mean is
\[
  \E[y_i^* | y_i^* < 0] = \tilde\eta_i + \frac{\phi(\tilde\eta_i)}{\Phi(\tilde\eta_i) - 1}.
\]
