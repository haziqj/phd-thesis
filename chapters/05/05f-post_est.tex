Working within a variational Bayesian framework means that we are able to perform inferences on any quantity of interest using the (approximate) posterior distributions obtained.
%First and foremost, posterior standard deviations explain the uncertainty surrounding parameters of the I-probit model are easy to come by in comparison to pure likelihood based methods.
Any of the post estimation procedures explained in the previous chapter when dealing with normal I-prior models can be extended here.

Prediction of a new data point $x_{\text{new}}$ is described.
Step one is to determine the distribution of the posterior regression functions in each class, $\bff(x_{\text{new}}) = \bw^\top\bh_\eta(x_{\text{new}})$, given values for the parameters $\theta$ of the I-probit model.
To this end, we use the posterior mean estimate for $\theta$, and denote them with tildes, as we have done so far in this chapter.
As we know, $\vecc\bw$ is normally distributed with mean and variance according to \cref{eq:varipostw}.
By writing $\vecc \tilde\bw = (\tilde \bw_{\bigcdot 1}, \dots, \tilde \bw_{\bigcdot m})^\top$ to separate out the I-prior random effects per class, we have that $\bw_{\bigcdot j} \sim \N_n(\tilde \bw_{\bigcdot 1}, \tilde\bV_w[j,j])$, and $\Cov(\bw_{\bigcdot j},\bw_{\bigcdot k}) = \tilde\bV_w[j,k]$, where the `$[\cdot,\cdot]$' indexes the $n\times n$ sub-block of the block matrix structured matrix $\bV_w$.
Thus, for each class $j=1,\dots,m$ and any $x \in \cX$,
\[
  f_j(x)|\by,\tilde\theta \sim \N\big(\,
  \tilde\bh_\eta(x)^\top\bw_{\bigcdot j}, \,
  \tilde\bh_\eta(x)^\top\tilde\bV_w[j,j]\tilde\bh_\eta(x)
  \big),
\]
and the covariance between the regression functions in two different classes is
\[
  \Cov\big[f_j(x),f_k(x)|\by,\tilde\theta \big] = 
  \tilde\bh_\eta(x)^\top \tilde\bV_w[j,k] \tilde\bh_\eta(x).
\]
Then, in step two, using the results obtained in the previous chapter in \hltodo{Section 4.4}, we have that the latent propensities $y_{\new,j}^*$ for each class are normally distributed with mean, variance, and covariances
\begin{alignat*}{3}
  \E[y_{\new,j}^*|\by,\tilde\theta]  
  &= \tilde\alpha_j + \E \big[ f_j(x_\new) |\by,\tilde\theta \big] 
  &&=: \hat\mu_j(x_\text{new}) \\
  \Var[y_{\new,j}^*|\by,\tilde\theta] 
  &= \Var\big[f(x_\new)|\by,\tilde\theta \big] + \bPsi^{-1}_{jj}
  &&=: \hat\sigma_j^2(x_\text{new}) \\
  \Cov[y_{\new,j}^*,y_{\new,k}^*|\by,\tilde\theta] 
  &= \Cov\big[f_j(x),f_k(x)|\by,\tilde\theta \big] + \bPsi^{-1}_{jk}
  &&=: \hat\sigma_{jk}(x_\text{new}).
\end{alignat*}

From here, step three would be to extract class information of data point $x_\text{new}$, which are contained in the normal distribution $\N_{m}\big(\hat\bmu_\new, \hat\bV_\new \big)$, where
\begin{equation*}
  \hat\bmu_\new = \big(\mu_1(x_\text{new}),\dots, \mu_m(x_\text{new}) \big)^\top 
  \hspace{0.5cm}\text{and}\hspace{0.5cm}
  \hat\bV_{\new,jk} = 
  \begin{cases}
    \hat\sigma_j^2(x_\text{new}) & \text{if } i = j \\
    \hat\sigma_{jk}(x_\text{new}) & \text{if } i \neq j. \\
  \end{cases}
\end{equation*}
%, subject to the truncation $\cA_j := \{ y_{\text{new},j}^* > y_{\text{new},k}^* | \forall k \neq j\}$ if the new observation belongs to class $j \in \{1,\dots, m\}$.
The predicted class is inferred from the latent variables via
\[
  \hat y_{\text{new}} = \argmax_k \hat\mu_k(x_\new), 
\]
while the probabilities for each class are obtained via integration of a multivariate normal density, as per \cref{eq:pij}, and restated here for convenience:
\begin{align*}
  \hat p_{\text{new},j} 
  &=  \idotsint\displaylimits_{\intset} \phi(y_{i1}^*, \dots, y_{im}^*|\hat\bmu_\new, \hat\bV_{\new}) \dint y_{i1}^* \cdots \dint y_{im}^*.
\end{align*}
For the independent I-probit model, class probabilities are obtained in a more compact manner via
\[
  \hat p_{\text{new},j} 
  = \E_Z \Bigg[ \mathop{\prod_{k=1}^m}_{k\neq j} 
  \Phi \left(\frac{\hat\sigma_j(x_\text{new})}{\hat\sigma_k(x_\text{new})} Z + \frac{\hat\mu_j(x_\new) - \hat\mu_k(x_\new)}{\hat\sigma_k^2(x_\text{new})} \right) \Bigg],
\]
as per \cref{eq:pij2}, since the $m$ components of $\bff(x_\new)$, and hence the $\by^*_{\new,j}$'s, are independent of each other ($\bPsi$ and $\hat\bV_\new$ are diagonal).










In the I-probit model, we use the approximate mean-field densities in lieu of the true posterior densities, and these are, for the most part, easy to sample from.
Take for example the class probabilities. 
We can obtain posterior samples for the $p_{ij}$'s by firstly sampling from the underlying normal latent variables, and then passing it through the probit link function.
Taking the empirical lower 2.5th and upper 97.5th percentile of this sample would give the upper and lower values for a 95\% credibility interval.

Model comparison...

