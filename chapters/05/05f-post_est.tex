For a new data point $x_{\text{new}}$, we calculate the predicted latent values $\tilde f_{\text{new}} = (\tilde f_{\text{new},1}, \dots, \tilde f_{\text{new},m})$ for each of the classes, using the variational estimates of the posterior means for the unknown quantities (denoted with tildes), as follows:
\[
  \tilde f_{\text{new},j} = \sum_{k=1}^n \tilde h_{\eta_j}(x_{\text{new}},x_k) \tilde w_{kj}, \hspace{0.5cm} j = 1,\dots,m.
\]
This is in fact the posterior mean for the regression functions evaluated at the new point $x_{\text{new}}$, which stems from a posterior normal distribution.
Denote $\bg(x_{\text{new}}) = \diag\big(\bh_{\eta_j}^\top(x_{\text{new}})\big)_{j=1}^m$, an $m \times nm$ matrix containing the kernel entries relating to this new data point.
From Gaussian process regression theory, we know that the prediction at $x_{\text{new}}$ for the latent variable $\by^*(x_{\text{new}})$ has $m$ components equal to $\tilde y_{\text{new},j}^* = \tilde\alpha_j + \tilde f_{\text{new},j}$ for $j=1,\dots,m$, and variance 
\[
  \bV_y(x_{\text{new}}) = \bg^\top(x_{\text{new}}) \tilde\bV_w \bg(x_{\text{new}}) + \tilde\bSigma.
\]
Thus, information relating to the class and its corresponding probabilities are contained within the normal distribution $\N_{m}\big(\by^*(x_{\text{new}}), \bV_y(x_{\text{new}}) \big)$, subject to the truncation $\cA_j := \{ y_{\text{new},j}^* > y_{\text{new},k}^* | \forall k \neq j\}$ if the new observation belongs to class $j \in \{1,\dots, m\}$.
The predicted class is inferred from the latent variables via
\[
  \hat y_{\text{new}} = \argmax_j \tilde y_{\text{new},j}^*, 
\]
while the probabilities for each class are once again obtained using the integral stated in \eqref{eq:pij}, stated here for convenience:
\begin{align*}
  \tilde p_{\text{new},j} 
  &= \int_{\cA_j} \N(\by^*(x_{\text{new}}), \bV_y(x_{\text{new}})) \d\by^*_{\text{new}}.
\end{align*}

For the independent I-probit model, each component of the latent variables $\by^*(x_{\text{new}})$ are calculated in a similar manner, with the difference being that the components would be independent of each other.
This is expressed in the form for the predictive covariance matrix $\bV_y(x_{\text{new}}) = \diag \big( v_1^2(x_{\text{new}}),\dots, v_m^2(x_{\text{new}})\big)$, where each variance component is given by
\[
  v_j^2(x_{\text{new}}) = \bh_{\eta_j}^\top(x_{\text{new}}) \tilde\bV_w \bh_{\eta_j}(x_{\text{new}}) + \tilde\sigma_{j}^2.
\]
Class prediction is the same as before, but class probabilities are obtained in a more compact manner via
\[
  \tilde p_{\text{new},j} 
  = \E_Z \Bigg[ \mathop{\prod_{k=1}^m}_{k\neq j} 
  \Phi \left(\frac{v_j}{v_k} Z + \frac{\tilde y_{\text{new},j}^* - \tilde y_{\text{new},k}^*}{v_k} \right) \Bigg],
\]
where $Z\sim\N(0,1)$ and $\Phi(\cdot)$ its CDF.

Working in a Bayesian framework means that we are able to perform inferences on any quantity of interest through posterior sampling.
In the I-probit model, we use the approximate mean-field densities in lieu of the true posterior densities, and these are, for the most part, easy to sample from.
Take for example the class probabilities. 
We can obtain posterior samples for the $p_{ij}$'s by firstly sampling from the underlying normal latent variables, and then passing it through the probit link function.
Taking the empirical lower 2.5th and upper 97.5th percentile of this sample would give the upper and lower values for a 95\% credibility interval.