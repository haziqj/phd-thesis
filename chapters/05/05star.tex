Observe data $\{(y_1,x_1),\dots,(y_n,x_n) \}$ where each $x_i \in \cX$.
Let $y_i \in \{ 1,\dots,m \}$ and write $y_i = (y_{i1},\dots,y_{im})$ where $y_{ij} = 1$ if $y_i = j$ and 0 otherwise.
For $j=1,\dots,m$, attempt to model
\begin{gather*}
  y_{ij} = \alpha_j + f_j(x_i) + \epsilon_{ij}  \\
  (\epsilon_{i1},\dots,\epsilon_{im})^\top \iid \N_m(\bzero,\bPsi^{-1})
\end{gather*}

Define $f_j(x) = f(x,j)$ such that each $f_j$ belong to some RKHS $\cF$ (and not separate RKHSs $\cF_j$).
The reproducing kernel of $\cF$ is $h:(\cX \times \{1,\dots,m\})^2 \to \bbR$ as defined by
\[
  h\big( (x,j), (x',j') \big) = a(j,j')h_\eta(x,x').
\]
Choices for $a:\{1,\dots,m\} \times \{1,\dots,m\} \to \bbR$ include 
\begin{enumerate}
  \item The Pearson kernel
  \[
    a(j,j') = \frac{\delta_{jj'}}{\Prob(X=j)} - 1
  \]
  \item The Identity kernel
  \[
    a(j,j') = \delta_{jj'}
  \]
\end{enumerate}
The kernel $h_\eta$ may be any of the usual kernels, i.e. canonical, fBm, Pearson, SE, polynomial, etc. and this kernel depends on the hyperparameters $\eta$.
Denote $\bH$ as the $n\times n$ matrix with $(r,s)$ entries equal to $h_\eta(x_r,x_s)$ for $r,s\in\{1,\dots,\}$.
Similarly, denote $\bA$ as the $m \times m$ matrix with $(k,j)$ entries equal to $a(k,j)$. 
Note that for the identity kernel, $\bA = \bI_m$.

The regression model in vector form:
\begin{gather*}
  \overbrace{
  \begin{pmatrix}
    y_{i1} \\ \vdots \\ y_{im}
  \end{pmatrix} 
  }^{\by_i}
  = 
  \overbrace{  
  \begin{pmatrix}
    \alpha_{1} \\ \vdots \\ \alpha_{m}
  \end{pmatrix} 
  }^\balpha
  +
  \overbrace{    
  \begin{pmatrix}
    f_1(x_i) \\ \vdots \\ f_m(x_i)
  \end{pmatrix} 
  }^{\bff(x_i)}
  +
  \overbrace{      
  \begin{pmatrix}
    \epsilon_{i1} \\ \vdots \\ \epsilon_{im}
  \end{pmatrix} 
  }^{\bepsilon_i}
  \\
  \bepsilon_i \iid \N_m(\bzero,\bPsi^{-1}).
\end{gather*}

An I-prior on the regression function $f:\cX \times \{1,\dots,m\} \to \bbR$ takes the form
\[
  f_j(x) = f(x,j) = \sum_{k=1}^m\sum_{i=1}^n a(j,k)h_\eta(x,x_i) w_{ij}
\]
where $(w_{i1},\dots,w_{im})^\top \iid \N_m(\bzero,\bPsi)$.


Rearrange the $n$ observations per class.
Let $\bff_j = \big(f_j(x_1),\dots,f_j(x_n)\big)^\top \in \bbR^n$.
We can write the I-prior as $\bff_j = \bA_{jj} \cdot \bH\bw_j$
Therefore, $\bff_j \sim \N_n(\bzero, \bPsi_{jj}\bA_{jj} \bH^2)$, and
\begin{align*}
  \Cov(\bff_j,\bff_k) &= \Cov(\bA_{jj} \cdot \bH\bw_j, \bA_{kk} \cdot \bH\bw_k) \\
  &= \bA_{jj}\bA_{kk} \cdot \bH \Cov(\bw_j,\bw_k) \bH \\
  &= \bA_{jj}\bA_{kk}\bPsi_{jk} \bH^2.
\end{align*}

Write $\bff = (\bff_1^\top, \dots, \bff_m^\top)^\top \bbR^{nm}$.
Then this is multivariate normal with mean $\bzero$ and covariance matrix equal to
\begin{align*}
  \Var \bff &= (\bA \otimes \bH) (\bPsi \otimes \bI_n) (\bA \otimes \bH) \\
  &= (\bA\bPsi\bA \otimes \bH^2) \\
  &= \big(\bOmega_{jk}\bH^2\big)_{j,k=1}^n
\end{align*}
where $\bOmega_{jk} = (\bA\bPsi\bA)_{jk}$. 
Out of interest, this can be expressed as a matrix normal distribution.
Write $\bw$ as the $n \times m$ matrix with entries equal to $w_{ij}$.
Then $\bw \sim \MN_{n,m}(\bzero,\bI_n,\bPsi)$ and $\bff = \bH\bw\bA \sim \MN_{n,m}(\bzero,\bH^2,\bA\bPsi\bA)$.

\subsection[Special case: Psi = I with identity kernel]{Special case: $\bPsi = \bI_m$ with identity kernel}

In this case, the matrix normal distribution for $\bff$ is $\MN_{n,m}(\bzero,\bH^2,\bI_m)$. 
That is to say, the \emph{columns} of $\bff$, i.e. $\bff_j$, are iid observations $\bff_j \iid \N_n(\bzero,\bH^2)$.
































