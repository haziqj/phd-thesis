Consider multinomial response variables $y_1, \dots, y_n$, where each response $y_i$ takes on one of the values $\{1,\dots,m\}$ from a set of $m$ possible values. We model each response as following a categorical distribution (a special case of the multinomial distribution)
\[
  y_i \sim \Mult(p_{i1}, \dots, p_{im}),
\]
with probability mass function (pmf)
\[
  p(y_i) = p_{i1}^{\ind[y_i = 1]} \cdots p_{im}^{\ind[y_i = m]},
\]
such that $p_{ij} \geq 0 $ for each $j$ and $\sum_{j=1}^m p_{ij} = 1$. It might also be convenient to think of the responses $y_i$ as comprising of a binary vector of length $m$, with a single `1' at the position corresponding to the value that $y_i$ takes. That is,
\[
  y_i = (y_{i1}, \dots, y_{im})
\]
with
\[
  y_{ik} =
  \begin{cases}
    1 &\text{ if } y_i = k \\
    0 &\text{ if } y_i \neq k
  \end{cases}
\]
and $\sum_{j=1}^m y_{ij} = 1$. In this formulation, each $y_{ij}$ is distributed as Bernoulli with probability $p_{ij}$.

Suppose for each observation $y_i$ there is an associated $p$-dimensional vector of covariates $x_i = (x_{i1}, \dots, x_{ip})$ belonging to some set $\cX$. We would like to model the multinomial outcomes $y_i$ based on these vectors of covariates. Assume that for each $y_i = (y_{i1}, \dots, y_{im})$, there exists a corresponding continuous, underlying, latent variable $y_i^* = (y_{i1}^*, \dots, y_{im}^*)$ such that
\[
  y_i =
  \begin{cases}
    1 &\text{ if } y_{i1}^* \geq y_{i2}^*, y_{i3}^*, \dots, y_{im}^* \\
    2 &\text{ if } y_{i2}^* \geq y_{i1}^*, y_{i3}^*, \dots, y_{im}^* \\
    \vdots \\
    m &\text{ if } y_{im}^* \geq y_{i2}^*, y_{i3}^*, \dots, y_{i\,m-1}^*. \\
  \end{cases}
\]
In other words, 
\[
  y_{ij} = \ind[y_{ij}^* = \max_k y_{ik}^*].
\]

We consider modelling the $n$ latent variables in each class $j=1,\dots,m$ according to the regression problem
\begin{equation}\label{eq:multinomial-latent}
  \begin{gathered}
    y_{ij}^* = f_j(x_i) + \epsilon_{ij} \\
    \epsilon_{ij} \iid \N(0, \sigma_j^2) \\
    i = 1,\dots,n
  \end{gathered}
\end{equation}
with $f_j:\cX \to \bbR$ being a regression function belonging to some reproducing kernel Hilbert space of functions $\cF_j$ having the reproducing kernel $h_{\lambda_j}: \cX \times \cX \to \bbR$. Here, $\lambda_j$ is the scale parameter for the reproducing kernel, so that $h_{\lambda_j}(\cdot) = \lambda_j h(\cdot)$. One advantage of the multinomial probit model is the ability to also model the correlations across choices, such that for each $j,k \in \{1,\dots,m\}$ and $j \neq k$,
\[
  \Corr[\epsilon_{ij}, \epsilon_{ik}] = \frac{\sigma_{jk}}{\sigma_j \, \sigma_k}.
\]
This setting is suitable for modelling multinomial data where the independence axiom is not desired. Such cases arise frequently in economics and social science. The famous Red-Bus-Blue-Bus example is often used to illustrate independence of irrelevant alternatives (IIA). Suppose commuters face the decision between taking cars and red busses. The addition of blue busses to commuters' choice should not affect the choice between cars or busses (assuming commuters are indifferent about the colour of the bus). Yet, if IIA is imposed, then the three choices are distinct, and the fact that red and blue busses are substitutable is ignored.

The IIA assumption is realised by fixing $\sigma_{jk} = 0$, $j \neq k$, which is clearly a simplification of the model (and as we will see later, benefits us in the algebra when deriving some distributional results). As this may not be suitable for certain modelling purposes, we might want to consider how this assumption can be relaxed, but we leave this for future work. 

Back to model \eqref{eq:multinomial-latent}: We wish to model the function $f_j$ as having an I-prior. Denoting $f_{ij} = f_j(x_i)$ as the evaluation of the function $f_j(\cdot)$ at $x_i$, and also $\bff_j = (f_{1j}, \dots, f_{nj})^\top$ as the vector containing all $n$ evaluations pertaining to the $j$th alternative, an I-prior on $f_j$ is 
\[
  \bff_j \sim \N(\bff_j^0, \cI_j)
\]
where $\cI_j$ is the $n \times n$ Fisher covariance kernel for the regression function $f_j$ in model \eqref{eq:multinomial-latent}, which has $(r,s)$ entries given by
\[
  \cI_j\big( f_j(x_r), f_j(x_s) \big) = \sigma_j^{-2} \sum_{k=1}^n \sum_{l=1}^n  h_{\lambda_j}(x_r, x_k) h_{\lambda_j}(x_l, x_s)
\]
and $\bff_j^0$ is a vector of prior means. By concatenating the vectors $\bff_1, \dots, \bff_m$ into the vector $\bff$ of length $nm$, it is easy to see that 
\[
  \bff \sim \N(\bff^0, \bOmega)
\]
where $\bff^0 = (\bff_1^0, \dots, \bff_m^0)^\top$ and $\bOmega = \diag(\cI_1, \dots, \cI_m)$. This stems from the fact that the error components are independent across choices (IIA), and as such, $\Cov[\bff_j, \bff_k] = \bzero$, $j \neq k$. In the more general case where $\sigma_{jk} \neq 0$, then extra care must be taken to ensure the covariances are represented in the I-prior covariance matrix.

We make two further simplifications to the model. Firstly, the choice model as stated is not identified in scale, i.e. multiplication of the latent variables by a positive constant does not make any difference to the outcome. This is a well known identification issue\footnotemark with the multinomial probit model, and this is typically overcome by setting some restrictions. In our case, we set all $\sigma_j^2 = 1$.\hltodo[But isn't identifiability resolved by setting all covariances to zero?]{}

\footnotetext{In the unrestricted case for a model with $m$ alternatives, there would be $m(m+1)/2$ variance components to estimate. However, in general, only $m(m-1)/2$ can be freely estimated.}

Secondly, we assume that the intercept functions (prior means) are constants, so that $\bff^0_j = \alpha_j\bone_n$ for $j=1,\dots,m$, and the $\alpha_j$s are just additional hyperparameters to be estimated. With all of these in mind, the I-prior model simplifies to
\begin{gather*}
  y_{ij}^* = \alpha_j + \sum_{k=1}^n h_{\lambda_j}(x_i, x_{k})w_{kj} + \epsilon_{ij} \\
  \epsilon_{ij} \iid \N(0,1) \\
  w_{kj} \iid \N(0,1) 
\end{gather*}
for each $j \in \{1,\dots,m\}$. Define $f_{ij} = \alpha_j + \sum_{k=1}^n h_{\lambda_j}(x_i, x_{k})w_k$, so that each $y_{ij}^*|f_{ij} \sim \N(f_{ij}, 1)$. The probit link is seen as follows:
\begin{align}
  p_{ij} 
%  &= \Prob[y_i = j] \\
%  &= \Prob[y_{ij} = 1] \\
  &= \Prob\left[y_{ij}^* = \max_k y_{ik}^* \right] \nonumber \\
  &= \Prob\left[y_{ij}^* > y_{ik}^* : \forall k \neq j \right] \nonumber \\
  &= \Prob\left[ 
  f_{ij} + \epsilon_{ij} > f_{ik} + \epsilon_{ik} : \forall k \neq j
   \right] \nonumber \\
  &= \Prob\left[\epsilon_{ik} - \epsilon_{ij} \leq f_{ij} - f_{ik} : \forall k \neq j \right] \nonumber \\
  &= \idotsint \ind\left[\epsilon_{ik} \leq \epsilon_{ij} + f_{ij} - f_{ik} : \forall k \neq j \right]
  \prod_{k=1}^m \big[ \phi(\epsilon_{ik}) \d\epsilon_{ik} \big] \nonumber \\
  &= \int \mathop{\prod_{k=1}^m}_{k\neq j} \Phi(\epsilon_{ij} + f_{ij} - f_{ik})
    \phi(\epsilon_{ij}) \d\epsilon_{ij} \label{eqn:probit-link} \\
  &= \E_Z \Bigg[ \mathop{\prod_{k=1}^m}_{k\neq j} \Phi(Z + f_{ij} - f_{ik}) \Bigg] \nonumber
%  &= \mathop{\prod_{k=1}^m}_{k\neq j} \Phi \left( e_k \right) 
\end{align}
where $Z$ is a standard normal random variable, and $\phi(\cdot)$ and $\Phi(\cdot)$ are the pdf and cdf of a standard normal distribution respectively. \hltodo[Citation?]{It is well know that for $m>3$ this has no closed form expression, which makes the probit model unattractive from a likelihood maximisation standpoint.}

The above describes $m$ regression functions being estimated for each class, and each of the $m$ regression functions estimated using an I-prior. It is possible for all $m$ regression functions\footnotemark to share a common I-prior $\bff_1, \dots, \bff_m \iid \N(\alpha\bone_n, \cI)$, so that only one set of intercept and RKHS scale parameters need to be estimated (instead of $m$ sets, one for each class). There is also flexibility in using the same covariance kernel for instance, but different intercepts for the $m$ I-priors, or vice-versa. The probit I-prior model can be represented by the following DAG.

\footnotetext{It is also possible to reparameterise the model (anchoring on one latent variable as the reference class and working with the latent differences) so that only $m-1$ I-priors are required.}

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[scale=1.1, transform shape]
    \tikzstyle{main}=[circle, minimum size=10mm, thick, draw=black!80, node distance=16mm]
    \tikzstyle{connect}=[-latex, thick]
    \tikzstyle{box}=[rectangle, draw=black!100]
      \node[main, fill=black!10] (H) [] {$x_i$};
      \node[main, double, double distance=0.6mm] (f) [right=of H, xshift=1.2cm] {$f_{ij}$};
      \node[main, double, double distance=0.6mm] (ystar) [right=of f, xshift=1cm] {$p_{ij}$};
      \node[main] (lambda) [above=of f, xshift=-1.5cm, yshift=-0.3cm] {$\lambda_j$};        
      \node[main] (alpha) [above=of f, xshift=1.5cm, yshift=-0.3cm] {$\alpha_j$};  
      \node[main, fill = black!10] (y) [right=of ystar, xshift=0.2cm] {$y_{i}$};
      \node[main] (w) [below=of f] {$w_{ij}$};  
      \path (alpha) edge [connect] (f)
            (lambda) edge [connect] (f)
            (H) edge [connect] node [above] {$h$} (f)    		
    		(f) edge [connect] node [above] {probit} (ystar)
    		(ystar) edge [connect] (y)
    		(w) edge [connect] (f);
      \node[rectangle, draw=black!100, fit={(H) ($(y.north east) + (0,0.4cm)$) (w)}] {}; 
      \node[rectangle, fit= (w) (y), label=below right:{$i=1,\dots,n$}, xshift=0.15cm, yshift=0.5cm] {};  % the label
      \node[rectangle, draw=black!100, fit={(lambda) ($(ystar.north east) + (0.5cm,0.7cm)$) ($(w.south west) + (-0.5,-0.7cm)$)}] {}; 
      \node[rectangle, fit={(f) ($(ystar.north east) + (0.5cm,0.7cm)$) ($(w.south west) + (-0.5,-0.7cm)$)}, label=below right:{$j=1,\dots,m$}, xshift=-1.75cm, yshift=0.4cm] {}; 
    \end{tikzpicture}
    \caption{A DAG of the probit I-prior  model. Observed nodes are shaded, while double-lined nodes represented known or calculable quantities. The latent variables $y_{ij}^*$ have been marginalised and absorbed into the probit. The $w_{ij}$s are the standard normal random-effects associated with the I-prior. There are at most $m$ sets of intercept ($\alpha_j$) and scale ($\lambda_j$) parameters to estimate.}
\end{figure}

\hltodo{this lemma not necessarily needed}

\begin{lemma}\label{lem:EPhiZ} 
  Let $Z \sim \N(0,1)$. Then 
  \[
    \E[\Phi(z)] = 1/2.
  \]
\end{lemma}

\begin{proof}
  Define the random variable $Y$ by $Y = \Phi(Z)$. Note that $Y \in [0,1]$, and is monotonically increasing within this interval. Then,
  \begin{align*}
    F(y) &= \Prob[Y \leq y] \\
    &= \Prob[\Phi(Z) \leq y] \\
    &= \Prob[Z \leq \Phi^{-1}(y)] \\
    &= \Phi\big( \Phi^{-1}(y) \big) = y
  \end{align*}
  which implies that $Y$ is uniform in the interval $[0,1]$. Thus,
  \[
    \E[\Phi(Z)] = 1/2.
  \]
\end{proof}

% This proof is wrong
%\begin{lemma}\label{lem:expectation-of-prod-phi}
%  Let $Z \sim \N(0,1)$. Then, for $\sigma > 0$,
%  \[
%    \E_Z \left[ \prod_{i=1}^n \Phi(\mu_i + \sigma_i Z) \right] = \prod_{i=1}^n \Phi \left(e_i \right).
%  \]
%  where $e_i$ are the elements of $\be = \bL^{-1} [\mu_1 \cdots \mu_n]^\top$, and $\bL$ is the Cholesky decomposition of the $n\times n$ matrix $\bSigma = [\sigma_1 \cdots \sigma_n] \otimes [\sigma_1 \cdots \sigma_n] + \bI_n$.
%\end{lemma}
%
%\begin{proof}
%Let $Z, Z_1, \dots, Z_n$ denote standard normal random variables independent of each other. Then 
%  \begin{align*}
%    \E_Z \left[ \prod_{i=1}^n \Phi(\mu_i + \sigma_i Z) \right]
%    &= \int \prod_{i=1}^n \Phi(\mu_i + \sigma_i z) \phi(z) \d z \\
%    &= \idotsint \ind[z_i < \mu_i + \sigma_i z: \forall i] \, \phi(z_1) \cdots \phi (z_n) \phi(z) \d z_1 \cdots \d z_n \d z \\
%    &= \Prob \left[Z_i < \mu_i + \sigma_i Z: \forall i \right] \\
%    &= \Prob \left[Z_i - \sigma_i Z < \mu_i: \forall i \right] \\
%  \end{align*}
%  Now if we define new random variables $U_i = Z_i - \sigma_i Z$ then each $U_i$ will be normally distributed with mean 0 and variance $1 + \sigma_i^2$. The covariance between any two pair of random variables $U_i$ and $U_j$ will be $\sigma_i\sigma_j$. Thus, the joint distribution of $\bU = (U_1, \dots, U_n)^\top$ is multivariate normal with mean $\bzero_n$ and covariance given by the $n \times n$ matrix $\bSigma = \bsigma \otimes \bsigma + \bI_n$, where $\bsigma = (\sigma_1, \dots, \sigma_n)^\top$, and $\otimes$ denotes the outer product. For a symmetric, positive-definite matrix such as $\bSigma$, we can obtain the Cholesky decomposition
%  \[
%    \bSigma = \bL\bL^\top
%  \]
%  where $\bL$ is lower-triangular. Now, if $\bW$ is a standardised $n$-variate normal distribution, then clearly $\bL\bW$ will have the same distribution as $\bU$. Thus, denoting $\bmu = (\mu_1, \dots, \mu_n)$,
%  \begin{align*}
%    \Prob \left[Z_i - \sigma_i Z < \mu_i: \forall i \right] 
%    &= \Prob[\bU < \bmu] \\
%    &= \Prob[\bL\bW < \bmu] \\
%    &= \Prob[\bW < \bL^{-1}\bmu] \\
%    &= \prod_{i=1}^n \Phi(e_i)
%  \end{align*}  
%  where $e_i$ are the elements of $\be = \bL^{-1}\bmu$.
%\end{proof}


\subsection{Special case: Binary responses}

Consider binary response variables $y_1, \dots, y_n$
\[
  y_i \sim \Bern(p_i).
\]
We also assume that there exists some continuous underlying latent variables $y_1^*, \dots, y_n^*$ such that
\[
  y_i =
  \begin{cases}
    1 &\text{if } y_i^* \geq 0 \\
    0 &\text{if } y_i^* < 0
  \end{cases}
\]
We consider modelling these latent variables according to the regression problem
\[
  y_i^* = \alpha + f(x_i) + \epsilon_i,
\]
where $\alpha$ is an intercept and the $\epsilon_i$s are iid normal with mean zero and some variance $1/\psi$. We can model these underlying latent variables with an I-prior. Let $x_i = (x_{i1}, \dots, x_{ip})$ be a set of covariates for each of the $i$ observations. Define the kernel matrix as $\mathbf H$, where the $(i,j)$ entries of this matrix are
\[
  \mathbf H(i,j) = h(x_i, x_j)
\]
with some positive definite kernel function $h$ defined on the set of covariates. An I-prior on the regression function $f$ is then 
\[
  f(x_i) = \lambda \sum_{j=1}^n h(x_i, x_j) w_j
\]
where $\lambda$ is the scale parameter of the RKHS with $h$ as the reproducing kernel, and the $w_j$ are iid normal with mean zero and variance $\psi$. In this case, we can show that
\begin{align*}
  p_i &= \Prob[y_i = 1] \\
  &= \Prob[y_i^* \geq 0] \\
  &= \Prob[\alpha + f(x_i) + \epsilon_i \geq 0 ] \\
  &= \Prob[\epsilon_i < \alpha + f(x_i)] \\
  &= \Phi\left(\psi^{1/2} \left(  \alpha + \lambda \sum\nolimits_{j=1}^n h(x_i, x_j) w_j \right) \right)
\end{align*}
where $\Phi$ is the cdf of a standard normal density.

\subsection{Prediction}
\hltodo{Need to move this to some place more suitable}

For a new data point $x_{\text{new}}$, we calculate the predicted latent values $\tilde f_{\text{new}} = (\tilde f_{\text{new},1}, \dots, \tilde f_{\text{new},m})$ for each of the classes, using the variational estimates of the posterior means for the unknown quantities (denoted with tildes), as follows:
\[
  \tilde f_{\text{new},j} = \tilde\alpha_j + \sum_{k=1}^n h_{\tilde\lambda_j}(x_{\text{new}}, x_{k})\tilde w_{kj}, \hspace{0.5cm} j = 1,\dots,m.
\]
The predicted class is equal to 
\[
  y_{\text{new}} = \argmax_j \tilde f_{\text{new},j}.
\]
To get the fitted probabilities for each class, the following integrals needs to be computed:
\begin{align*}
  \tilde p_{\text{new},j} 
  &= \E_Z \Bigg[ \mathop{\prod_{k=1}^m}_{k\neq j} \Phi\left(Z + \tilde f_{\text{new},j} - \tilde f_{\text{new},k} \right) \Bigg]  \\
  &= \int \mathop{\prod_{k=1}^m}_{k\neq j} \Phi\left(z + \tilde f_{\text{new},j} - \tilde f_{\text{new},k} \right) \phi(z) \d z
\end{align*}
for each $j \in \{1,\dots, m\}$.

\subsection{Standard errors for prediction}
\hltodo{Need to move this to some place more suitable}

Let $\by^*_i = (y^*_{i1}, \dots, y^*_{im})^\top$, and similarly $\bff_i = (f_{i1}, \dots, f_{im})^\top$. We know that 
\[
 \by^*_i \sim \N_m(\bff_i, \bI_m)
\]
and the class probabilities are contained in the vector $\bp_i = (p_{i1}, \dots, p_{im})^\top$ where each component is given by
\[
  p_{ij} = \E_Z \Bigg[ \mathop{\prod_{k=1}^m}_{k\neq j} \Phi(Z + f_{ij} - f_{ik}) \Bigg] =: g_j(z).
\]

The pdf of each of the $p_{ij}$ is obtained via the pdf transform formula
\[
  p(p_{ij}) = \phi\big(g_j^{-1}(p_{ij})\big) \left\vert \frac{\partial}{\partial p_{ij}}  g_j^{-1}(p_{ij}) \right\vert 
  = \frac{\partial}{\partial p_{ij}} \left[\Phi\big( g_j^{-1}(p_{ij}) \big) \right].
\]

Of interest are the $k$-th moments
\begin{align*}
  \E[p_{ij}^k] 
  &= \int_0^1 p_{ij}^k \, \frac{\partial}{\partial p_{ij}} \left[\Phi\big( g_j^{-1}(p_{ij}) \big) \right] \d p_{ij} \\
  &= \left[p_{ij}^k \, \Phi\big( g_j^{-1}(p_{ij}) \big) \right]_0^1 
  - \int_0^1 k p_{ij}^{k-1} \, \Phi\big( g_j^{-1}(p_{ij}) \big) \d p_{ij} \\
%  &= \left[p_{ij}^k \, \Phi\big( g_j^{-1}(p_{ij}) \big) \right]_0^1 
%  - k\E[p_{ij}^{k-1}]
\end{align*}

We can get the inverse function $g_j^{-1}:[0,1] \to \bbR$ by differentiating both sides with respect to $z$
\begin{align*}
  g_j(z) &= \int \mathop{\prod_{k=1}^m}_{k\neq j} \Phi(z + f_{ij} - f_{ik}) \phi(z) \d z \\
  \frac{\partial}{\partial z} g_j(z) &= \phi(z) \mathop{\prod_{k=1}^m}_{k\neq j} \Phi(z + f_{ij} - f_{ik})
\end{align*}
and solving this equation in $z$.

\[
    g(z) = \int_{-\infty}^\infty \mathop{\prod_{k=1}^m} \Phi(z + c_k) \phi(z) \d z 
\]

