%The challenge of estimation is then to first overcome this intractability by means of a suitable approximation of the integral.
%Several methods may be employed, namely quadrature methods, Laplace approximation, and Markov chain Monte Carlo (MCMC) methods, but these all fail in the face of high dimensionality when the sample size $n$ is large.

As with the normal I-prior model, an estimate of the posterior regression function with optimised hyperparameters is sought.
The log likelihood function $L(\cdot)$ for $\theta$ using all $n$ observations $\{(y_1,x_1),\dots,(y_n,x_n)\}$ is obtained by integrating out the I-prior from the categorical likelihood, as follows:
\begin{align}
  L(\theta) 
  &= \log \int p(\by | \bw, \theta) \, p(\bw|\theta) \dint \bw \nonumber \\
  &= \log \int \prod_{i=1}^n \prod_{j=1}^m \Big( g_j^{-1}\big(\alpha_k + 
  \greyoverbrace{f_k(x_i)}{\hidewidth\sum_{i'=1}^n h_\eta(x_i,x_{i'})w_{i'j}\hidewidth}
  \,\big)_{k=1}^m \Big)^{[y_i=j]} \cdot \MN_{n,m}(\bw|\bzero,\bI_n, \bPsi) \dint \bw \label{eq:intractablelikelihood}
\end{align}
where we have denoted the probit relationship from \cref{eq:pij} using the function $g_j^{-1}:\bbR^m \to [0,1]$.
Unlike in the continuous response models, the integral does not present itself in closed form due to the conditional categorical PMF of the $y_i$'s, which they themselves involve integrals of multivariate normal densities.
For binary response models, $g^{-1}$ is simply the probit function, but for multinomial responses, this can be quite challenging to evaluate---more on this in \hltodo{Section X}.

Furthermore, the posterior distribution of the regression function, which requires the density of $\bw|\by$, depends on the marginalisation provided by \cref{eq:intractablelikelihood}.
The challenge of estimation is then to first overcome this intractability by means of a suitable approximation of the integral.
We present three possible avenues to achieve this aim, namely the Laplace approximation, Markov chain Monte Carlo (MCMC) methods, and  variational Bayes.

%Methods of approximating the integral in \eqref{eq:intractablelikelihood} such as quadrature methods, Laplace approximation and MCMC tend to fail or are unsatisfactorily slow to accomplish.
%The main reason for this is the dimension of this integral, which is $nm$, and in particular, for large sample sizes and/or number of classes, is unfeasible for such methods.

\subsection{Laplace approximation}

To compute the posterior density $p(\bw|\by) \propto p(\by|\bw)p(\bw) =: e^{Q(\bw)}$ with normalising constant equal to the marginal density of $\by$, $p(\by) = \int e^{Q(\bw)} \dint \bw$, we have established that this is intractable.
Laplace's method \citep[ยง4.1.1, pp. 777--778]{kass1995bayes} entails expanding a Taylor series for $Q$ about its posterior mode $\hat\bw = \argmax_\bw p(\by|\bw)p(\bw)$, which gives the relationship
\begin{align*}
  Q(\bw) 
  &= Q(\hat\bw) + 
  \cancelto{0}{(\bw - \hat\bw)^\top \nabla Q(\hat\bw)} 
  - \half (\bw - \hat\bw)^\top \bOmega (\bw - \hat\bw) + \cdots \\
  &\approx Q(\hat\bw) + 
  - \half (\bw - \hat\bw)^\top \bOmega (\bw - \hat\bw),
\end{align*}
because, assuming that $Q$ has a unique maxima, $\nabla Q$ evaluated at its mode is zero.
This is recognised as the logarithm of an unnormalised Gaussian density, implying $\bw|\by \sim \N_n(\hat\bw,\bOmega^{-1})$.
Here, $\bOmega = -\nabla^2 Q(\bw)|_{\bw=\hat\bw}$ is the negative Hessian of $Q$ evaluated at the posterior mode, and is typically obtained as a byproduct of the maximisation routine of $Q$ using gradient or quasi-gradient based methods.

The marginal distribution is then approximated by
\begin{align*}
  p(\by) 
  &\approx \int \exp
  \greyoverbrace{Q(\bw)}{\hidewidth Q(\hat\bw) - \half (\bw - \hat\bw)^\top \bOmega (\bw - \hat\bw)\hidewidth}
   \dint \bw \\
  &= (2\pi)^{n/2} \abs{\bOmega}^{-1/2} e^{Q(\hat\bw)} 
  \int (2\pi)^{-n/2} \abs{\bOmega}^{1/2} \exp \left(- \half (\bw - \hat\bw)^\top \bOmega (\bw - \hat\bw) \right) \dint\bw \\
  &= (2\pi)^{n/2} \abs{\bOmega}^{-1/2} p(\by|\hat\bw)p(\hat\bw).
%  &= \cancelto{1}{\int \phi(\bw|\hat\bw,\bOmega^{-1})}
\end{align*} 
The log marginal density of course depends on the parameters $\theta$, which becomes the objective function to maximise in a likelihood maximising approach.
Note that, should a fully Bayesian approach be undertaken, i.e. priors prescribed on the model parameters using $\theta \sim p(\theta)$, then this approach is viewed as a maximum a posteriori approach.

In any case, each evaluation of the objective function $L(\theta) = \log p(\by|\btheta)$ involves finding the posterior modes $\hat\bw$.
This is a slow and difficult undertaking, especially for large sample sizes $n$---even assuming computation of the class probabilities $g^{-1}$ is efficient---because the dimension of this integral is exactly the sample size.
Furthermore, obtaining standard errors for the parameters are cumbersome, and it is likely that a computationally burdensome bootstrapping approach is needed.
Lastly, as a comment, Laplace's method only approximates the true marginal likelihood well if the true function is small far away from the mode.

% IT TURNS OUT THE EM IS DIFFICULT! BECAUSE THERE IS NO INHERENT ASSUMPTION OF INDEPENDENCE BETWEEN YSTAR AND W, SO THE DISTRIBUTIONS ARE DIFFICULT TO ASCERTAIN. 
% IN THE VARIATIONAL ALGORITHM, THIS IS ASSUMED IN A MEAN-FIELD APPROXIMATION
%\subsection{Expectation-maximisation algorithm}
%
%An EM algorithm similar to the one seen in the previous Chapter can be employed, with a slight modification.
%This time, treat both the latent propensities $\by^*$ and the I-prior random effects $\bw$ as `missing', so the complete data is $\{\by,\by^*,\bw\}$.
%Now, due to the independence of the observations $i=1,\dots,n$, the complete data log-likelihood is
%\begin{align*}
%  \log p&(\by,\by^*,\bw) \\
%  &= \sum_{i=1}^n \Big\{ 
%  \log p(y_i|\by^*_{i \bigcdot}) + \log p(\by^*_{i \bigcdot}|\bw_{i \bigcdot}) + \log p(\bw_{i \bigcdot}) 
%  \Big\} \\
%  &= - \half \sum_{i=1}^n \ind[y_{ij}^* = \max_k y_{ik}^*] \Bigg[
%%   \cancel{\half\log\abs{\bPsi}} 
%    (\by^*_{i \bigcdot} - \balpha - \bw_{i \bigcdot}^\top\bH_\eta)^\top \bPsi (\by^*_{i \bigcdot} - \balpha - \bw_{i \bigcdot}^\top\bH_\eta)
%   +  \bw_{i \bigcdot}^\top \bPsi^{-1} \bw_{i \bigcdot} \Bigg] \\
%  &\phantom{==} 
%%  \cancel{- \half\log\abs{\bPsi}} 
%   + \const
%\end{align*}
%which looks like the complete data log-likelihood seen previously in \cref{eq:QfnEstep}, except that here, together with the $\bw_{i \bigcdot}$'s, the $\by^*_{i \bigcdot}$'s are never observed.
%
%For the E-step, it is of interest to determine the posterior density $p(\by^*,\bw|\by) = p(\by^*|\bw,\by)p(\bw|\by) = p(\bw|\by^*,\by)p(\by^*|\by)$.
%For the latent propensities, the conditional posterior mean of a normally distributed subject to a conical truncation $\cC_j = \{y_{ij}* > y_{ik}^* \,|\, \forall k \neq j \}$, i.e. $\by^*_{i \bigcdot}|\bw_{i \bigcdot},\{y_i=j\} \iid \tN_m(\balpha + \hat\bw_{i \bigcdot}^\top\bH_\eta, \bPsi^{-1},\cC_j)$, for each $i=1,\dots,n$, and $\hat\bw$ is the posterior mean of $\bw$.
%The distribution for $\bw|\by^*$ is found to be similar to the posterior distribution of $\bw$ in the normal case as in \cref{eq:posteriorw}, except with $\by$ replaced with $\by^*$.
%To be specific, $\vecc \bw | \by^* \sim \N_{nm}(\vecc \tilde\bw, \tilde \bV_w)$, where
%\begin{align*}
%  \vecc \tilde\bw = \tilde\bV_w (\bPsi \otimes \bH_\eta) \vecc(\by^* - \bone_n\balpha^\top)
%  \hspace{0.5cm}\text{and}\hspace{0.5cm}
%  \tilde \bV_w^{-1} = (\bPsi \otimes \bH_\eta^2) + (\bPsi^{-1} \otimes \bI_n).
%\end{align*}
%To obtain the first and second posterior moments for the I-prior random effects, use the law of total expectations:
%\begin{gather*}
%  \E[\vecc \bw|\by]
%  = \E \big[ \E[\vecc \bw | \by^*] \big| \by \big] =: \hat\bw \\
%  \text{and}\\
%  \E[\vecc \bw (\vecc \bw)^\top|\by]
%  = \E \big[ \E[\vecc \bw (\vecc \bw)^\top| \by^*] \big| \by \big] =: \hat\bW .
%\end{gather*}
%The $Q$ function, whose argument is $\theta = \{\balpha,\eta,\bPsi\}$, is then 
%\begin{align*}
%  Q(\theta) 
%  &= \E_{\by^*,\bw}\big[ \log p&(\by,\by^*,\bw) | \by,\theta^{(t)}\big] \\
%  &= \const - \half 
%\end{align*}
%



\subsection{Variational inference}

We turn to variational inference as a method of estimation. 
Variational methods are widely discussed in the machine learning literature, but there have been efforts to popularise it in statistics \citep{blei2017variational}.
In a fully Bayesian setting, one obtains an approximation to the intractable posterior distribution of interest, which is then used for inferential purposes in lieu of the actual posterior distribution.

In addition to the I-probit model, suppose that prior distributions are assigned on the hyperparameters of the model, $\theta \sim p(\theta)$.
By appending the latent variables $\{\by^*,\bw\}$ to the hyperparameters $\theta$, we seek an approximation
\[
  p(\by^*,\bw,\theta | \by) \approx \tilde q(\by^*,\bw,\theta),
\]
where $\tilde q$ satisfies $\tilde q = \argmin_q \KL(q\Vert p)$, subject to certain constraints.
The constraint considered by us in this thesis is that $q$ satisfies a \emph{mean-field} factorisation
\[
  q(\by^*,\bw,\theta) = q(\by^*)q(\bw)q(\theta).
\]
Under this scheme, the posterior for $\by^*$ is found to be a \emph{conically truncated multivariate normal} distribution, and for $\bw$, a multivariate normal distribution.
The posterior density $q(\theta)$ is often of a recognisable form, and usually one of the exponential family densities (normal, Wishart or gamma).
This is useful, because point estimates of the hyperparameters can be taken to be either the mean or mode of these well-known distributions.
In cases where $q(\theta)$ does not conform to an exponential family type density, then inference can still be done by sampling methods.

It can be shown that, for some variational density $q$, the marginal log-likelihood is an upper-bound for the quantity $\cL$
\[
  \log p(\by) \geq 
    \E_{ q} \log p(\by,\by^*,\bw,\theta)
    - \E_{ q} \log \tilde q(\by^*,\bw,\theta) =: \cL,
%  }{\cL},
\]
a quantity often referred to as the \emph{evidence lower bound} (ELBO).
It turns out that minimising $\KL(q\Vert p)$ is equivalent to maximising the ELBO, a quantity that is more practical to work with than the KL divergence.
That is, if $\tilde q$ approximates the true posterior well, then the ELBO is a suitable proxy for the maximised marginal log-likelihood.

The algorithm to obtain $\tilde q$ which maximises the ELBO is known as the \emph{coordinate ascent variational inference} (CAVI) algorithm.
The algorithm itself typically condenses to that of a simple, sequential updating scheme, akin to the expectation-maximisation (EM) algorithm for exponential families we saw in Chapter 4, which is very fast to implement compared to the other methods described in the previous subsections.
A full derivation of the variational algorithm used by us will be described in \cref{sec:iprobitvar}.
%\citep{mclachlan2007algorithm}

\subsection{Markov chain Monte Carlo methods}

As an alternative to the deterministic Bayesian approach of variational inference, it is possible to use Markov chain Monte Carlo sampling methods as an approach to stochastically approximate the intractable posterior distribution.

\citet{albert1993bayesian} showed that the latent variable approach to probit models can be analysed using exact Bayesian methods, due to the underlying normality structure.
Paired with corresponding conjugate prior choices, sampling from the posterior is very simple using a Gibbs sampling approach.
On the other hand, this data augmentation scheme enlarges the variable space to $n+q$ dimensions, where $q$ is the number of parameters to estimate, which is inefficient and computationally challenging especially when $n$ is large.
It is no longer possible to marginalise the normal latent variables from the model, as this is intractable, as discussed previously.

Hamiltonian Monte Carlo is another possibility, since it does not require conjugacy.
For binary models, this is a feasible approach because the class probabilities are a function of the normal CDF, which means that it is doable using off-the-shelf software such as \proglang{Stan}.
However, with multinomial responses, the arduous task of computing class probabilities, which involve integration of an at most $m$-dimensional normal density, must be addressed separately.

%In summary, the computational challenge here stems from two sources: 1) integrating out the random effects $\bw$; and 2) evaluating class probabilities.
%Point 1) is addressed using a Gibbs sampling data augmentation scheme (latent variable approach), but this is not feasible with large $n$.
%Point 2) remains regardless whether Gibbs sampling or HMC is used.

\subsection{Comparison of estimation methods}

\hltodo{Compare: Laplace, variational and HMC.}

The three estimation methods described aim to overcome the intractable integral by means of either a deterministic approximation (Laplace and variational inference) or a stochastic approximation (MCMC).
In the Laplace and variational method, the posterior distribution of $\bw$ ends up being approximated by a Gaussian distribution, although the mean and variance is different in each method.
In essence, once $\bw|\by$ is approximately normal, then estimation of the parameters $\theta$ using a direct optimisation approach or an EM-type approach is straightforward.
On the other hand, MCMC approximates the density $p(\bw|\by)$ using samples generated via Gibbs sampling or HMC, and these samples would asymptotically be representative of draws from the true posterior.

Consider the data set...
Plot the data. Explain priors for HMC and variational. Compare.



