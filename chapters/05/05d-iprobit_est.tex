%The challenge of estimation is then to first overcome this intractability by means of a suitable approximation of the integral.
%Several methods may be employed, namely quadrature methods, Laplace approximation, and Markov chain Monte Carlo (MCMC) methods, but these all fail in the face of high dimensionality when the sample size $n$ is large.

As with the normal I-prior model, an estimate of the posterior regression function with optimised hyperparameters is sought.
The log likelihood function $L(\cdot)$ for $\theta$ using all $n$ observations $\{(y_1,x_1),\dots,(y_n,x_n)\}$ is obtained by integrating out the I-prior from the categorical likelihood, as follows:
\begin{align}
  L(\theta) 
  &= \log \int p(\by | \bw, \theta) \, p(\bw|\theta) \dint \bw \nonumber \\
  &= \log \int \prod_{i=1}^n \prod_{j=1}^m \Big( g^{-1}\big(\alpha_k + 
  \greyoverbrace{f_k(x_i)}{\hidewidth\sum_{i'=1}^n h_\eta(x_i,x_{i'})w_{i'}\hidewidth}
  \,\big)_{k=1}^m \Big)^{[y_i=j]} \cdot \phi(\bw|\bzero,  \bPsi \otimes \bI_n ) \dint \bw \label{eq:intractablelikelihood}
\end{align}
where we have denoted the probit relationship from \eqref{eq:pij} using the function $g^{-1}:\bbR^m \to [0,1]$.
Unlike in the continuous response models, the integral does not present itself in closed form due to the conditional categorical PMF of the $y_i$'s, which they themselves involve integrals of normal densities.
Furthermore, the posterior distribution of the regression function, which requires the density of $\bw|\by$, depends on the marginalisation provided by \cref{eq:intractablelikelihood}.
The challenge of estimation is then to first overcome this intractability by means of a suitable approximation of the integral.
We present three possible avenues to achieve this aim, namely the Laplace approximation, Markov chain Monte Carlo (MCMC) methods, and  variational Bayes.

%Methods of approximating the integral in \eqref{eq:intractablelikelihood} such as quadrature methods, Laplace approximation and MCMC tend to fail or are unsatisfactorily slow to accomplish.
%The main reason for this is the dimension of this integral, which is $nm$, and in particular, for large sample sizes and/or number of classes, is unfeasible for such methods.

\subsection{Laplace approximation}

One is interested in the posterior density $p(\bw|\by) \propto p(\by|\bw)p(\bw) =: e^{Q(\bw)}$, with normalising constant equal to the marginal density of $\by$, $p(\by) = \int e^{Q(\bw)} \dint \bw$, and we have established that the calculation of this marginal density is intractable.
Laplace's method \citep[ยง4.1.1, pp. 777--778]{kass1995bayes} entails expanding a Taylor series for $Q$ about its posterior mode $\hat\bw = \argmax_\bw p(\by|\bw)p(\bw)$, and this gives the relationship
\begin{align*}
  Q(\bw) 
  &= Q(\hat\bw) + 
  \cancelto{0}{(\bw - \hat\bw)^\top \nabla Q(\hat\bw)} 
  - \half (\bw - \hat\bw)^\top \bOmega (\bw - \hat\bw) + \cdots \\
  &\approx Q(\hat\bw) + 
  - \half (\bw - \hat\bw)^\top \bOmega (\bw - \hat\bw)
\end{align*}
because, assuming that $Q$ has a unique maxima, $\nabla Q$ evaluated at its mode is zero.
This is recognised as the logarithm of an unnormalised Gaussian density, implying $\bw|\by \sim \N_n(\hat\bw,\bOmega^{-1})$.
Here, $\bOmega = -\nabla^2 Q(\bw)|_{\bw=\hat\bw}$ is the negative Hessian of $Q$ evaluated at the posterior mode.

The marginal distribution is then approximated by
\begin{align*}
  p(\by) 
  &\approx \int \exp
  \greyoverbrace{Q(\bw)}{\hidewidth Q(\hat\bw) - \half (\bw - \hat\bw)^\top \bOmega (\bw - \hat\bw)\hidewidth}
   \dint \bw \\
  &= (2\pi)^{n/2} \abs{\bOmega}^{-1/2} e^{Q(\hat\bw)} 
  \int (2\pi)^{-n/2} \abs{\bOmega}^{1/2} \exp \left(- \half (\bw - \hat\bw)^\top \bOmega (\bw - \hat\bw) \right) \dint\bw \\
  &= (2\pi)^{n/2} \abs{\bOmega}^{-1/2} p(\by|\hat\bw)p(\hat\bw).
%  &= \cancelto{1}{\int \phi(\bw|\hat\bw,\bOmega^{-1})}
\end{align*} 
The log marginal density of course depends on the parameters $\theta$, which becomes the objective function to maximise in a likelihood maximising approach.
Note that, should a fully Bayesian approach be undertaken, i.e. priors prescribed on the model parameters using $\theta \sim p(\theta)$, then this approach is viewed as a maximum a posteriori approach.

In fact, under an EM algorithm approach, using the approximate posterior density which is normally distributed is simply using the posterior mode in lieu of the actual posterior means.
\hltodo[Expand on this further.]{}

In any case, each evaluation of the objective function $L(\theta) = \log p(\by|\btheta)$ involves finding the posterior modes $\hat\bw$.
This is a slow and difficult undertaking, especially for large sample sizes $n$, because the dimension of this integral is exactly the sample size.
Furthermore, standard errors for the parameters are cumbersome to calculate as well.
Lastly, as a comment, Laplace's method only approximates the true marginal likelihood well if the true function is small far away from the mode.

% IT TURNS OUT THE EM IS DIFFICULT! BECAUSE THERE IS NO INHERENT ASSUMPTION OF INDEPENDENCE BETWEEN YSTAR AND W, SO THE DISTRIBUTIONS ARE DIFFICULT TO ASCERTAIN. 
% IN THE VARIATIONAL ALGORITHM, THIS IS ASSUMED IN A MEAN-FIELD APPROXIMATION
%\subsection{Expectation-maximisation algorithm}
%
%An EM algorithm similar to the one seen in the previous Chapter can be employed, with a slight modification.
%This time, treat both the latent propensities $\by^*$ and the I-prior random effects $\bw$ as `missing', so the complete data is $\{\by,\by^*,\bw\}$.
%Now, due to the independence of the observations $i=1,\dots,n$, the complete data log-likelihood is
%\begin{align*}
%  \log p&(\by,\by^*,\bw) \\
%  &= \sum_{i=1}^n \Big\{ 
%  \log p(y_i|\by^*_{i \bigcdot}) + \log p(\by^*_{i \bigcdot}|\bw_{i \bigcdot}) + \log p(\bw_{i \bigcdot}) 
%  \Big\} \\
%  &= - \half \sum_{i=1}^n \ind[y_{ij}^* = \max_k y_{ik}^*] \Bigg[
%%   \cancel{\half\log\abs{\bPsi}} 
%    (\by^*_{i \bigcdot} - \balpha - \bw_{i \bigcdot}^\top\bH_\eta)^\top \bPsi (\by^*_{i \bigcdot} - \balpha - \bw_{i \bigcdot}^\top\bH_\eta)
%   +  \bw_{i \bigcdot}^\top \bPsi^{-1} \bw_{i \bigcdot} \Bigg] \\
%  &\phantom{==} 
%%  \cancel{- \half\log\abs{\bPsi}} 
%   + \const
%\end{align*}
%which looks like the complete data log-likelihood seen previously in \cref{eq:QfnEstep}, except that here, together with the $\bw_{i \bigcdot}$'s, the $\by^*_{i \bigcdot}$'s are never observed.
%
%For the E-step, it is of interest to determine the posterior density $p(\by^*,\bw|\by) = p(\by^*|\bw,\by)p(\bw|\by) = p(\bw|\by^*,\by)p(\by^*|\by)$.
%For the latent propensities, the conditional posterior mean of a normally distributed subject to a conical truncation $\cC_j = \{y_{ij}* > y_{ik}^* \,|\, \forall k \neq j \}$, i.e. $\by^*_{i \bigcdot}|\bw_{i \bigcdot},\{y_i=j\} \iid \tN_m(\balpha + \hat\bw_{i \bigcdot}^\top\bH_\eta, \bPsi^{-1},\cC_j)$, for each $i=1,\dots,n$, and $\hat\bw$ is the posterior mean of $\bw$.
%The distribution for $\bw|\by^*$ is found to be similar to the posterior distribution of $\bw$ in the normal case as in \cref{eq:posteriorw}, except with $\by$ replaced with $\by^*$.
%To be specific, $\vecc \bw | \by^* \sim \N_{nm}(\vecc \tilde\bw, \tilde \bV_w)$, where
%\begin{align*}
%  \vecc \tilde\bw = \tilde\bV_w (\bPsi \otimes \bH_\eta) \vecc(\by^* - \bone_n\balpha^\top)
%  \hspace{0.5cm}\text{and}\hspace{0.5cm}
%  \tilde \bV_w^{-1} = (\bPsi \otimes \bH_\eta^2) + (\bPsi^{-1} \otimes \bI_n).
%\end{align*}
%To obtain the first and second posterior moments for the I-prior random effects, use the law of total expectations:
%\begin{gather*}
%  \E[\vecc \bw|\by]
%  = \E \big[ \E[\vecc \bw | \by^*] \big| \by \big] =: \hat\bw \\
%  \text{and}\\
%  \E[\vecc \bw (\vecc \bw)^\top|\by]
%  = \E \big[ \E[\vecc \bw (\vecc \bw)^\top| \by^*] \big| \by \big] =: \hat\bW .
%\end{gather*}
%The $Q$ function, whose argument is $\theta = \{\balpha,\eta,\bPsi\}$, is then 
%\begin{align*}
%  Q(\theta) 
%  &= \E_{\by^*,\bw}\big[ \log p&(\by,\by^*,\bw) | \by,\theta^{(t)}\big] \\
%  &= \const - \half 
%\end{align*}
%

\subsection{Markov chain Monte Carlo methods}

\citet{albert1993bayesian} showed that the latent variable approach to probit models can be analysed using exact Bayesian methods, due to the underlying normality structure.
Paired with corresponding conjugate prior choices, sampling from the posterior is very simple using a Gibbs sampling approach.
On the other hand, this data augmentation scheme enlarges the variable space to $n+q$ dimensions, where $q$ is the number of parameters to estimate, which is inefficient and computationally challenging especially when $n$ is large.
It is no longer possible to marginalise the normal latent variables from the model, as this is intractable, just as we discussed previously.

Hamiltonian Monte Carlo is another possibility, since it does not require conjugacy.
For binary models, this is a feasible approach because the class probabilities are a function of the normal CDF, which means that it is doable in off-the-shelf software such as \proglang{Stan}.
Things get out of hand with multinomial responses, because the intractability of computing class probabilities is not addressed.

In summary, the computational challenge here stems from two sources: 1) integrating out the random effects $\bw$; and 2) evaluating class probabilities.
Point 1) is addressed using a Gibbs sampling data augmentation scheme (latent variable approach), but this is not feasible with large $n$.
Point 2) remains regardless whether Gibbs sampling or HMC is used.



\subsection{Variational inference}

We turn to variational inference as a method of estimation. Variational methods are widely discussed in the machine learning literature, and there have been efforts to popularise it in statistics \citep{blei2017variational}.

By factorising appropriately, we can obtain approximated posteriors for the regression function and the parameters of the I-prior model.
The algorithm itself typically condenses to that of a simple, sequential updating scheme, akin to the expectation-maximisation (EM) algorithm for exponential families we saw in Chapter 4, which is very fast to implement compared to the other methods described in the previous subsections.
A full derivation of the variational algorithm used by us will be described in \cref{sec:iprobitvar}.
%\citep{mclachlan2007algorithm}

\subsection{Comparison of estimation methods}

\hltodo{Compare: Laplace, variational and HMC.}

The three estimation methods described aim to overcome the intractable integral by means of either a deterministic approximation (Laplace and variational inference) or a stochastic approximation (MCMC).
In the Laplace and variational method, the posterior distribution of $\bw$ ends up being approximated by a Gaussian distribution, although the mean and variance is different in each method.
In essence, once $\bw|\by$ is approximately normal, then estimation of the parameters $\theta$ using a direct optimisation approach or an EM-type approach is straightforward.
On the other hand, MCMC approximates the density $p(\bw|\by)$ using samples generated via Gibbs sampling or HMC, and these samples would asymptotically be representative of draws from the true posterior.

Consider the data set...
Plot the data. Explain priors for HMC and variational. Compare.



