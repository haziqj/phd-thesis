\subsection{A brief introduction to variational inference}

Suppose that, in a fully Bayesian setting, we append the unknown model parameters to the latent variables to form $\bz = \{\by^*, \bw, \theta\}$.
The crux of variational inference is this: find a suitably close distribution function $q(\bz)$ that approximates the true posterior $p(\bz|\by)$, where closeness here is defined in the Kullback-Leibler divergence sense,
\[
  \KL(q\Vert p) = \int \log \frac{q(\bz)}{p(\bz|\by)} q(\bz) \dint \bz.
\]
One may then show that log marginal density (the log of the intractable integral) holds the following bound:
    \begin{align}
      \log p(\by) &= \log p(\by,\bz) - \log p(\bz|\by) \nonumber \\
      &= \int \left\{ \log \frac{p(\by,\bz)}{q(\bz)} - \log \frac{p(\bz|\by)}{q(\bz)} \right\} q(\bz) \dint \bz \nonumber \\    
      &=  \cL(q) +  \KL(q \Vert p) \nonumber \\
      &\geq \cL(q) \label{eq:varbound}
    \end{align}
since the KL divergence is a non-negative quantity.
The functional $\cL(q)$ given by 
\begin{align}
  \cL(q) 
  &= \int \log \frac{p(\by,\bz)}{q(\bz)} q(\bz) \dint \bz \nonumber \\
%  &= \E_{\bz\sim q}[\log p(\by,\bz) - \log q(\bz)] \nonumber \\
  &= \E_{\bz\sim q}[\log p(\by,\bz)] + H(q), \label{eq:elbo1}
\end{align}
where $H$ is the entropy functional, is known as the \emph{evidence lower bound} (ELBO), which serves as the proxy objective function in the likelihood maximisation problem.
Evidently, the closer $q$ is to the true $p$, the better, and this is achieved by maximising $\cL$, or equivalently, minimising the KL divergence\footnote{The astute reader will realise that $\KL(q||p)$ is impossible to compute, since one does not know the true distribution $p(\bz|\by)$. Efforts are concentrated on maximising the ELBO instead.} from $p$ to $q$.
Note that the bound \cref{eq:varbound} achieves equality if and only if $q \equiv p$, but of course the true form of the posterior is unknown to us.
Maximising $\cL(q)$ or minimising $\KL(q\Vert p)$ with respect to the density $q$ is a problem of calculus of variations, which incidentally, is where variational inference takes its name.

Maximising $\cL$ over all possible density functions $q$ is not possible without considering certain constraints.
Two such constraints are described. 
The first, is to make a distributional assumption regarding $q$, for which it is parameterised by $\nu$.
For instance, we might choose the closest normal distribution to the posterior $p(\bz|\by)$ in terms of KL divergence.
In this case, the task is to find optimal mean and variance parameters of a normal distribution.

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}
    \fill (4.05,2.1) circle (0pt) node[right,yshift=1] {$p(\bz|\by)$};
    \draw[ultra thick] (0,0) ellipse (4cm and 2.2cm);
    \node at (-2.8,0.7) {$q(\bz;\nu)$};
    \draw[thick,colred] (-0.8,-0.5) to [curve through={(-2,0) .. (-2,-0.8) .. (1,-0.8) .. (-1,1.5) .. (0,1.5) ..(0.3,0.1) .. (0.5,1.5) .. (2,0) .. (2.1,-0.1) .. (2.2,1.1) .. (3,0.9)}] (3.317,1.225);  % curve using hobby tikz
    \draw[dashed, very thick,black!50] (3.317,1.225) -- (4.05,2.1);
    \fill (-0.8,-0.5) circle (2.2pt) node[below] {$\nu^{\text{init}}$};
    \fill (3.317,1.225) circle (2.2pt) node[left,yshift=-1] {$\nu^*$};
    \fill (4.05,2.1) circle (2.2pt) node[right,yshift=1] {$p(\bz|\by)$};
    \node[gray] at (4.5,1.4) {$\KL(q\Vert p)$};
  \end{tikzpicture}
  \caption{Schematic view of variational inference. The aim is to find the closest distribution $q$ (parameterised by a variational parameter $\nu$) to $p$ in terms of KL divergence within the set of variational distributions, represented by the ellipse.}
\end{figure}

The second type of constraint, and the one considered in this thesis, is simply an assumption that the approximate posterior $q$ factorises into $M$ disjoint factors.
Supposing that the elements of $\bz$ may indeed be partitioned into $M$ disjoint groups $\bz = (z^{(1)},\dots,z^{(M)})$, then the structure
\[
  q(\bz) = \prod_{k=1}^M q_k(z^{(k)})
\]
for $q$ is considered.
This factorised form of variational inference is known in the statistical physics literature as the \emph{mean-field theory} \citep{itzykson1991statistical}.

Denote by $\tilde q$ the distributions which minimise the Kullbeck-Leibler divergence (maximise the variational lower bound).
By appealing to \citet[equation 10.9, p. 466]{bishop2006pattern}, we find that for each $\xi \in \{ \by^*,\bw,\theta \} =: \cZ$, $\tilde q$ satisfies
\begin{align}\label{eq:qtilde}
  \log \tilde q(\xi) = \E_{-\xi} [\log p(\by, \by^*, \bw, \theta)] + \const
\end{align}
where expectation of the log joint density of $(\by, \by^*, \bw, \theta)$ is taken with respect to all of the unknowns $\cZ$ except the one currently in consideration, under their respective $q$ densities. 
Estimates of the latent variables and parameters are then obtained by taking the mean of their respective approximate posterior distribution.

In practice, rather than an explicit calculation of the normalising constant, one simply needs to inspect \cref{eq:qtilde} to recognise it as a known log-density function, which is the case when exponential family distributions are considered.
That is, suppose that each complete conditional $p(\xi|\cZ_{-\xi}, \by)$ follows an exponential family distribution,
\[
  p(\xi|\cZ_{-\xi}, \by) = B(\xi)\exp \big(\ip{\zeta_\xi(\cZ_{-\xi}, \by) , \xi} - A(\zeta_\xi) \big).
\]
Then, from \cref{eq:qtilde},
\begin{align*}
  \tilde q(\xi)
  &\propto \exp\big(\E_{-\xi}[\log p(\xi|\cZ_{-\xi}, \by)] \big) \\
  &= \exp \Big(\log B(\xi) + \E \ip{\zeta_\xi(\cZ_{-\xi}, \by) , \xi} - \E [ A(\zeta_\xi) ] \Big) \\
  &\propto B(\xi)\exp \E\ip{\zeta_\xi(\cZ_{-\xi}, \by) , \xi}
\end{align*}
is also in the same exponential family.
In situations where there is no closed form expression for $\tilde q$, then one resorts to sampling methods such as a Metropolis random walk to estimate quantities of interest.
This stochastic step within a deterministic algorithm has been explored before in the context of a Monte Carlo EM algorithm---see \citet[ยง4, pp. 537--538]{meng1997algorithm} and references therein.

\subsection{The EM algorithm is intractable---variational Bayes EM}












