\subsection{A brief introduction to variational inference}

Suppose that, in a fully Bayesian setting, we append the unknown model parameters to the latent variables to form $\bz = \{\by^*, \bw, \theta\}$.
The crux of variational inference is this: find a suitably close distribution function $q(\bz)$ that approximates the true posterior $p(\bz|\by)$, where closeness here is defined in the Kullback-Leibler divergence sense,
\[
  \KL(q\Vert p) = \int \log \frac{q(\bz)}{p(\bz|\by)} q(\bz) \dint \bz.
\]
One may then show that log marginal density (the log of the intractable integral) holds the following bound:
    \begin{align}
      \log p(\by) &= \log p(\by,\bz) - \log p(\bz|\by) \nonumber \\
      &= \int \left\{ \log \frac{p(\by,\bz)}{q(\bz)} - \log \frac{p(\bz|\by)}{q(\bz)} \right\} q(\bz) \dint \bz \nonumber \\    
      &=  \cL(q) +  \KL(q \Vert p) \nonumber \\
      &\geq \cL(q) \label{eq:varbound}
    \end{align}
since the KL divergence is a non-negative quantity.
The functional $\cL(q)$ given by 
\begin{align}
  \cL(q) 
  &= \int \log \frac{p(\by,\bz)}{q(\bz)} q(\bz) \dint \bz \nonumber \\
%  &= \E_{\bz\sim q}[\log p(\by,\bz) - \log q(\bz)] \nonumber \\
  &= \E_{\bz\sim q}[\log p(\by,\bz)] + H(q), \label{eq:elbo1}
\end{align}
where $H$ is the entropy functional, is known as the \emph{evidence lower bound} (ELBO), which serves as the proxy objective function in the likelihood maximisation problem.
Evidently, the closer $q$ is to the true $p$, the better, and this is achieved by maximising $\cL$, or equivalently, minimising the KL divergence\footnote{The astute reader will realise that $\KL(q||p)$ is impossible to compute, since one does not know the true distribution $p(\bz|\by)$. Efforts are concentrated on maximising the ELBO instead.} from $p$ to $q$.
Note that the bound \cref{eq:varbound} achieves equality if and only if $q \equiv p$, but of course the true form of the posterior is unknown to us.
Maximising $\cL(q)$ or minimising $\KL(q\Vert p)$ with respect to the density $q$ is a problem of calculus of variations, which incidentally, is where variational inference takes its name.

Maximising $\cL$ over all possible density functions $q$ is not possible without considering certain constraints.
Two such constraints are described. 
The first, is to make a distributional assumption regarding $q$, for which it is parameterised by $\nu$.
For instance, we might choose the closest normal distribution to the posterior $p(\bz|\by)$ in terms of KL divergence.
In this case, the task is to find optimal mean and variance parameters of a normal distribution.

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}
    \fill (4.05,2.1) circle (0pt) node[right,yshift=1] {$p(\bz|\by)$};
    \draw[ultra thick] (0,0) ellipse (4cm and 2.2cm);
    \node at (-2.8,0.7) {$q(\bz;\nu)$};
    \draw[thick,colred] (-0.8,-0.5) to [curve through={(-2,0) .. (-2,-0.8) .. (1,-0.8) .. (-1,1.5) .. (0,1.5) ..(0.3,0.1) .. (0.5,1.5) .. (2,0) .. (2.1,-0.1) .. (2.2,1.1) .. (3,0.9)}] (3.317,1.225);  % curve using hobby tikz
    \draw[dashed, very thick,black!50] (3.317,1.225) -- (4.05,2.1);
    \fill (-0.8,-0.5) circle (2.2pt) node[below] {$\nu^{\text{init}}$};
    \fill (3.317,1.225) circle (2.2pt) node[left,yshift=-1] {$\nu^*$};
    \fill (4.05,2.1) circle (2.2pt) node[right,yshift=1] {$p(\bz|\by)$};
    \node[gray] at (4.5,1.4) {$\KL(q\Vert p)$};
  \end{tikzpicture}
  \caption{Schematic view of variational inference. The aim is to find the closest distribution $q$ (parameterised by a variational parameter $\nu$) to $p$ in terms of KL divergence within the set of variational distributions, represented by the ellipse.}
\end{figure}

The second type of constraint, and the one considered in this thesis, is simply an assumption that the approximate posterior $q$ factorises into $M$ disjoint factors.
Supposing that the elements of $\bz$ may indeed be partitioned into $M$ disjoint groups $\bz = (z^{(1)},\dots,z^{(M)})$, then the structure
\[
  q(\bz) = \prod_{k=1}^M q_k(z^{(k)})
\]
for $q$ is considered.
This factorised form of variational inference is known in the statistical physics literature as the \emph{mean-field theory} \citep{itzykson1991statistical}.

Denote by $\tilde q$ the distributions which minimise the Kullbeck-Leibler divergence (maximise the variational lower bound).
By appealing to \citet[equation 10.9, p. 466]{bishop2006pattern}, we find that for each $\xi \in \{ \by^*,\bw,\theta \} =: \cZ$, $\tilde q$ satisfies
\begin{align}\label{eq:qtilde}
  \log \tilde q(\xi) = \E_{-\xi} [\log p(\by, \by^*, \bw, \theta)] + \const
\end{align}
where expectation of the log joint density of $(\by, \by^*, \bw, \theta)$ is taken with respect to all of the unknowns $\cZ$ except the one currently in consideration, under their respective $q$ densities. 
Estimates of the latent variables and parameters are then obtained by taking the mean of their respective approximate posterior distribution.

In practice, rather than an explicit calculation of the normalising constant, one simply needs to inspect \cref{eq:qtilde} to recognise it as a known log-density function, which is the case when exponential family distributions are considered.
That is, suppose that each complete conditional $p(\xi|\cZ_{-\xi}, \by)$ follows an exponential family distribution,
\[
  p(\xi|\cZ_{-\xi}, \by) = B(\xi)\exp \big(\ip{\zeta_\xi(\cZ_{-\xi}, \by) , \xi} - A(\zeta_\xi) \big).
\]
Then, from \cref{eq:qtilde},
\begin{align*}
  \tilde q(\xi)
  &\propto \exp\big(\E_{-\xi}[\log p(\xi|\cZ_{-\xi}, \by)] \big) \\
  &= \exp \Big(\log B(\xi) + \E \ip{\zeta_\xi(\cZ_{-\xi}, \by) , \xi} - \E [ A(\zeta_\xi) ] \Big) \\
  &\propto B(\xi)\exp \E\ip{\zeta_\xi(\cZ_{-\xi}, \by) , \xi}
\end{align*}
is also in the same exponential family.
In situations where there is no closed form expression for $\tilde q$, then one resorts to sampling methods such as a Metropolis random walk to estimate quantities of interest.
This stochastic step within a deterministic algorithm has been explored before in the context of a Monte Carlo EM algorithm---see \citet[ยง4, pp. 537--538]{meng1997algorithm} and references therein.

\subsection{Variational methods and the EM algorithm}

\hltodo{Goal: maximise log-likelihood wrt parameters theta.}

\subsection{The EM algorithm is intractable---variational Bayes EM?}

% IT TURNS OUT THE EM IS DIFFICULT! BECAUSE THERE IS NO INHERENT ASSUMPTION OF INDEPENDENCE BETWEEN YSTAR AND W, SO THE DISTRIBUTIONS ARE DIFFICULT TO ASCERTAIN. 
% IN THE VARIATIONAL ALGORITHM, THIS IS ASSUMED IN A MEAN-FIELD APPROXIMATION

Consider employing an EM algorithm, similar to the one seen in the previous chapter, to estimate I-probit models.
This time, treat both the latent propensities $\by^*$ and the I-prior random effects $\bw$ as `missing', so the complete data is $\{\by,\by^*,\bw\}$.
Now, due to the independence of the observations $i=1,\dots,n$, the complete data log-likelihood is
\begin{align*}
  \log p&(\by,\by^*,\bw) \\
  &= \sum_{i=1}^n \Big\{ 
  \log p(y_i|\by^*_{i \bigcdot}) + \log p(\by^*_{i \bigcdot}|\bw_{i \bigcdot}) + \log p(\bw_{i \bigcdot}) 
  \Big\} \\
  &= - \half \sum_{i=1}^n \ind[y_{ij}^* = \max_k y_{ik}^*] \Bigg[
%   \cancel{\half\log\abs{\bPsi}} 
    (\by^*_{i \bigcdot} - \balpha - \bw_{i \bigcdot}^\top\bh_\eta(x_i))^\top \bPsi (\by^*_{i \bigcdot} - \balpha - \bw_{i \bigcdot}^\top\bh_\eta(x_i)) \\
  &\hspace{9.5cm} + \bw_{i \bigcdot}^\top \bPsi^{-1} \bw_{i \bigcdot} \Bigg] 
%  \cancel{- \half\log\abs{\bPsi}} 
   + \const
\end{align*}
which looks like the complete data log-likelihood seen previously in \cref{eq:QfnEstep}, except that here, together with the $\bw_{i \bigcdot}$'s, the $\by^*_{i \bigcdot}$'s are never observed.

For the E-step, it is of interest to determine the posterior density $p(\by^*,\bw|\by) = p(\by^*|\bw,\by)p(\bw|\by)$, which apparently is hard to obtain.
We can go as far as determining that the full conditional of the latent propensities is multivariate subject to a conical truncation $\cC_j = \{y_{ij}^* > y_{ik}^* \,|\, \forall k \neq j \}$, i.e. $\by^*_{i \bigcdot}|\bw_{i \bigcdot},\{y_i=j\} \iid \tN_m(\balpha + \bw_{i \bigcdot}^\top\bh_\eta(x_i), \bPsi^{-1},\cC_j)$, for each $i=1,\dots,n$, and that $\vecc \bw|\by^*\sim \N(\tilde\bw,\tilde\bV_w)$ is found to be similar to the distribution in \cref{eq:varipostw}.
%To be specific, $\vecc \bw | \by^* \sim \N_{nm}(\vecc \tilde\bw, \tilde \bV_w)$, where
%\begin{align*}
%  \vecc \tilde\bw = \tilde\bV_w (\bPsi \otimes \bH_\eta) \vecc(\by^* - \bone_n\balpha^\top)
%  \hspace{0.5cm}\text{and}\hspace{0.5cm}
%  \tilde \bV_w^{-1} = (\bPsi \otimes \bH_\eta^2) + (\bPsi^{-1} \otimes \bI_n).
%\end{align*}
To obtain the first and second posterior moments for the I-prior random effects, we can use the law of total expectations:
\begin{gather*}
  \E[\vecc \bw|\by]
  = \E_{\by^*} \big[ \E[\vecc \bw | \by^*] \big| \by \big] =: \hat\bw \\
  \text{and}\\
  \E[\vecc \bw (\vecc \bw)^\top|\by]
  = \E_{\by^*} \big[ \E[\vecc \bw (\vecc \bw)^\top| \by^*] \big| \by \big] =: \hat\bW ,
\end{gather*}
but this requires $p(\by^*|\by)$ which does not come by easily.
A similar problem has been faced by \citet{chan1997maximum}, who analysed binary linear probit models with random effects.
The authors ultimately resort to Monte Carlo sampling within an EM framework to overcome the difficult distributions of interest.

Suppose that, instead of the true posterior distribution $p(\by^*,\bw|\by)$ being used, a mean-field variational approximation $q(\by^*,\bw) = q(\by^*)q(\bw)$ is used instead.
As we know from \hltodo{Section X}, $q(\by^*)$ is a truncated multivariate normal distribution, and $q(\bw)$ is multivariate normal, whose means and second moments can be computed with some  effort.
Let $\bar\by^* = \by^* - \bone_n\balpha^\top$.
The (approximate) E-step then entails computing
\begin{align*}
  Q(\theta) 
  &= \E_{\by^*,\bw\sim q}  \log p(\by,\by^*,\bw|\theta) \\
  &= \const -\half \tr\E_{\by^*,\bw\sim q} \left[ 
  \bPsi(\bar\by^{*\top}\bar\by^* + \bw^\top\bH_\eta^2\bw - 2\bar\by^*\bPsi\bw^\top\bH_\eta )
  + \bPsi^{-1} \bw^\top\bw 
  \right].
\end{align*}
In the M-step, this is maximised with respect to $\theta$.
An iterative procedure is a natural port of call to address the coupling in the posterior variational densities and also parameter dependencies.
This leads to the so-called \emph{variational Bayes EM algorithm} (VB-EM) \citep{beal2003}.

In variational inference, a fully Bayesian treatment of the parameters is considered, with the aim of obtaining approximation to their posterior distributions.
In VB-EM, the variational approximation is only performed on the latent, or `missing' variables, to use the EM nomenclature.
After a variational E-step, the M-step proceeds as usual, and as such, all of the material relating to the EM in the previous chapter is applicable.
The VB-EM can also be seen as obtaining (approximate) maximum a posteriori estimates with diffuse priors on the parameters.
As per the discussion in \hltodo{Section X}, this alleviates the problem of non-conjugacy of the complete conditional for $\bPsi$.

One downside to VB-EM is that it is not entirely certain how one could obtain standard errors for the parameters, other than by bootstrapping.










