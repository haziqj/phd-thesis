Consider a statistical model for which we have real-valued observations $\by := \{y_1,\dots,y_n\}$, which are treated as realisations from an assumed probability distribution with parameters $\theta$.
The crux of statistical inference is to estimate $\theta$ given the observed values.
In the \emph{frequentist setting}, the \emph{likelihood} function, or simply likelihood, is a function of the parameters $\theta$ which measures the plausibility of the parameter value given the observed data to fit a statistical model.
It is defined as the mapping $\theta \mapsto p(\by|\theta)$, where $p(\by|\theta)$ is the probability density function (or in the case of discrete observations, the probability mass function) of the modelled distribution of the observations.
 
It is logical to consider the parameter set which provides the largest likelihood value,
\begin{equation}
  \hat\theta_{\text{ML}} = \argmax_\theta p(\by|\theta).
\end{equation}
The value $\hat\theta$ is referred to as the \emph{maximum likelihood estimate} for $\theta$.
For convenience, it is often the cases that the \emph{log-likelihood} function $L(\theta) = \log p(\by|\theta)$ is maximised instead.
As the logarithm is a monotonically increasing function, the maximiser of the log-likelihood function is exactly the maximiser of the likelihood function itself.
Besides invariance, the ML estimate comes with the attractive limiting property $\surd{n}(\theta_{\text{ML}} - \theta_{\text{true}}) \xrightarrow{\text{dist.}} \N(0,\cI)$ \citep{casella2002statistical} as sample size $n\to\infty$, where $\cI$ is the Fisher information (consistent, efficient, and asymptotically normal).

The \emph{Bayesian} approach to estimating $\theta$ takes a different outlook, in that it supplements what is already known from the data with additional information in the form of prior beliefs about the parameters.
This usually means treating the parameters as random, following some distribution dictated by a \emph{prior density} $p(\theta)$.
There are many ways of categorising different types of priors, but broadly speaking, priors, and hence Bayesian analysis \citep{robert2007bayesian,kadane2011principles}, can be either \emph{subjective} or \emph{objective}, with the demonyms `subjectivists' and `objectivists' used to refer to those subscribing to each respective principle.
Subjectivists assert that probabilities are merely opinions, while objectivists, in contrast, view probabilities as an extension of logic.
In this regard, objectives Bayes seek to minimise the statistician's contribution to inference and `let data speak for itself', while subjective Bayes does the opposite.

In either case, inference about the parameters are then performed using the \emph{posterior density}
\begin{equation}
  p(\theta|\by) \propto 
  \greyoverbrace{p(\by|\theta)}{\text{likelihood}}
  \times
  \greyoverbrace{p(\theta)}{\text{prior}},
\end{equation}

The normalising constant is the marginal likelihood over the distribution of the parameters, $p(\by) = \int p(\by|\theta) p(\theta) \dint \theta$.
Note that this quantity is free of $\theta$ because the parameters have been marginalised out, or put another way, considered in  entirety and averaged over all possible values of $\theta$ drawn from its prior density.
The quantity $p(\by)$ is also known as the \emph{model evidence}, or simply, \emph{evidence}.

The posterior density $p(\theta|\by)$ encapsulates the uncertainty surrounding the parameters $\theta$ after observing the data $\by$.
The \emph{posterior mean} 
\begin{equation}\label{eq:postmeanest}
  \tilde \theta = \int \theta p(\theta|\by) \dint\theta
\end{equation}
is normally taken to be the point estimate for $\theta$, with its uncertainty usually reported in the form of a \emph{credible interval}: if $\theta_k$ is the $k$'th component of $\theta$, then a $(1-\alpha) \times 100\%$ credible interval for $\theta_k$ is $(\theta_k^l, \theta_k^u)$, where  $\Prob( \theta_k^l \leq \theta_k \leq \theta_k^u ) = (1-\alpha) \times 100\%$.
Under a quadratic loss function, $\tilde\theta$ minimises the expected loss $\E[(\theta - \theta_{\text{true}})^2 ]$ \citep[ยง4.4.2, Result 3]{berger2013statistical}, and is hence also viewed as the \emph{minimum mean squared error} (MMSE) estimator.

On a practical note, integration over the parameter space may be intractable, for instance, the model consists of a large number of parameters for which we would like the posterior mean of, or the marginalising integral cannot be found in closed form.
Markov chain Monte Carlo (MCMC) methods are the standard way of approximating such integrals, by way of random sampling from the posterior.
The sample $\{\theta^{(1)},\dots,\theta^{(T)} \}$ is then manipulated in a way to derive its approximation.
In the case of the posterior mean,
\begin{equation}
  \hat{\E[\theta|\by]} = \frac{1}{T}\sum_{i=1}^T \theta^{(t)}
\end{equation}
gives an approximation, and its $(1-\alpha) \times 100\%$ credible interval can be approximated using the lower $\alpha/2\times 100\%$ and upper $(1-\alpha/2)\times 100\%$ quantile of the sample.

One may also find the value of $\theta$ which maximises the posterior,
\begin{equation}\label{eq:mapest}
  \hat\theta_{\text{MAP}} = \argmax_\theta p(\by|\theta)p(\theta),
\end{equation}
which is the mode of the posterior distribution.
This quantity is known as the \emph{maximum a posteriori} (MAP) estimate. 
It is different from the ML estimate in that the maximisation objective is augmented with with the prior density for $\theta$.
In this sense, MAP estimation can be seen as regularisation of the ML estimation procedure, whereby a `penalty' term is added to avoid overfitting.

MAP estimation is often criticised for not being representative of Bayesian methods.
That is, MAP estimation returns a point estimation with no apparent way of quantifying uncertainty of this point estimate.
Furthermore, unlike ML estimators, MAP estimators are invariant under reparameterisation.
If $\theta$ is a random variable with density $p(\theta)$, then the pdf of $\xi := g(\theta)$, where $g:\theta \mapsto g(\theta)$ is a one-to-one transformation, is
\begin{equation}\label{eq:pdftransform}
  p_\xi(\xi) = p_\theta\big(g^{-1}(\xi)\big) \left\vert \frac{\d }{\d \xi} g^{-1}(\xi) \right\vert.
\end{equation}
The second term in \cref{eq:pdftransform} is called the \emph{Jacobian (determinant)}.
Therefore, a different parameterisation of $\theta$ will impact the location of the maximum because of the introduction of the Jacobian into the optimisation objective \cref{eq:mapest}.

The term \emph{empirical Bayes} \citep{robbins1956empirical,casella1985introduction} refers to procedure in which features of the prior is informed by the data.
This is realised by parameterising the prior by a hyper-parameter $\eta$, i.e. $\theta \sim p(\theta|\eta)$.
%Consider a scenario in which the parameters to be estimated are the mean components of a multivariate normal distribution. 
%It is reasonable to assume that the means were drawn from an identical prior distribution, hyper-parameterised by $\eta$.
%%That is, if $\theta = (\theta_1,\dots,\theta_p)^\top$, then $\theta_k|\eta \iid p(\theta_k|\eta)$ for each $k=1,\dots,p$.
%The posterior density for $\theta$ is now written
%\begin{equation}
%  p(\theta|\by) 
%  = \int p(\theta,\eta|\by)p(\eta|\by) \dint\eta
%  = \int \frac{p(\by|\theta)p(\theta|\eta)}{p(\by|\eta)}p(\eta|\by) \dint\eta.
%\end{equation}
%Assuming that the density $p(\eta|\by)$ is concentrated around a point estimate, $\hat\eta = \argmax_\eta p(\eta|\by)$ say, then we can assume that
%\begin{equation}
%  p(\theta|\by) \approx \frac{p(\by|\theta)p(\theta|\hat\eta)}{p(\by|\hat\eta)}.
%\end{equation}
%This approach of optimising the hyperparameters $\eta$ by maximising the 
Values for the hyper-parameter are clearly important, because they appear in the posterior for $\theta$: 
\begin{equation}\label{eq:empbayes1}
  p(\theta|\by) = \frac{p(\by|\theta)p(\theta|\eta)}{p(\by|\eta)} 
\end{equation}
To avoid the subjectivist's approach of specifying values for $\eta$ a priori, one instead turns to the data for guidance.
Information concerning $\eta$ is contained in the marginal likelihood $p(\by|\eta) = \int p(\by|\theta)p(\theta|\eta) \dint \theta$.
This paves the way for using the \emph{maximum marginal likelihood} estimate
\begin{equation}
  \hat\eta = \argmax_\eta p(\by|\eta) 
\end{equation}
in place of $\eta$ in the equation of \cref{eq:empbayes1}.
This procedure is coined \emph{maximum likelihood type-II} by \citet{rasmussen2006gaussian}, and is commonly referred to as such in the machine learning literature.
It is also commonplace in statistics, especially in random-effects or latent variable models which employ a maximum likelihood procedure such as EM algorithm.

As a remark, estimation of $\eta$ itself can be made to conform to Bayesian philosophy, i.e., by placing priors on it and inferring $\eta$ through its posterior.
Such a procedure is referred to as \emph{Bayesian hierarchical modelling}.
A motivation for doing this is because the ML estimate of $\eta$ ignores any uncertainty in it.
Of course, the hyper-prior for $\eta$ could  parameterised by a hyper-hyper-parameter, and itself have a prior, and so on and so forth.
Evidently the model is specified until such a point where there are parameters of the model which are left `unoptimised' and must be specified in subjective manner.


\subsection{The EM algorithm for ML estimation}

\hltodo{direct ml ``difficult'', meaning no closed form estimates and requires numerical methods. gradient-based newton or quasinewton need derivatives, if not readily available then approximate numerical methods. if the z were known, then it is easy => EM algorithm.}

Often times, there are unobserved, random variables $\bz=\{z_1,\dots,z_n\}$ that are assumed to make up the data generative process, prescribed in the statistical model through the \emph{joint pdf} $p(\by,\bz|\theta)$.
Examples of models that include latent variables are plenty: Gaussian mixture models, latent class analysis, factor models, random coefficient models, and so on.
In order to obtain the ML estimates through a direct maximisation of the likelihood, it is necessary to first marginalise out the latent variables via
\begin{equation}\label{eq:varint}
  p(\by|\theta) 
  = \int 
  \greyoverbrace{p(\by|\bz,\theta)p(\bz|\theta)}{p(\by,\bz|\theta)}
  \dint \bz
\end{equation}
and obtain the \emph{marginal likelihood}.
Note that the integral is replaced by a summation over all possible values in the case of discrete latent variables $\bz$.


\subsection{A functional view of EM}

\subsection{A brief introduction to variational inference}
\label{sec:varintro}

Consider a statistical model for which we have observations $\by := \{y_1,\dots,y_n\}$, but also some latent variables $\bz := \{z_1,\dots,z_n\}$.
Typically, in such models, there is a want to to evaluate the integral 
\begin{equation}
  \cI = \int p(\by|\bz) p(\bz) \dint \bz.
\end{equation}

Marginalising out the latent variables in \cref{eq:varint} is usually a precursor to obtaining a log-likelihood function to be maximised, in a frequentist setting.
In Bayesian analysis, the $\bz$'s are parameters which are treated as random, and the integral corresponds to the marginal density for $\by$, on which the posterior depends.

In many instances, for one reason or another, evaluation of $\cI$ is difficult, in which case inference is halted unless a way of overcoming the intractable integral \cref{eq:varint} is found.
Here, we discus \emph{variational inference} (VI), a fully Bayesian treatment of the statistical model with a deterministic algorithm, i.e. does not involve sampling from posteriors.
The crux of variational inference is this: find a suitably close distribution function $q(z)$ that approximates the true posterior $p(\bz|\by)$, where closeness here is defined in the Kullback-Leibler divergence sense,
\[
  \KL(q\Vert p) = \int \log \frac{q(\bz)}{p(\bz|\by)} q(\bz) \dint \bz.
\]
Posterior inference is then conducted using $q(\bz)$ in lieu of $p(\bz|\by)$.
Advantages of this method are that 1) it is fast to implement computationally (compared to MCMC); 2) convergence is assessed simply by monitoring a single convergence criterion; and 3) it works well in practice, as attested to by the many studies implementing VI.

Briefly, we present the motivation behind variational inference and the minimisation of the KL divergence.
Denote by $q(\cdot)$ some density function of $\bz$.
One may show that log marginal density (the log of the intractable integral \cref{eq:varint}) holds the following bound:
\begin{align}
  \log p(y) &= \log p(\by,\bz) - \log p(\bz|\by) \mycomment{\footnotesize (Bayes' theorem)} \nonumber \\
  &= \int \left\{ \log \frac{p(\by,\bz)}{q(\bz)} - \log \frac{p(\bz|\by)}{q(\bz)} \right\} q(\bz) \dint z \mycomment{\footnotesize (expectations both sides)} \hspace{0.7cm} \nonumber \\    
  &=  \cL(q) +  \KL(q \Vert p) \nonumber \\
  &\geq \cL(q) \label{eq:varbound}
\end{align}
since the KL divergence is a non-negative quantity.
The functional $\cL(q)$ given by 
\begin{align}
  \cL(q) 
  &= \int \log \frac{p(\by,\bz)}{q(\bz)} q(\bz) \dint \bz \nonumber \\
%  &= \E_{\bz\sim q}[\log p(\by,\bz) - \log q(\bz)] \nonumber \\
  &= \E_{\bz\sim q}\log p(\by,\bz) + H(q), \label{eq:elbo1}
\end{align}
where $H$ is the entropy functional, is known as the \emph{evidence lower bound} (ELBO).
Evidently, the closer $q$ is to the true $p$, the better, and this is achieved by maximising $\cL$, or equivalently, minimising the KL divergence from $p$ to $q$.
Note that the bound \cref{eq:varbound} achieves equality if and only if $q(\bz) \equiv p(\bz|\by)$, but of course the true form of the posterior is unknown to us---see \cref{sec:varEM} for a discussion.
Maximising $\cL(q)$ or minimising $\KL(q\Vert p)$ with respect to the density $q$ is a problem of calculus of variations, which incidentally, is where variational inference takes its name.
The astute reader will realise that $\KL(q||p)$ is impossible to compute, since one does not know the true distribution $p(\bz|\by)$. Efforts are concentrated on maximising the ELBO instead.

Maximising $\cL$ over all possible density functions $q$ is not possible without considering certain constraints.
Two such constraints are described. 
The first, is to make a distributional assumption regarding $q$, for which it is parameterised by $\nu$.
For instance, we might choose the closest normal distribution to the posterior $p(\bz|\by)$ in terms of KL divergence.
In this case, the task is to find optimal mean and variance parameters of a normal distribution.

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}
    \draw[ultra thick] (0,0) ellipse (4cm and 2.2cm);
    \node at (-2.8,0.7) {$q(\bz;\nu)$};
    \draw[thick,colred] (-0.8,-0.5) to [curve through={(-2,0) .. (-2,-0.8) .. (1,-0.8) .. (-1,1.5) .. (0,1.5) ..(0.3,0.1) .. (0.5,1.5) .. (2,0) .. (2.1,-0.1) .. (2.2,1.1) .. (3,0.9)}] (3.317,1.225);  % curve using hobby tikz
    \draw[dashed, very thick,black!50] (3.317,1.225) -- (4.05,2.1);
    \fill (-0.8,-0.5) circle (2.2pt) node[below] {$\nu^{\text{init}}$};
    \fill (3.317,1.225) circle (2.2pt) node[left,yshift=-1] {$\nu^*$};
    \fill (4.05,2.1) circle (2.2pt) node[right,yshift=1] {$p(\bz|\by)$};
    \node[gray] at (4.5,1.4) {$\KL(q\Vert p)$};
  \end{tikzpicture}
  \vspace{-1em}
  \caption[Schematic view of variational inference. The aim is to find the closest distribution $q$ (parameterised by a variational parameter $\nu$) to $p$ in terms of KL divergence within the set of variational distributions, represented by the ellipse.]{Schematic view of variational inference\footnotemark. The aim is to find the closest distribution $q$ (parameterised by a variational parameter $\nu$) to $p$ in terms of KL divergence within the set of variational distributions, represented by the ellipse.}
\end{figure}
\footnotetext{Reproduced from the talk by David Blei entitled `Variational Inference: Foundations and Innovations', 2017. URL: \url{https://simons.berkeley.edu/talks/david-blei-2017-5-1}.}

The second type of constraint, and the one considered in this thesis, is simply an assumption that the approximate posterior $q$ factorises into $M$ disjoint factors.
Partition $\bz$ into $M$ disjoint groups $\bz = (z_{[1]},\dots,z_{[M]})$.
Note that each factor $z_{[k]}$ may be multidimensional.
Then, the structure
\[
  q(\bz) = \prod_{k=1}^M q_k(z_{[k]})
\]
for $q$ is considered.
This factorised form of variational inference is known in the statistical physics literature as the \emph{mean-field theory} \citep{itzykson1991statistical}.

Let us denote the distributions which minimise the Kullback-Leibler divergence (maximise the variational lower bound) by the use of tildes.
By appealing to \citet[equation 10.9, p. 466]{bishop2006pattern}, we find that for each $z_{[k]}$, $k=1,\dots,M$, $\tilde q_k$ satisfies
\begin{align}\label{eq:qtilde}
  \log \tilde q_k(z_{[k]}) = \E_{-k} \log p(\by,\bz) + \const
\end{align}
where expectation of the joint log density of $\by$ and $\bz$ is taken with respect to all of the unknowns $\bz$, except the one currently in consideration $z_{[k]}$, under their respective $\tilde q_k$ densities. 
%Estimates of the latent variables and parameters are then obtained by taking the mean of their respective approximate posterior distribution.

In practice, rather than an explicit calculation of the normalising constant, one simply needs to inspect \cref{eq:qtilde} to recognise it as a known log-density function, which is the case when exponential family distributions are considered.
That is, suppose that each complete conditional $p(z_{[k]}|\bz_{-k}, \by)$, where $\bz_{-k} = \{z_{[i]} | i \neq k\}$, follows an exponential family distribution
\[
  p(z_{[k]}| \bz_{-k}, \by ) 
  = B(z_{[k]})\exp \big(\ip{\zeta_k(\bz_{-k}, \by) , z_{[k]}} - A(\zeta_k) \big).
\]
Then, from \cref{eq:qtilde},
\begin{align*}
  \tilde q(z_{[k]})
  &\propto \exp\big(\E_{-k}\log p(z_{[k]}| \bz_{-k}, \by) \big) \\
  &= \exp \Big(\log B(z_{[k]}) + \E \ip{\zeta_k(\bz_{-k}, \by) , z_{[k]}} - \E [ A(\zeta_k) ] \Big) \\
  &\propto B(z_{[k]})\exp \E\ip{\zeta_\xi(\bz_{-k}, \by) , z_{[k]}}
\end{align*}
is also in the same exponential family.
In situations where there is no closed form expression for $\tilde q$, then one resorts to sampling methods such as a Metropolis random walk to estimate quantities of interest.
This stochastic step within a deterministic algorithm has been explored before in the context of a Monte Carlo EM algorithm---see \citet[ยง4, pp. 537--538]{meng1997algorithm} and references therein.

One notices that the optimal mean-field variational densities for each component are coupled with one another, in the sense that the distribution $\tilde q_k$ depends on the moments of the rest of the components $\bz_{-k}$.
For very simple problems, an exact solution for each $\tilde q_k$ can be found, but usually, the way around this is to employ an iterative procedure.
The \emph{coordinate ascent mean-field variational inference} (CAVI) algorithm cycles through each of the distributions in turn, updating them in sequence starting with arbitrary distributions as initial values.

\begin{algorithm}[H]
\caption{The CAVI algorithm}\label{alg:cavi}
  \begin{algorithmic}[1]
    \State \textbf{initialise} Variational factors $q_k(z_{[k]})$
    \While{ELBO $\cL(q)$ not converged}
      \For{$k = 1,\dots,M$}
        \State $\tilde q_k(z_{[k]}) \gets \const \times \exp\E_{-k}\log p(\by,\bz)$ \Comment{from \eqref{eq:qtilde}}
      \EndFor
      \State $\cL(q) \gets \E_{\bz \sim \prod_k\tilde q_k}\log p(\by, \bz) + \sum_{k=1}^m H\big[ q_k(z_{[k]})\big]$ \Comment{Update ELBO}
    \EndWhile
    \State \textbf{return} $\tilde q(\bz) = \prod_{k=1}^M \tilde q_j(z_{[k]})$ 
  \end{algorithmic}
\end{algorithm}

Each iteration of the CAVI brings about an improvement in the ELBO (hence the name coordinate ascent).
The algorithm terminates when there is no more significant improvement in the ELBO, indicating a convergence of the CAVI.
\citet{blei2017variational} notes that the ELBO is typically a non-convex function, in which case convergence may be to (one of possibly many) local optima.
A simple solution would be to restart the CAVI at multiple initial values, and the solution giving the highest ELBO is the distribution that is closest to the true posterior.

\subsection{Variational methods and the EM algorithm}
\label{sec:varEM}
\input{05-energy}

Consider again the latent variable setup described in \cref{sec:varintro}, but suppose the goal now is to maximise the (marginal) log-likelihood of the parameters $\theta$ of the model.
We will see how the EM algorithm relates to minimising the KL divergence between a density $q(\bz)$ and the posterior of $\bz$, and connect this idea to variational methods.

\begin{figure}[t]
  \centering
  \energyem 
  \caption[Illustration of the decomposition of the log likelihood.]{Illustration\footnotemark~of the decomposition of the log-likelihood into $\cL_\theta(q)$ and $\KL[q(\bz) \Vert p(\bz|\by)]$. The quantity $\cL_\theta(q)$ is a lower bound for the log-likelihood.}
  \label{fig:loglikdecomp}
\end{figure}
\footnotetext{Reproduced from \citet[Figure 9.11]{bishop2006pattern}.}

As we did in deriving \cref{eq:varbound}, we decompose the marginal log-likelihood as
\begin{align*}
  \log p(y|\theta) 
  &= \E \left[\log \frac{p(\by,\bz|\theta)}{q(\bz)} \right] - \E \left[\log \frac{p(\bz|\by,\theta)}{q(\bz)} \right]  = \cL(q) + \KL(q\Vert p).
%  &= \log p(\by,\bz|\theta) - \log p(\bz|\by) \mycomment{\footnotesize (Bayes' theorem)}  \\
%  &= \int \left\{ \log \frac{p(\by,\bz)}{q(\bz)} - \log \frac{p(\bz|\by)}{q(\bz)} \right\} q(\bz) \dint z \mycomment{\footnotesize (expectations both sides)} \hspace{0.7cm}  \\
%  &\geq \cL(q)
\end{align*}
This decomposition is shown in \cref{fig:loglikdecomp}.
We realise that the KL divergence non-negative, and is zero exactly when $q(\bz) \equiv p(\bz|\by,\theta)$.
Substituting this into the above equation yields the relationship
\begin{align*}
  \log p(y|\theta) 
  &= \E \left[\log \frac{p(\by,\bz|\theta)}{p(\bz|\by,\theta)} \right] 
  - \cancel{\E \left[\log \frac{p(\bz|\by,\theta)}{p(\bz|\by,\theta)} \right]}  \\
  &= \E \log p(\by,\bz|\theta) - \E p(\bz|\by,\theta).
\end{align*}
By taking expectations under the posterior distribution with known parameter values $\theta^{(t)}$, the term on the left becomes the $Q$ function of the E-step
\[
  Q(\theta) = Q(\theta|\theta^{(t)}) = \E_\bz\left[ \log p(\by,\bz|\theta) \,\big|\, \by,\theta^{(t)} \right],
\]
while the term on the left is an entropy term.
Thus, minimising the KL divergence corresponds to the E-step in the EM algorithm.
As a side fact, for any $\theta$, we find that
\begin{align*}
  \log p(\by|\theta) - \log p(\by|\theta^{(t)}) 
  &= Q(\theta|\theta^{(t)}) - Q(\theta^{(t)}|\theta^{(t)}) + \Delta \, \text{entropy} \\
  &\geq Q(\theta|\theta^{(t)}) - Q(\theta^{(t)}|\theta^{(t)}).
\end{align*}
because entropy differences are positive by Gibbs' inequality.
We see that maximising $Q$ with respect to $\theta$ (the M-step) brings about an improvement to the log-likelihood value.
To summarise, the EM algorithm is seen as
\begin{itemize}
  \item \textbf{E-step}. Maximise $\cL_\theta\big[q(\bz)\big]$ with respect to $q$, keeping $\theta$ fixed. This is equivalent to minimising $\KL(q\Vert p)$.
  \item \textbf{M-step}. Maximise $\cL\big[q(\bz|\theta)\big]$ with respect to $\theta$, keeping $q$ fixed. 
\end{itemize}

When the true posterior distribution $p(\bz|\by)$ is not tractable, then the E-step becomes intractable as well.
By constraining the maximisation in the E-step to consider $q$ belonging to a family of tractable densities, the E-step yields a variational approximation $\tilde q$ to the true posterior.
In \cref{sec:varintro}, we saw that constraining $q$ to be of a factorised form, then $\tilde q$ is a mean-field density.
This form of the EM is known as \emph{variational Bayes EM algorithm} (VB-EM) \citep{beal2003}.

\begin{figure}[p]
  \centering
  \energyemEstep \hspace{0.5cm}
  \energyvbEstep
  \energyemMstep \hspace{0.5cm}
  \energyvbMstepa
  \energyemMstepfade \hspace{0.5cm}
  \energyvbMstepb
  \energyemMstepfade \hspace{0.5cm}
  \energyvbMstepc
  \vspace{-1em} 
  \caption{Illustration of EM vs VB-EM. Whereas the EM guarantees an increase in log-likelihood value (red shaded region), the VB-EM does not.}
\end{figure}

In variational inference, a fully Bayesian treatment of the parameters is considered, with the aim of obtaining approximation to their posterior distributions.
In VB-EM, the variational approximation is only performed on the latent, or `missing' variables, to use the EM nomenclature.
After a variational E-step, the M-step proceeds as usual, and as such, all of the material relating to the EM in the previous chapter is applicable.
The VB-EM can also be seen as obtaining (approximate) maximum a posteriori estimates with diffuse priors on the parameters.

\hltodo{variational inference, EM algorithm, variational Bayes EM, differences, pros cons, MAP vs MLE, MAP vs fully Bayes}
