\subsection{Some theory}

Variational inference is a deterministic approximation of finding maximum likelihood estimates when the likelihood involves intractable integrals. The term `variational' comes from variational calculus - the mathematical analysis that deals with optimising functionals. 

In standard calculus, we deal with input variables ($\theta$, say) and a function of $\theta$ ($p$, say). We are then interested in solving the maximisation problem 
\[
  \argmax_\theta p(\theta).
\]
In the case where $p$ is a likelihood function then the solution to this problem is the maximum likelihood estimate. Typically we derive this by solving first-order conditions ($\delta p(\theta)/\delta\theta = 0$).

Variational calculus allows us to solve maximisation problems involving functionals. In this case, the inputs are functions $p$, and functionals are merely mappings from a set of functions to the real numbers. An example of a functional is the entropy of a pdf
\[
  \cH[p] = - \int p(x) \log p(x) \d x.
\]
We can pose similar optimisation problems with functionals, such as
\[
  \argmax_p \cH[p],
\]
for which the solution is a probability distribution which maximises the entropy function over all possible set of functions $p$. Variational calculus itself is not in any way a form of approximation. However, it can prove to be unfeasible to explore the set of all possible functions, in which case some restrictions have to be made. For example, we could consider only a certain family of functions, or as we will see later, that the function factorises easily.

\subsection{The KL divergence}

Let us consider an inferential problem for which we have $n$ (assumed) iid observations $y = (y_1,\dots,y_n)$, and perhaps also some latent variables $z = (z_1, \dots, z_n)$ that requires taking care of. In a fully Bayesian model, we can think of the $z$ as containing the parameters to be estimated as well. The goal is to find an approximation for the posterior distribution $p(z|y)$ as well as for the likelihood $p(y)$ (the model evidence, in Bayesian terminology).

Consider the Kullbackâ€“Leibler divergence between any distribution $q$ of the latent variables $z$, and the posterior $p(z|y)$
\[
  \KL(q \Vert p) = \E_q \left[ \log \frac{q(z)}{p(z|y)} \right]
  = \int q(z) \log \frac{q(z)}{p(z|y)} \d z.
\]
It is interesting to note that the log-likelihood $\log p(y)$ can be decomposed into a term which involves a KL divergence between some distribution $q$ and the posterior, and a linear functional of $q$:
\begin{align*}
  \log p(y) &= \log p(y,z) - \log p(z|y) \\
  \log p(y) &= \log p(y,z) - \log p(z|y) - \log q(z) + \log q(z) \\
  \int \log p(y) q(z) \d z&= \int \left\{ \log \frac{p(y,z)}{q(z)} - \log \frac{p(z|y)}{q(z)} \right\} q(z) \d z \\
  \log p(y) &= \cL(q) + \KL(q \Vert p) \\  
  &\phantom{.}{\color{gray} \geq \cL(q)}
\end{align*}

From the properties of the KL divergence, we know that it is a positive quantity. Thus, the functional $\cL$ is typically referred to as the \emph{lower bound}, and this serves as the proxy objective function in the likelihood maximisation problem. Note that maximising the lower bound is equivalent to minimising the KL-divergence. Of course $\KL(q \Vert p) \geq 0$ and achieves equality if and only if $q \equiv p$, but for whatever reason we cannot work with the posterior distribution $p(z|y)$ and instead must make some approximation to it in the form of $q(z)$. Incidentally, the EM algorithm is a special case of the variational inference in which $q \equiv p$. In such cases, one can either get closed-form estimates of the E-step involving the posterior distribution, or find ways around it by other estimation techniques.

\subsection{Factorised distributions}

In order to proceed with variational inference, we first make some assumptions about the distribution $q$. Our goal really is to restrict the form of $q$ such that computations become tractable. Suppose we partition the elements of $z$ into $m$ disjoint groups $z = (z^{(1)}, \dots, z^{(m)})$. We consider a restriction on $q$ such that
\begin{align}\label{eq:meanfieldtheory}
  q(z) = \prod_{i=1}^m q_i(z^{(i)}),
\end{align}
i.e., the distribution $q$ factorises with respect to the $m$ groups. \hl{This type of approximation has also been studied in Physics under mean-field theory}. Among all distributions $q$ which have the form $\eqref{eq:meanfieldtheory}$, we seek to find one which maximises the lower bound $\cL(q)$. Consider first the impact of this mean field assumption on the functional $\cL(q)$:
\begin{align*}
  \cL(q) &= \int \prod_{i=1}^m (q_i \d z_i) \log \left( \frac{p(y,z)}{\prod_{i=1}^m q_i} \right)  \\
  &= \int \prod_{i=1}^m (q_i \d z_i) \left( \log p(y,z) - \sum_{i=1}^m\log  q_i \right) \\
  &= \int q_k \prod_{i \neq k} (q_i \d z_i) \left( \log p(y,z) - \log q_k - \sum_{i \neq k} \log  q_i \right) \d z_k \\
  &= \int q_k  \left(\int \prod_{i \neq k} (q_i \d z_i) \log p(y,z) \right) \d z_k - \int q_k \log q_k  \d z_k + \const \\
  &= \int q_k \log \tilde p(y,z_k) \d z_k - \int q_k \log q_k  \d z_k + \const \\
  &= -\KL (q_k \Vert \tilde p) + \const
\end{align*}
where we have defined a new distribution $\tilde p(y,z_k)$ by the relation
\begin{align*}
  \log \tilde p(y,z_k) 
  &= \int \prod_{i \neq k} (q_i \d z_i) \log p(y,z) + \const \\
  &= \E_{-k}\left[ \log p(y,z) \right] + \const
\end{align*}
That is, $\tilde p(y,z_k)$ proportional to the exponent of the expectation of the joint distribution $p(y,z)$ under the factorised distribution $q$ as in \eqref{eq:meanfieldtheory}, but excluding factor $k$. The task of maximising $\cL$ is then equivalent to minimising the KL divergence $\KL (q_k \Vert \tilde p)$ $-$ for which the solution is $q_j \equiv \tilde p(y,z_j)$ for all $j \in \{1, \dots, m \}$.

In practice the normalising constant does not need to be calculated explicitly, because it can be found by inspection (if the kernel of the density $\tilde p$ is a recognisable form). It should be emphasised that the factorisation is the only assumption made to restrict the family of $q$, and that no explicit assumption about the functional form of $q$ is made.

