Computational challenges for the I-probit model stems from two sources: 1) calculation of the class probabilities \cref{eq:pij}; and 2) storage and time requirements for the variational EM algorithm.
Ways in which to overcome these challenges are discussed.
In addition, we also discuss considerations to take into account if estimation of the error precision $\bPsi$ is desired, and thus pave the way for future work.

\subsection{Efficient computation of class probabilities}
\label{sec:mnint}

The issue at hand here is that for $m>4$, the evaluation of the class probabilities in \cref{eq:pij} is computationally burdensome using classical methods such as quadrature methods \citet{geweke1994alternative}.
As such, simulation techniques (Monte Carlo integration) are employed instead.
The simplest strategy to overcome this is a frequency simulator (otherwise known as Monte Carlo integration): obtain random samples from $\N_{m}\big(\bmu(x_i), \bPsi^{-1}\big)$, and calculate how many of these samples fall within the required  region.
This method is fast and yields unbiased estimates of the class probabilities.
However, in an extensive comparative study of various probability simulators, \citet{hajivassiliou1996simulation} concluded that the Geweke-Hajivassiliou-Keane (GHK) probability simulator \citep{geweke1989bayesian,hajivassiliou1998method,keane1994solution} is the most reliable under a multitude of scenarios.
This is now described, and for clarity, we drop the subscript $i$ denoting individuals. 

Suppose that an observation $y=j$ has been made.
Reformulate $\by^*$ in \cref{eq:latentmodel} by anchoring on the $j$'th latent variable $y_j^*$ to obtain
\[
  \bz := (
  \greyoverbrace{y_1^* - y_j^*}{ z_1},
  \dots,
  \greyoverbrace{y_{j-1}^* - y_j^*}{ z_{j-1}},
  \greyoverbrace{y_{j+1}^* - y_j^*}{ z_{j}},
  \dots, 
  \greyoverbrace{y_m^* - y_j^*}{ z_{m-1}},
  )^\top \in \bbR^{m-1}.
\]
Note that we have indexed the vector $\bz$ using $j' = k$ if $k < j$, and $j' = k -1$ if $k > j$ for $k=1,\dots,m$, so that the index $j'$ runs from $1$ to $m-1$.
Let $\bQ \in \bbR^{(m-1)\times m}$ be a matrix formed by inserting a column of minus ones at the $j$'th position in an $(m-1)$ identity matrix.
We can then write $\bz = \bQ\by^*$, and thus we have that $\bz\sim\N_{m-1}(\bnu_{(j)}, \bOmega_{(j)})$, where $\bnu_{(j)} = \bQ\bmu(x_i)$ and $\bOmega_{(j)} = \bQ\bPsi^{-1}\bQ^\top$.
These are indexed by `$(j)$' because the transformation is dependent on which latent variable the $\bz$'s are anchored on.

\begin{remark}
  Incidentally, the probit model in \cref{eq:latentmodel} is equivalently represented by 
  \begin{equation}\label{eq:latentmodel2}
    y_i = 
    \begin{cases}
      1 & \text{if } \max (y_{i2}^* - y_{i1}^*,\dots,y_{im}^* - y_{i1}^*) < 0 \\
      j & \text{if } \max (y_{i2}^* - y_{i1}^*,\dots,y_{im}^* - y_{i1}^*) = y_{ij}^* - y_{i1}^* \geq 0,
    \end{cases}
  \end{equation}   
  which is obtained by anchoring on the first latent variable (referred to as the reference category), although the choice of reference category is arbitrary.
  This is similar to fixing the latent variables of the reference category to zero, and thus, as discussed previously in \cref{sec:iia}, full identification is achieved by fixing one more element of the covariance matrix.
\end{remark}

For the symmetric and positive definite covariance matrix $\bOmega_{(j)}$, obtain its Cholesky decomposition as $\bOmega_{(j)} = \bL\bL^\top$, where $\bL$ is a lower triangular matrix.
Then, $\bz = \bnu_{(j)} + \bL\bzeta$, where $\bzeta \sim\N_{m-1}(\bzero,\bI_{m-1})$.
That is,
\begin{align*}
  \begin{pmatrix}
    z_1 \\
    z_2 \\
    \vdots \\
    z_{m-1}
  \end{pmatrix}  
  &=
  \begin{pmatrix}
    \nu_{(j)1} \\
    \nu_{(j)2} \\    
    \vdots \\
    \nu_{(j)m-1}
  \end{pmatrix}  
  + 
  \begin{pmatrix}
    L_{11} &       &       & \\
    L_{21} &L_{22} &       & \\
    \vdots &\vdots &\ddots & \\
    L_{m-1,1} &L_{m-1,2} &\cdots &L_{m-1,m-1} \\
  \end{pmatrix} 
  \begin{pmatrix}
    \zeta_1 \\
    \zeta_2 \\    
    \vdots \\
    \zeta_{m-1}
  \end{pmatrix} \\ %\displaybreak
  &=
  \begin{pmatrix}
    \nu_{(j)1} + L_{11}\zeta_1 \\
    \nu_{(j)2} + \sum_{k=1}^2 L_{k2} \zeta_k \\    
    \vdots \\
    \nu_{(j)m-1} + \sum_{k=1}^{m-1} L_{k,m-1} \zeta_k
  \end{pmatrix}.  
\end{align*}

With this setup, the probability $p_{j}$ of an observation belonging to class $j$, which is equivalent to the probability that each $ z_{j'} < 0$, $j'=1,\dots,m-1$, can be expressed as
\begin{align*}
  p_j 
  ={}& \Prob( z_1 < 0,\dots, z_{m-1} < 0) \\
  ={}& 
  \Prob(\zeta_1 < u_1,\dots,\zeta_{m-1}<u_{m-1}) \\
  ={}& 
  \Prob(\zeta_1 < u_1)
  \Prob(\zeta_2 < u_2|\zeta_1 < u_1)
  \Prob(\zeta_3 < u_3|\zeta_1 < u_1,\zeta_2 < u_2)
  \cdots \\
  &\cdots
  \Prob(\zeta_{m-1}<u_{m-1}|\zeta_1 < u_1,\dots,\zeta_{m-2}<u_{m-2}),
\end{align*}
where 
\begin{equation*}
  u_{j'} = 
  u_{j'}(\zeta_1,\dots,\zeta_{j'-1}) =
  \begin{cases}
    - \nu_{(j)1} / L_{11} &\text{for } j' = 1 \\
    - \big(\nu_{(j)j'} + \sum_{k=1}^{j'-1} L_{kj'}\zeta_k \big) / L_{j'j'} &\text{for } j' = 2,\dots,m-1
  \end{cases}
\end{equation*}
The GHK algorithm entails making draws from one-sided right truncated standard normal distributions (for instance, using an inverse transform method detailed in \cref{apx:truncuninorm},  \mypageref{apx:truncuninorm}):
\begin{itemize}
  \item Draw $\tilde\zeta_1 \sim \tN(0,1,-\infty,u_1)$.
  \item Draw $\tilde\zeta_2 \sim \tN(0,1,-\infty,\tilde u_2)$, where $\tilde u_2 = u_2(\tilde\zeta_1)$.
  \item Draw $\tilde\zeta_3 \sim \tN(0,1,-\infty,\tilde u_3)$, where $\tilde u_3 = u_3(\tilde\zeta_1,\tilde\zeta_2)$. 
  \item $\cdots$
  \item Draw $\tilde\zeta_{m-1} \sim \tN(0,1,-\infty,\tilde u_{m-2})$, where $\tilde u_{m-1} = u_m(\tilde\zeta_1,\dots,\tilde\zeta_{m-2})$.
\end{itemize}
These values are then used in the following manner:
\begin{itemize}
  \item Use $\tilde\zeta_1$ to obtain a ``draw'' of $\Prob(\zeta_2 < u_2|\zeta_1 < \zeta_1)$,
  \begin{align*}
    \widetilde\Prob(\zeta_2 < u_2|\zeta_1 < \zeta_1)
    &=\Prob(\zeta_2 < u_2|\zeta_1 = \tilde\zeta_1) \\
    &= \Phi \Big( - \big(\nu_{(j)2} +  L_{12}\tilde\zeta_1 \big) / L_{22} \Big)
  \end{align*}
  \item Use $\tilde\zeta_1$ and $\tilde\zeta_2$ to obtain a ``draw'' of $\Prob(\zeta_3 < u_3|\zeta_1 < u_1,\zeta_2 < u_2)$,
  \begin{align*}
    \widetilde\Prob(\zeta_3 < u_3|\zeta_1 < u_1,\zeta_2 < u_2)
    &=\Prob(\zeta_3 < u_3|\zeta_1 = \tilde\zeta_1,\zeta_2 = \tilde\zeta_2)\\
    &= \Phi \Big( - \big(\nu_{(j)3} + L_{13}\tilde\zeta_1 + L_{23}\tilde\zeta_2 \big) / L_{33} \Big)
  \end{align*} 
  \item And so on. 
\end{itemize}
Therefore, a simulated probability for $p_j$ (denoted with a tilde) is obtained as
\begin{equation}\label{eq:tildepj}
  \tilde p_j = \Phi\left( -\nu_{(j)1}/ L_{11} \right) 
  \prod_{j'=2}^{m-1} \Phi \left( 
  - \big(\nu_{(j)j'} + \textstyle\sum_{k=1}^{j'-1} L_{kj'}\tilde\zeta_k \big) / L_{j'j'} 
  \right).
\end{equation}
By performing the above scheme $T$ number of times to obtain $T$ such simulated probabilities $\{p_j^{(1)},\dots,p_j^{(T)} \}$, the actual probability of interest $p_j$ is then approximated by the sample mean of the draws,
\[
  \hat p_j = \frac{1}{T} \sum_{t=1}^T p_j^{(t)}.
\]

If it so happens that one of the standard normal cdfs in \cref{eq:tildepj} is extremely small, this can cause loss of significance due to floating-point errors (catastrophic cancellation).
It is better to work on a log-probability scale, so the products in \cref{eq:tildepj} turn into sums, and revert back by exponentiating.

\begin{remark}
  The GHK algorithm provides reasonably fast and accurate calculations of class probabilities when $\bPsi$ is dense.
  As we alluded to earlier in the chapter, the class probabilities condense to a unidimensional integral involving products of normal cdfs (c.f. equation \ref{eq:pij2}) if $\bPsi$ is diagonal.
  Note that if $\bPsi$ is diagonal, then the transformed $\bOmega_{(j)} = \bQ\bPsi^{-1}\bQ^\top$ is certainly not: the components of $\bz$ are correlated because they are all anchored on the same random variable.
  Thus, direct evaluation of \cref{eq:pij2} using quadrature methods avoids the Cholesky step and random sampling employed by the GHK method.
\end{remark}

%\begin{remark}
%  The GHK probability simulator relates to the \emph{method of simulated likelihood}.  
%\end{remark}

\subsection{Efficient Kronecker product inverse}
\label{sec:complxiprobit}

As with the normal I-prior model, the time complexity of the variational inference algorithm for I-probit models is dominated by the step involving the posterior evaluation of the I-prior random effects $\bw$, which essentially is the inversion of an $nm \times nm$ matrix.
The matrix in question is %(from \cref{eq:varipostw})
\begin{align}
  \bV_w = \big[ (\bPsi \otimes \bH_\eta^2) + (\bPsi^{-1} \otimes \bI_n) \big]^{-1}. \tag{from \ref{eq:varipostw}}
\end{align}
We can actually exploit the Kronekcer product structure to compute the inverse efficiently.
Perform an orthogonal eigendecomposition of $\bH_\eta$ to obtain $\bH_\eta = \bV\bU\bV^\top$ and of $\bPsi$ to obtain $\bPsi = \bQ\bP\bQ^\top$.
This process takes $O(n^3 + m^3) \approx O(n^3)$ time if $m\ll n$ or if done in parallel, and needs to be performed once per CAVI iteration.
Then, manipulate $\bV_w^{-1}$ as follows:
\begin{align*}
  \bV_w^{-1} 
  &= (\bPsi \otimes \bH_\eta^2) + (\bPsi^{-1} \otimes \bI_n) \\
  &= (\bQ\bP\bQ^\top \otimes \bV\bU^2\bV^\top) + (\bQ\bP^{-1}\bQ^\top \otimes \bV\bV^\top) \\
  &= (\bQ \otimes \bV)(\bP \otimes \bU^2)(\bQ^\top \otimes \bV^\top) + 
  (\bQ \otimes \bV)(\bP^{-1} \otimes \bI_n)(\bQ^\top \otimes \bV^\top) \\
  &= (\bQ \otimes \bV)(\bP \otimes \bU^2 + \bP^{-1} \otimes \bI_n)(\bQ^\top \otimes \bV^\top) 
\end{align*}
Its inverse is 
\begin{align*}
  \bV_w 
  &=  (\bQ^\top \otimes \bV^\top)^{-1}(\bP \otimes \bU^2 + \bP^{-1} \otimes \bI_n)^{-1} (\bQ \otimes \bV)^{-1} \\
  &= (\bQ \otimes \bV)(\bP \otimes \bU^2 + \bP^{-1} \otimes \bI_n)^{-1}(\bQ^\top \otimes \bV^\top)
\end{align*}
which is easy to compute since the middle term is an inverse of diagonal matrices.
This brings time complexity of the variational EM algorithm down to a similar requirement as if $\bPsi$ were diagonal.
Unfortunately, storage requirements remain at $O(n^2m^2)$ when $\bPsi$ is dense, because the entire $nm \times nm$ matrix $\bV_w$ is needed to evaluate the posterior mean of $\vecc\bw$.

\subsection{Estimation of \texorpdfstring{$\bPsi$}{$\Psi$} in future work}
\label{sec:difficultPsi}

Suppose that $\bPsi \in \bbR^{m\times m}$ is a free parameter to be estimated, bearing in mind that only $m(m-1)/2 - 1$ variance components are identified in the I-probit model (see \cref{sec:iia}).
If so, the $Q$ function from \cref{eq:iprobitQEstep} conditional on the rest of the parameters can be written as
\vspace{-1.3em}
\begin{align*}
  Q(\bPsi|\balpha,\eta)
  &= \const 
  -\half \tr 
  \big( 
  \bPsi
  \greyoverbrace{\E\big( (\by^* - \bmu)^\top (\by^* - \bmu) \big)}{\bG_1}
  +
  \bPsi^{-1}
  \greyoverbrace{\E(\bw^\top\bw)}{\bG_2} 
  \Big)
\end{align*}
with $\bmu = \bone_n\balpha^\top + \bH_\eta\bw$.
This can be solved using numerical methods, though it must be ensured that the identifiability constraints and positive-definiteness are satisfied.
Specifically in the case where $\bPsi$ is a diagonal matrix $\diag(\psi_1,\dots,\psi_m)$, then
\begin{align*}
  \vspace{-0.5em}
  Q(\bPsi|\balpha,\eta)
  ={}& \const -\half \sum_{j=1}^m \psi_j \tr
  \E\big( (\by^*_{\bigcdot j} - \bmu_{\bigcdot j})(\by^*_{\bigcdot j} - \bmu_{\bigcdot j})^\top \big) \\
  & -\half \sum_{j=1}^m \psi_j^{-1} \tr
  \E(\bw_{\bigcdot j}\bw_{\bigcdot j}^\top)
\end{align*}
is maximised, for $j=1,\dots,m$, at
\[
  \hat\psi_j = \left( 
  \frac{\E( \bw_{\bigcdot j}^\top\bw_{\bigcdot j} ) }{\E\big( (\by^*_{\bigcdot j} - \bmu_{\bigcdot j})^\top (\by^*_{\bigcdot j} - \bmu_{\bigcdot j}) \big) }
  \right)^{\half},
\] 
independently of the rest of the other $\psi_k$'s, $k\neq j$.
As per the derivations in \cref{apx:qw} \colp{\mypageref{eq:trCEwDw}}, the numerator of this expression is equal to $\tr(\tilde\bV_{w_j} + \tilde\bw_{\bigcdot j}\tilde\bw_{\bigcdot j}^\top) = \tr (\tilde\bW_{jj})$.
The denominator on the other hand is
\[
  \E(\by_{\bigcdot j}^{*\top}\by_{\bigcdot j}^*) - 
  n\alpha_j^2 - \tr( \bH_\eta^2 \tilde\bW_{jj}) 
  - 2\by^{*\top}_{\bigcdot j}\bH_\eta\tilde\bw_{\bigcdot j}
  - 2\alpha_j \sum_{i=1}^n\sum_{i'=1}^n (y_{ij}^* - h_\eta(x_{i},x_{i'}) \tilde w_{ij}).
\]

In either the full or I-probit model, solving $\bPsi$ involves the second moments of a truncated normal distribution.
In the case where $\bPsi$ is dense, this is obtained by Monte Carlo methods, where samples from a truncated multivariate normal distribution are obtained using Gibbs sampling.
Although this strategy can be used when $\bPsi$ is diagonal, we show that the form for the second moments  involve integration of standard normal cdfs and pdfs \colp{\cref{thm:contruncn}, \mypageref{thm:contruncn}}, much like the formula for the first moments.
