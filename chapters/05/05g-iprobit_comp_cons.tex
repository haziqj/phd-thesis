Computational challenges for the I-probit model stems from two sources: 1) calculation of the class probabilities \cref{eq:pij}; and 2) storage and time requirements for the CAVI.
We also discuss issues faced with the estimation of the error precision $\bPsi$, and suggest ways to overcome this for future work.

\subsection{Efficient computation of class probabilities}
\label{misc:mnint}

%\hltodo{computationally burdensome using quadrature methods for m > 4. Realise that the dimension of the integral is m-1. describe two methods: 1) frequency sampler 2) efficient frequency sampler. NOTE THIS PART IS DIFFERENT FROM SIMULATING FROM TRUNCATED NORMAL---THIS ONLY APPEARS IN BAYESIAN METHODS SUCH AS GIBBS, VI, etc.}

As an opening remark, note that the dimension of the integral \cref{eq:pij} is $m-1$, since the $j$'th coordinates is fixed relative to the others.
An alternative specification of the I-probit model can be made in terms of \emph{relative differences} of the latent propensities.
Choosing the first category as the reference category, define new random variables $z_{ij} = y_{ij}^* - y_{i1}^*$, for $j = 2,\dots,m$. 
The model \cref{eq:latentmodel} is equivalently represented by
\begin{equation}
  y_i = 
  \begin{cases}
    1 & \text{if } \max (z_{i2},\dots,z_{im}) < 0 \\
    j & \text{if } \max (z_{i2},\dots,z_{im}) = z_{ij} \geq 0.
  \end{cases}
\end{equation} 
Write $\bz_{i\bigcdot} = (z_{i2},\dots,z_{im})^\top \in \bbR^{m-1}$.
Then $\bz_{i\bigcdot} = \bQ\by^*_{i\bigcdot}$, where $\bQ \in \bbR^{(m-1)\times m}$  is the $(m-1)$ identity matrix pre-augmented with a column vector of minus ones.
We have that $\bz_{i\bigcdot} \iid \N_{m-1}(\bnu, \bOmega)$, where $\bnu_{i\bigcdot} = \bQ\bmu_{i\bigcdot}$ and $\bOmega = \bQ\bPsi^{-1}\bQ^\top$.
Note that if $\bPsi$ is diagonal, then the transformation to $\bOmega$ will not retain diagonality---indeed, each component will undoubtedly be correlated with one another as they are all anchored on the same latent variable.

Now, the class probabilities for $j=2,\dots,m$ are
\begin{align}
  p_{ij} = 
  \int\displaylimits_{\{z_{ik} < 0 \,|\, \forall k \neq 1,j \}} \ind[z_{ij} \geq 0] \,\phi(\bz_{i\bigcdot} |\bnu_{i\bigcdot}, \bOmega) \dint\bz_{i\bigcdot} \ . \label{eq:pij3}
\end{align}
The class probability $p_{i1}$ is simply $p_{i1} = 1- \sum_{k\neq 1} p_{ik}$.
From this representation of the model, with $m=2$ (binary outcomes) we see that
\[
  p_{i1} = 
%  \int \ind(z_{i2} < 0) \phi(z_{i2}) \dint z_{i2} = 
  \Phi \left( \frac{z_{i2} - \nu}{\omega^{1/2}} \right)
  \hspace{0.5cm}\text{and}\hspace{0.5cm}
  p_{i2} = 
%  \int \ind(z_{i2} \geq 0) \phi(z_{i2}) \dint z_{i2} = 
  1 - \Phi \left( \frac{z_{i2} - \nu}{\omega^{1/2}} \right),
\]
where $\Phi(\cdot)$ is the CDF of the standard normal univariate distribution, and $\nu$ and $\omega$ are the mean and variance of the univariate random variable $\bz_{i\bigcdot} = z_{i2}$.
The probit link function involving the cdf of a standard normal is clearly seen here, especially if the error precision is treated as fixed such that $\omega = 1$. 

The issue at hand here is that for $m>4$, the evaluation of the class probabilities in \cref{eq:pij} is computationally burdensome using classical methods such as quadrature methods \citet{geweke1994alternative}.

The simplest strategy to overcome this is a frequency simulator (otherwise known as Monte Carlo integration): obtain random samples from $\N_{m-1}(\bnu_{i\bigcdot}, \bOmega)$, and calculate how many of these samples fall within the required  region.
This method is fast and yields unbiased estimates of the class probabilities.
However, accuracy of this method is questionable when the mean $\bnu_{i\bigcdot}$ of the multivariate normal is many standard deviations away from zero (the cutoff region as per \cref{eq:pij3}).

A more reliable method is the probability simulator of Geweke-Hajivassiliou-Keane (GHK)  \citep{geweke1991efficient,hajivassiliou1996simulation,keane1994computationally}, which we describe now.
For clarity, we drop the subscript $i$ denoting individuals, and write $\bz = (z_1,\dots,z_m)$, remembering that $z_1=0$.
Suppose that an observation $y=j$ has been made.
Rewrite the model by anchoring on the $j$'th latent variable $z_{j}$ as follows:
\[
  \tilde\bz := (
  \greyoverbrace{z_1 - z_j}{\tilde z_1},
  \dots,
  \greyoverbrace{z_{j-1} - z_j}{\tilde z_{j-1}},
  \greyoverbrace{z_{j+1} - z_j}{\tilde z_{j+1}},
  \dots, 
  \greyoverbrace{z_m - z_j}{\tilde z_m},
  )^\top \in \bbR^{m-1}.
\]
Let $\bnu_{(j)}$ and $\bOmega_{(j)}$ be the appropriately transformed mean vector and covariance matrix for $\tilde \bz$.
These are indexed by `$(j)$' because the transformation is dependent on which latent variable the $\bz$'s are anchored on.
Since this transformation is linear, $\tilde\bz \sim \N_{m-1}(\bnu_{(j)},\bOmega_{(j)})$.
For the symmetric and positive definite matrix $\bPsi^{-1}$, obtain its Cholesky decomposition as $\bOmega_{(j)} = \bL\bL^\top$, where $\bL$ is a lower triangular matrix.
Then, $\tilde\bz = \bnu_{(j)} + \bL\bzeta$, where $\bzeta \sim\N_{m-1}(\bzero,\bI_{m-1})$.
That is,
\begin{equation*}
  \begin{pmatrix}
    \tilde z_1 \\
    \tilde z_2 \\
    \vdots \\
    \tilde z_m
  \end{pmatrix}  
  =
  \begin{pmatrix}
    \nu_{(j)1} \\
    \nu_{(j)2} \\    
    \vdots \\
    \nu_{(j)m}
  \end{pmatrix}  
  +
  \begin{pmatrix}
    L_{11} &       &       & \\
    L_{21} &L_{22} &       & \\
    \vdots &\vdots &\ddots & \\
    L_{m1} &L_{m2} &\cdots &L_{mm} \\
  \end{pmatrix} 
  \begin{pmatrix}
    \zeta_1 \\
    \zeta_2 \\    
    \vdots \\
    \zeta_m
  \end{pmatrix}
  =
  \begin{pmatrix}
    \nu_{(j)1} + L_{11}\zeta_1 \\
    \nu_{(j)2} + \sum_{k=1}^2 L_{k2} \zeta_k \\    
    \vdots \\
    \nu_{(j)m} + \sum_{k=1}^m L_{km} \zeta_k
  \end{pmatrix}.  
\end{equation*}

With this setup, we can calculate $p_{j}$, the probability of class $j$, which is equivalent to the probability that each $\tilde z_k = z_k - z_j < 0$, as follows
\begin{align*}
  p_j 
  &= \Prob(\tilde z_1 < 0,\dots,\tilde z_{j-1} < 0, \tilde z_{j+1} < 0,\dots, \tilde z_m < 0) \\
  &= 
  \Prob(\zeta_1 < u_1,\dots,\zeta_{j-1}<u_{j-1},\zeta_{j+1}<u_{j+1},\dots,\zeta_{m}<u_{m}) \\
  &= 
  \Prob(\zeta_1 < u_1)
  \Prob(\zeta_2 < u_2|\zeta_1 < u_1)
  \cdots \\
  &\phantom{==}\cdots
  \Prob(\zeta_{m}<u_{m}|\zeta_1 < u_1,\dots,\zeta_{j-1}<u_{j-1},\zeta_{j+1}<u_{j+1},\dots,\zeta_{m-1}<u_{m-1}),
\end{align*}
where $u_i = u_i(\zeta_1,\dots,\zeta_{i-1}) = - (\nu_{(j)i} + \sum_{k=1}^{i-1} L_{ki}\zeta_k) / L_{ii}$.
Thus, the integral involving a $(m-1)$-variate normal density \cref{eq:pij3} is turned into a product of $m-1$ univariate normal cdfs, which can be computed fairly efficiently in modern computer systems.

As an aside, the GHK probability simulator, can be used to sample from a truncated multivariate normal distribution:
\begin{itemize}
  \item Draw $\tilde\zeta_1 \sim \tN(0,1,-\infty,u_1)$.
  \item Draw $\tilde\zeta_2 \sim \tN(0,1,-\infty,\tilde u_2)$, where $\tilde u_2 = u_2(\tilde\zeta_1)$.
  \item $\cdots$
  \item  Draw $\tilde\zeta_{j-1} \sim \tN(0,1,-\infty,\tilde u_{j-1})$, where $\tilde u_{j-1} = u_{j-1}(\tilde\zeta_1,\dots,\tilde\zeta_{j-2})$.
  \item Draw $\tilde\zeta_{j+1} \sim \tN(0,1,-\infty,\tilde u_{j+1})$, where $\tilde u_{j+1} = u_{j+1}(\tilde\zeta_1,\dots,\tilde\zeta_{j-1})$.
  \item $\cdots$
  \item Draw $\tilde\zeta_m \sim \tN(0,1,-\infty,\tilde u_m)$, where $\tilde u_m = u_m(\tilde\zeta_1,\dots,\tilde\zeta_{j-1},\tilde\zeta_{j+1},\dots,\tilde\zeta_{m-1})$.
\end{itemize}
Then, $\tilde z = \bnu_{(j)} \bL\tilde\zeta$ will be distributed according to $\N_{m-1}(\bnu_{(j)},\bOmega_{(j)})$.
Any quantity of interest, e.g. $\E r(\tilde z)$, can then be estimated by the sample mean.
In the variational algorithm, we require quantities such as first and second moments and also the entropy of a truncated multivariate normal distribution.
Alternative methods are also discussed in the appendix.

Finally, a point on independent probit models.
As we alluded to earlier in the chapter, the class probabilities condense to a unidimensional integral involving products of normal cdfs (see \cref{eq:pij2}) if $\bPsi$ is diagonal.
While this represents a massive simplification, care should be taken when dealing with the formula in \cref{eq:pij2}.
When at least one of the normal cdfs in the product is extremely small, this can cause loss of significance due to floating-point errors.
In the \pkg{iprobit} package, the product of normal cdfs is handled as a sum on the log scale to avoid this issue.

\subsection{Computational complexity of the CAVI algorithm}
\label{sec:complxiprobit}

As with the normal I-prior model, the time complexity of the variational inference algorithm for I-probit models is dominated by the step involving the posterior evaluation of the I-prior random effects $\bw$, which essentially is the inversion of an $nm \times nm$ matrix.
The matrix in question is %(from \cref{eq:varipostw})
\begin{align}
  \bV_w = \big[ (\bPsi \otimes \bH_\eta^2) + (\bPsi^{-1} \otimes \bI_n) \big]^{-1}. \tag{from \ref{eq:varipostw}}
\end{align}
We can actually exploit the Kronekcer product structure to compute the inverse efficiently.
Perform an orthogonal eigendecomposition of $\bH_\eta$ to obtain $\bH_\eta = \bV\bU\bV^\top$ and of $\bPsi$ to obtain $\bPsi = \bQ\bP\bQ^\top$.
This process takes $O(n^3 + m^3) \approx O(n^3)$ time if $m\ll n$ or if done in parallel, and needs to be performed once per CAVI iteration.
Then, manipulate $\bV_w^{-1}$ as follows:
\begin{align*}
  \bV_w^{-1} 
  &= (\bPsi \otimes \bH_\eta^2) + (\bPsi^{-1} \otimes \bI_n) \\
  &= (\bQ\bP\bQ^\top \otimes \bV\bU^2\bV^\top) + (\bQ\bP^{-1}\bQ^\top \otimes \bV\bV^\top) \\
  &= (\bQ \otimes \bV)(\bP \otimes \bU^2)(\bQ^\top \otimes \bV^\top) + 
  (\bQ \otimes \bV)(\bP^{-1} \otimes \bI_n)(\bQ^\top \otimes \bV^\top) \\
  &= (\bQ \otimes \bV)(\bP \otimes \bU^2 + \bP^{-1} \otimes \bI_n)(\bQ^\top \otimes \bV^\top) 
\end{align*}
Its inverse is 
\begin{align*}
  \bV_w 
  &=  (\bQ^\top \otimes \bV^\top)^{-1}(\bP \otimes \bU^2 + \bP^{-1} \otimes \bI_n)^{-1} (\bQ \otimes \bV)^{-1} \\
  &= (\bQ \otimes \bV)(\bP \otimes \bU^2 + \bP^{-1} \otimes \bI_n)^{-1}(\bQ^\top \otimes \bV^\top)
\end{align*}
which is easy to compute since the middle term is an inverse of diagonal matrices.
This brings time complexity of the CAVI down to a similar requirement as if $\bPsi$ was diagonal.

Storage requirements are still $O(n^2)$, and methods described in the previous chapter are applicable, particularly the discussion surrounding exponential family EM algorithm.
Prediction of a new data point is $O(n^2m)$, because there are essentially $m$ `separate' normal I-prior regressions, and each take $O(n^2)$ to evaluate.

\subsection{Difficulties faced with estimating \texorpdfstring{$\bPsi$}{$\Psi$}}

Suppose that, alongside the $\by^*$, $\bw$, $\eta$ and $\balpha$ in the CAVI algorithm described in \cref{sec:iprobitvar}, $\bPsi$ is a free parameter to be estimated.
If so, we find that the variational density $q$ for $\bPsi$ satisfies
\begin{align*}
  q(\bPsi)
  &\propto \exp\Bigg[-\half \tr \Big(
  \big( %\bG + 
  \greyoverbrace{\E[(\by^* - \bmu)^\top (\by^* - \bmu)]}{\bG_1}
  \big)\bPsi +
  \greyoverbrace{\E[\bw^\top\bw]}{\bG_2} \bPsi^{-1}
  \Big) \Bigg] \times p(\bPsi)
\end{align*}
where $p(\bPsi)$ is a prior density chosen for $\bPsi$.
Unfortunately, this does not resemble any known distribution, regardless of the prior choice for $\bPsi$.
One can resort to sampling techniques to obtain quantities such as the mean or entropy, which are needed, but this has not been studied for this project due to time limitations.
Even if this was possible, this requires, among other things, second moments of a truncated multivariate normal density $\by^*$, and also of $\bH_\eta\bw \in \bbR^{n\times m}$---of which both are a bit awkward to obtain.

What we realise, however, is that the \emph{posterior mode} is relatively easy to obtain, especially with an improper prior $p(\bPsi)\propto \const$ 
To see this, we look specifically at the case where $\bPsi$ is diagonal.
On the log scale,
\begin{align*}
  \log  q(\psi_j)
  &= \const - \half\sum_{j=1}^m \psi_j \E\norm{\by_{\bigcdot j}^* -  \bmu_{\bigcdot j}}^2 - \half\sum_{j=1}^m \psi_j^{-1} \E\norm{\bw_{\bigcdot j}}^2 \\
\end{align*}
is maximised, for $j=1,\dots,m$, at
\[
  \hat\psi_j = \sqrt{\frac{ \E\norm{\by_{\bigcdot j}^* -  \bmu_{\bigcdot j}}^2 }{\E\norm{\bw_{\bigcdot j}}^2}}.
\]
Perhaps, if the posterior mean is close to the mode, and not withstanding the involved calculations of the require second moments, then this quantity can be used instead in the CAVI algorithm.
This ties in well with a \emph{variational Bayes EM algorithm}, which is an alternative to a fully Bayesian treatment of variational inference.
This is discussed in \hltodo{Section X}.



