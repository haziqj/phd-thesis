\section{Proofs related to conically truncated multivariate normal distribution}

\subsection{Proof of Lemma}

Recall that for $Y \sim \tN(\mu,\sigma^2,-\infty,b)$, for some function $g$ of $Y$, we have that
\[
  \E g(Y) = \Phi(\beta)^{-1} \int g(y) \ind[y < b] \phi(y|\mu,\sigma^2) \dint y,
\]
and in particular, we have
\begin{align}
  \E[Y - \mu] &= -\sigma \frac{\phi(\beta)}{\Phi(\beta)} \label{eq:tnXminMu} \\
  \E[Y - \mu]^2 &= \sigma^2 \left[ 1 -  \frac{ \beta\phi(\beta)}{\Phi(\beta)}  \right] \label{eq:tnXminMusq}
\end{align}
where $\beta = (b - \mu)/\sigma$.
For the conically truncated multivariate normal distribution $X \sim \tN_d(\mu,\Sigma,\cA_j)$, where $\Sigma = \diag(\sigma_1^2,\dots,\sigma_d^2)$.
The independence structure of $\Sigma$ makes it possible to consider the expectations of each of the components separately by marginalising out the rest of the components. 
For simplicity, denote $p(x_k) = \phi(x_k|\mu_k,\sigma_k) = \sigma^{-1}_k \phi(\frac{x_k - \mu_k}{\sigma_k})$.
For $i \neq j$, we have
\begin{align}
  \E g(X_i)
  &= C^{-1} \idotsint  g(x_i) \ind[x_k < x_j, \forall k \neq j]  \prod_{k=1}^d p(x_k) \dint x_1 \cdots \dint x_d \nonumber \\
  &= C^{-1} \frac{\Phi((x_j-\mu_j)/\sigma_j)}{\Phi((x_j-\mu_j)/\sigma_j)} \iint g(x_i) \ind[x_i < x_j] p(x_i) p(x_j) \mathop{\prod_{k=1}^d}_{k \neq i,j} \Phi \left( \frac{x_j - \mu_k}{\sigma_k} \right)  \dint x_i \dint x_j \nonumber \\
  &= C^{-1}  \int \E_{X_i\sim\tN(\mu_i,\sigma_i^2,-\infty,x_j)} g(X_i) \mathop{\prod_{k=1}^d}_{k \neq j} \Phi \left( \frac{x_j - \mu_k}{\sigma_k} \right)  p(x_j)\dint x_j \label{eq:tnproofi}
  \end{align}
where $C$ is the normalising constant for $X$, while for the $j$'the component we have
\begin{align}
  \E g(X_j)
  &= C^{-1} \idotsint  g(x_j) \ind[x_k < x_j, \forall k \neq j]  \prod_{k=1}^d p(x_k) \dint x_1 \cdots \dint x_d \nonumber \\
  &= C^{-1} \int  g(x_j)  \mathop{\prod_{k=1}^d}_{k \neq j} \Phi \left( \frac{x_j - \mu_k}{\sigma_k} \right) p(x_j) \dint x_d. \label{eq:tnproofj}  
\end{align}

Plugging in \cref{eq:tnXminMu} for $g(X_i)=X_i - \mu_i$ in \cref{eq:tnproofi} we get
\begin{align*}
  \E X_i - \mu_i
  &= -C^{-1} \int  \left(  \sigma_i  \phi \left( \frac{x_j-\mu_i}{\sigma_i} \right) \Big/ \Phi \left( \frac{x_j-\mu_i}{\sigma_i}\right)  \right)  \mathop{\prod_{k=1}^d}_{k \neq j} \Phi \left( \frac{x_j - \mu_k}{\sigma_k} \right)  p(x_j) \dint x_j \\
  &= - \sigma_i C^{-1} \int  \phi \left( \frac{x_j-\mu_i}{\sigma_i} \right)  \mathop{\prod_{k=1}^d}_{k \neq i,j} \Phi \left( \frac{x_j - \mu_k}{\sigma_k} \right)  p(x_j) \dint x_j \\
  &= - \sigma_i C^{-1} \int  \phi \left( \frac{\sigma_j z + \mu_j -\mu_i}{\sigma_i} \right)  \mathop{\prod_{k=1}^d}_{k \neq j} \Phi \left( \frac{\sigma_j z + \mu_j - \mu_k}{\sigma_k} \right)  \phi(z) \dint z \\
  &= - \sigma_i C^{-1} \E_Z\bigg[ \phi \left( \frac{\sigma_j Z + \mu_j -\mu_i}{\sigma_i} \right)  \mathop{\prod_{k=1}^d}_{k \neq j} \Phi \left( \frac{\sigma_j Z + \mu_j - \mu_k}{\sigma_k} \right) \bigg]
\end{align*}
where $Z$ is the distribution of $\N(0,1)$, and we had used a change of variable $x_j = \sigma_j z + \mu_j$, so that $p(x_j) = \sigma_j^{-1} \phi(z)$ and $\d x_j = \sigma_j \d z$.
For the $j$'th component, substitute $g(x_j) = x_j - \mu_j$ in \cref{eq:tnproofj} to get
\begin{align*}
  \E X_j - \mu_j
  &= C^{-1} \int (x_j - \mu_j)  \mathop{\prod_{k=1}^d}_{k \neq j} \Phi \left( \frac{x_j - \mu_k}{\sigma_k} \right) p(x_j) \dint x_j \\
  &= C^{-1} \sigma_j  \int z  \mathop{\prod_{k=1}^d}_{k \neq j} \Phi \left( \frac{\sigma_j z + \mu_j - \mu_k}{\sigma_k} \right) \phi(z) \dint z \\
  &= \sigma_j  \mathop{\sum_{i=1}^d}_{i \neq j} \sigma_i C^{-1} \E \Bigg[ \phi \left( \frac{\sigma_j Z + \mu_j - \mu_i}{\sigma_i} \right) \mathop{\prod_{k=1}^d}_{k \neq i,j} \Phi \left( \frac{\sigma_j Z + \mu_j - \mu_k}{\sigma_k} \right) \Bigg] \\
  &= - \sigma_j \mathop{\sum_{i=1}^d}_{i \neq j} \big(\E X_i - \mu_i \big),
\end{align*}
where we have made use of Lemma \ref{lem:EZgZ} in the second last step.

For the second moment, plug in \cref{eq:tnXminMusq} for $g(X_i)= (X_i - \mu_i)^2$ in \cref{eq:tnproofi} to get
\begin{align*}
  \E [X_i - \mu_i]^2
  &= -C^{-1} \int  \sigma_i^2 \left[ 1 -  \frac{ \beta\phi(\beta)}{\Phi(\beta)}  \right]  \mathop{\prod_{k=1}^d}_{k \neq j} \Phi \left( \frac{x_j - \mu_k}{\sigma_k} \right)  p(x_j) \dint x_j \\
  &= -\sigma_i^2 C^{-1} 
  \greyoverbrace{\int \mathop{\prod_{k=1}^d}_{k \neq j} \Phi \left( \frac{x_j - \mu_k}{\sigma_k} \right)  p(x_j) \dint x_j}{C} \\
  &\phantom{==} + \sigma_i^{\cancel{2}} C^{-1} \int  
  \frac{x_j-\mu_i-\mu_j +\mu_j}{\cancel{\sigma_i}}
  \phi \left( \frac{x_j-\mu_i}{\sigma_i} \right)
  \mathop{\prod_{k=1}^d}_{k \neq i,j} \Phi \left( \frac{x_j - \mu_k}{\sigma_k} \right)  p(x_j) \dint x_j \\
  &= -\sigma_i^2 + \sigma_i C^{-1} \int  
  (x_j - \mu_j)
  \phi \left( \frac{x_j-\mu_i}{\sigma_i} \right)
  \mathop{\prod_{k=1}^d}_{k \neq i,j} \Phi \left( \frac{x_j - \mu_k}{\sigma_k} \right)  p(x_j) \dint x_j \\
  &\phantom{==} +  (\mu_i-\mu_j)  \cdot 
  \greyoverbrace{-\sigma_i C^{-1} \int  
  \phi \left( \frac{x_j-\mu_i}{\sigma_i} \right)
  \mathop{\prod_{k=1}^d}_{k \neq i,j} \Phi \left( \frac{x_j - \mu_k}{\sigma_k} \right)  p(x_j) \dint x_j}{\E X_i - \mu_i} \\
  &= (\mu_i-\mu_j)(\E X_i - \mu_i) -\sigma_i^2 \\
  &\phantom{==}
  + \sigma_i C^{-1} \int  \sigma_j z 
  \phi \left( \frac{x_j-\mu_i}{\sigma_i} \right)
  \mathop{\prod_{k=1}^d}_{k \neq i,j} \Phi \left( \frac{\sigma_j z + \mu_j - \mu_k}{\sigma_k} \right)  \phi(z) \dint z \\
  &= (\mu_i-\mu_j)(\E X_i - \mu_i) -\sigma_i^2 \\
  &\phantom{==} + \sigma_i\sigma_j C^{-1} 
  \E \Bigg[
  Z \phi \left( \frac{\sigma_j Z + \mu_j -\mu_i}{\sigma_i} \right) 
  \mathop{\prod_{k=1}^d}_{k \neq i,j} \Phi \left( \frac{\sigma_j Z + \mu_j - \mu_k}{\sigma_k} \right) 
  \Bigg]
\end{align*}

%\begin{align*}
%  \sigma_i C^{-1} \int  &\sigma_j z
%  \mathop{\prod_{k=1}^d}_{k \neq j} \Phi \left( \frac{\sigma_j z + \mu_j - \mu_k}{\sigma_k} \right)  \phi(z) \dint z\\
%  &= \sigma_i\sigma_j C^{-1} \E \Bigg[Z \mathop{\prod_{k=1}^d}_{k \neq j} \Phi \left( \frac{\sigma_j Z + \mu_j - \mu_k}{\sigma_k} \right) \Bigg] \\
%  &= \sigma_i(\E X_j - \mu_j)
%\end{align*}



\newpage
\begin{proof}
\begin{enumerate}[label=(\roman*)]
    
 

  \item The differential entropy is given by
  \begin{align*}
    \cH(p) &= -\int p(\bx) \log p(\bx) \d \bx = -\E \left[ \log p(\bx) \right] \\
    &= -\E \left[-\log C - \half[d] \log 2\pi - \half \sum_{i=1}^d \log \sigma_i^2 - \half \sum_{i=1}^d \left( \frac{x_i - \mu_i}{\sigma_i} \right)^2 \right] \\
    &= \log C + \half[d] \log 2\pi + \half \sum_{i=1}^d \log \sigma_i^2 + \half \sum_{i=1}^d \frac{1}{\sigma_i^2} \E [ x_i - \mu_i ]^2.
  \end{align*}
\end{enumerate}  
\end{proof}

\begin{lemma}\label{lem:EZgZ}
  Let $Z \sim \N(0,1)$. Then for all $m \in \{\bbN \, | \, m > 1\}$ and $(\mu, \sigma) \in \bbR \times \bbR^+$, 
  \[
    \E \Bigg[ Z \mathop{\prod_{k=1}^m}_{k \neq j} \Phi(\sigma_k Z + \mu_k) \Bigg]
    = \mathop{\sum_{i=1}^m}_{i \neq j} \E \Bigg[ \sigma_i \phi(\sigma_i Z + \mu_i) \mathop{\prod_{k=1}^m}_{k \neq i,j} \Phi (\sigma_k Z + \mu_k) \Bigg]
  \]
  for some $j \in \{1, \dots, m\}$.
\end{lemma}

\begin{proof}
  Use the fact that for any differentiable function $g$, $\E[Zg(Z)] = \E[g'(Z)]$, and apply the result with the function $g_m:z \mapsto \prod_{k \neq j} \Phi(\sigma_k z + \mu_k)$. All that is left is to derive the derivative of $g$, and we use an inductive proof to do this. 
  
  We adopt the following notation for convenience:
  \begin{align*}
    \phi_i = \phi(\sigma_i z + \mu_i) \\
    \Phi_i = \Phi(\sigma_i z + \mu_i) 
  \end{align*}
  
  The simplest case is when $m=2$, which can be trivially shown to be true. Without loss of generality, let $j=1$. Then
  \begin{align*}
    g_2(z) &= \Phi_2 \\
    \Rightarrow g_2'(z) &= \sigma_2 \phi_2 = \mathop{\sum_{i=1}^2}_{i \neq 1} \Bigg[ \sigma_i \phi_i \mathop{\sum_{k=1}^2}_{k \neq 1,2} \Phi_k \Bigg].
  \end{align*}
  
  Now assume that the inductive hypothesis holds for some $m \in \{\bbN \, | \, m > 1\}$. That is, the derivative of
  \[
    g_m(z) = \mathop{\prod_{k=1}^m}_{k \neq j} \Phi_k
  \]
  which is
  \[
    g_m'(z) = \mathop{\sum_{i=1}^m}_{i \neq j} \bigg[  \sigma_i \phi_i \mathop{\prod_{k=1}^m}_{k \neq i,j} \Phi_k \bigg],
  \]
  is assumed to be true. Assume that without loss of generality, $j \neq m+1$. Then the derivative of
  \[
    g_{m+1}(z) = \mathop{\prod_{k=1}^{m+1}}_{k \neq j} \Phi_k = g_m(z) \Phi_{m+1}
  \]
  is found to be
  \begin{align*}
    g_{m+1}'(z) &= \sigma_{m+1} \phi_{m+1} g_m(z) + g_m'(z) \Phi_{m+1} \\
    &= \sigma_{m+1} \phi_{m+1} \mathop{\prod_{k=1}^m}_{k \neq j} \Phi_k + \mathop{\sum_{i=1}^m}_{i \neq j} \bigg[  \sigma_i \phi_i \mathop{\prod_{k=1}^m}_{k \neq i,j} \Phi_k \bigg] \Phi_{m+1} \\
    &= \sigma_{m+1} \phi_{m+1} \mathop{\prod_{k=1}^{m+1}}_{k \neq j, m+1} \Phi_k + \mathop{\sum_{i=1}^m}_{i \neq j} \bigg[  \sigma_i \phi_i \mathop{\prod_{k=1}^{m+1}}_{k \neq i,j} \Phi_k \bigg] \\
    &= \mathop{\sum_{i=1}^{m+1}}_{i \neq j} \bigg[  \sigma_i \phi_i \mathop{\prod_{k=1}^{m+1}}_{k \neq i,j} \Phi_k \bigg] \\
    &= g_{m+1}'(z).
  \end{align*}
  Thus, by induction and linearity of expectations, the proof is complete.
\end{proof}
